{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTERR2zBG_yN",
        "outputId": "a59791aa-b0e6-41ee-9e48-9b63350a017a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.67)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain  langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf pymupdf langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r87_mBaMYq4",
        "outputId": "2bba96a9-f8c3-46b5-c17c-8be0dad27d9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.7.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.1.6)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.67)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (0.4.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (4.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVQo6UXfeSyI",
        "outputId": "dd620226-4b98-4e12-a5b2-ee65d255ffae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path='/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf'"
      ],
      "metadata": {
        "id": "5i0-CYZ6Krxu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Step 1: Load the PDF book\n",
        "pdf_path = \"/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf\"\n",
        "loader = PyMuPDFLoader(pdf_path)\n",
        "pages = loader.load()\n",
        "\n",
        "# Optional: Attach metadata to each page\n",
        "for i, page in enumerate(pages):\n",
        "    page.metadata[\"page_number\"] = i + 1  # Add page number\n",
        "    page.metadata[\"source\"] = pdf_path\n",
        "    page.metadata[\"title\"] = \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"\n",
        "\n",
        "# Step 2: Split each page while preserving metadata\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks = []\n",
        "for page in pages:\n",
        "    split_page_chunks = text_splitter.split_documents([page])\n",
        "    for chunk in split_page_chunks:\n",
        "        chunk.metadata.update(page.metadata)  # Propagate metadata to chunk\n",
        "        chunks.append(chunk)\n",
        "\n",
        "# Step 3: View some chunks with metadata\n",
        "for i, chunk in enumerate(chunks[:5]):\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print(chunk.page_content[:300], \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq8ib8V_HGHL",
        "outputId": "834b6a53-efb0-49ab-97b6-ae9e399feda4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chunk 1 ---\n",
            "Metadata: {'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2022-10-03T19:58:49+00:00', 'source': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'file_path': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'total_pages': 864, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Géron, Aurélien;', 'subject': '', 'keywords': '', 'moddate': '2022-10-03T16:42:30-04:00', 'trapped': '', 'modDate': \"D:20221003164230-04'00'\", 'creationDate': 'D:20221003195849Z', 'page': 0, 'page_number': 1}\n",
            "Aurélien Géron\n",
            "Hands-On \n",
            "Machine Learning \n",
            "with Scikit-Learn, \n",
            "Keras & TensorFlow\n",
            "Concepts, Tools, and Techniques \n",
            "to Build Intelligent Systems\n",
            "Third\n",
            "Edition\n",
            "TM \n",
            "\n",
            "--- Chunk 2 ---\n",
            "Metadata: {'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2022-10-03T19:58:49+00:00', 'source': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'file_path': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'total_pages': 864, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Géron, Aurélien;', 'subject': '', 'keywords': '', 'moddate': '2022-10-03T16:42:30-04:00', 'trapped': '', 'modDate': \"D:20221003164230-04'00'\", 'creationDate': 'D:20221003195849Z', 'page': 1, 'page_number': 2}\n",
            "DATA SCIENCE / MACHINE LEARNING\n",
            "“An exceptional \n",
            "resource to study \n",
            "machine learning. You \n",
            "will find clear-minded, \n",
            "intuitive explanations, \n",
            "and a wealth of \n",
            "practical tips.” \n",
            "—François Chollet\n",
            "Author of Keras, author of \n",
            "Deep Learning with Python\n",
            "“This book is a great \n",
            "introduction to the \n",
            "theory a \n",
            "\n",
            "--- Chunk 3 ---\n",
            "Metadata: {'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2022-10-03T19:58:49+00:00', 'source': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'file_path': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'total_pages': 864, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Géron, Aurélien;', 'subject': '', 'keywords': '', 'moddate': '2022-10-03T16:42:30-04:00', 'trapped': '', 'modDate': \"D:20221003164230-04'00'\", 'creationDate': 'D:20221003195849Z', 'page': 1, 'page_number': 2}\n",
            "simple, efficient tools to implement programs capable of learning \n",
            "from data. This bestselling book uses concrete examples, minimal \n",
            "theory, and production-ready Python frameworks (Scikit-Learn, \n",
            "Keras, and TensorFlow) to help you gain an intuitive understanding \n",
            "of the concepts and tools for buildi \n",
            "\n",
            "--- Chunk 4 ---\n",
            "Metadata: {'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2022-10-03T19:58:49+00:00', 'source': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'file_path': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'total_pages': 864, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Géron, Aurélien;', 'subject': '', 'keywords': '', 'moddate': '2022-10-03T16:42:30-04:00', 'trapped': '', 'modDate': \"D:20221003164230-04'00'\", 'creationDate': 'D:20221003195849Z', 'page': 1, 'page_number': 2}\n",
            "•\t Exploit unsupervised learning techniques such as \n",
            "dimensionality reduction, clustering, and anomaly detection\n",
            "•\t Dive into neural net architectures, including convolutional  \n",
            "nets, recurrent nets, generative adversarial networks, \n",
            "autoencoders, diffusion models, and transformers\n",
            "•\t Use TensorFlow \n",
            "\n",
            "--- Chunk 5 ---\n",
            "Metadata: {'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2022-10-03T19:58:49+00:00', 'source': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'file_path': '/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'total_pages': 864, 'format': 'PDF 1.5', 'title': 'Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow', 'author': 'Géron, Aurélien;', 'subject': '', 'keywords': '', 'moddate': '2022-10-03T16:42:30-04:00', 'trapped': '', 'modDate': \"D:20221003164230-04'00'\", 'creationDate': 'D:20221003195849Z', 'page': 2, 'page_number': 3}\n",
            "Aurélien Géron\n",
            "Hands-On Machine Learning\n",
            "with Scikit-Learn, Keras,\n",
            "and TensorFlow\n",
            "Concepts, Tools, and Techniques to\n",
            "Build Intelligent Systems\n",
            "THIRD EDITION\n",
            "Boston\n",
            "Farnham\n",
            "Sebastopol\n",
            "Tokyo\n",
            "Beijing\n",
            "Boston\n",
            "Farnham\n",
            "Sebastopol\n",
            "Tokyo\n",
            "Beijing \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.document_loaders import PDFPlumberLoader\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCEH5R4yYziJZlRmpClwLE7FGTrlvnUCdk\"\n",
        "from langchain.document_loaders import PyMuPDFLoader, TextLoader, UnstructuredWordDocumentLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "epHxHUi4PXSG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "ejRtJ7JQPXcK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import (\n",
        "    PyMuPDFLoader,\n",
        "    TextLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        "    UnstructuredPowerPointLoader,\n",
        ")\n",
        "\n",
        "def load_documents(directory_path):\n",
        "    \"\"\"Load all PDF, TXT, DOCX, and PPTX documents from a directory\"\"\"\n",
        "    documents = []\n",
        "    titles = []\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        path = os.path.join(directory_path, filename)\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "        # Select appropriate loader\n",
        "        if ext == \".pdf\":\n",
        "            loader = PyMuPDFLoader(path)\n",
        "        elif ext == \".txt\":\n",
        "            loader = TextLoader(path, encoding=\"utf-8\")\n",
        "        elif ext == \".docx\":\n",
        "            loader = UnstructuredWordDocumentLoader(path)\n",
        "        elif ext == \".pptx\":\n",
        "            loader = UnstructuredPowerPointLoader(path)\n",
        "        else:\n",
        "            print(f\"Skipping unsupported file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Load and enrich documents\n",
        "            docs = loader.load()\n",
        "            title = os.path.splitext(filename)[0]\n",
        "            titles.append(title)\n",
        "\n",
        "            for doc in docs:\n",
        "                doc.metadata[\"title\"] = title\n",
        "                doc.metadata[\"source_file\"] = filename\n",
        "\n",
        "            documents.extend(docs)\n",
        "            print(f\"Loaded document: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ Total documents loaded: {len(documents)}\")\n",
        "    return documents, titles\n"
      ],
      "metadata": {
        "id": "N8Yh6cLgPXjV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_chunk_size(documents, chunk_size=1000):\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    doc_groups = {}\n",
        "    for doc in documents:\n",
        "        doc_id = doc.metadata.get(\"title\") or doc.metadata.get(\"source_file\") or \"Unknown_Document\"\n",
        "        doc_groups.setdefault(doc_id, []).append(doc)\n",
        "\n",
        "    # Process each grouped document\n",
        "    for doc_id, docs in doc_groups.items():\n",
        "        full_text = \" \".join([doc.page_content for doc in docs])\n",
        "        source_file = docs[0].metadata.get(\"source_file\", doc_id)\n",
        "\n",
        "        num_chunks = (len(full_text) + chunk_size - 1) // chunk_size\n",
        "\n",
        "        for i in range(0, len(full_text), chunk_size):\n",
        "            chunk_text = full_text[i:i + chunk_size]\n",
        "            chunk_index = i // chunk_size\n",
        "\n",
        "            chunk = Document(\n",
        "                page_content=chunk_text,\n",
        "                metadata={\n",
        "                    \"title\": doc_id,\n",
        "                    \"source_file\": source_file,\n",
        "                    \"chunk_index\": chunk_index,\n",
        "                    \"total_chunks\": num_chunks\n",
        "                }\n",
        "            )\n",
        "            all_chunks.append(chunk)\n",
        "\n",
        "        print(f\"✅ Split document '{doc_id}' into {num_chunks} chunks\")\n",
        "\n",
        "    print(f\"\\n📄 Total chunks created: {len(all_chunks)}\")\n",
        "    return all_chunks\n"
      ],
      "metadata": {
        "id": "JxNAtnowPrYs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(chunks):\n",
        "    \"\"\"\n",
        "    Create a vector store from document chunks\n",
        "    \"\"\"\n",
        "    print(\"Creating vector store...\")\n",
        "    # Using HuggingFace embeddings (open-source alternative)\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    # Create the vector store\n",
        "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "    print(\"Vector store created successfully\")\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "_Az2o-AgUuT1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/docs\"\n",
        "documents, candidate_names = load_documents(folder_path)\n",
        "chunks=split_by_chunk_size(documents)\n",
        "vector_store = create_vector_store(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJC0FTWwU23R",
        "outputId": "fb7a04db-0e45-477c-e1ef-38770c7771c9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded document: Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf\n",
            "Skipping unsupported file: .ipynb_checkpoints\n",
            "\n",
            "✅ Total documents loaded: 864\n",
            "✅ Split document 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow' into 1778 chunks\n",
            "\n",
            "📄 Total chunks created: 1778\n",
            "Creating vector store...\n",
            "Vector store created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"ensemble learning\"\n",
        "relevant_docs = vector_store.similarity_search(topic, k=30)  # top 5 relevant chunks\n"
      ],
      "metadata": {
        "id": "4f2WaDN4VWis"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDcFl9m8e042",
        "outputId": "af07a04f-a3f0-493e-9bd7-1147992abbff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='b5e87d13-d64f-4d6b-969d-4d2b8e94a502', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 460, 'total_chunks': 1778}, page_content='e method, you can train a group of decision tree\\nclassifiers, each on a different random subset of the training set. You can then obtain\\nthe predictions of all the individual trees, and the class that gets the most votes is\\nthe ensemble’s prediction (see the last exercise in Chapter 6). Such an ensemble of\\ndecision trees is called a random forest, and despite its simplicity, this is one of the\\nmost powerful machine learning algorithms available today.\\nAs discussed in Chapter 2, you will often use ensemble methods near the end of\\na project, once you have already built a few good predictors, to combine them\\ninto an even better predictor. In fact, the winning solutions in machine learning\\ncompetitions often involve several ensemble methods—most famously in the Netflix\\nPrize competition.\\nIn this chapter we will examine the most popular ensemble methods, including\\nvoting classifiers, bagging and pasting ensembles, random forests, and boosting, and\\nstacking ensembles.\\n211 Voting Classifiers\\n'),\n",
              " Document(id='85721d11-e11b-40b5-b1b3-080b46f018d7', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 503, 'total_chunks': 1778}, page_content=' trained a blender, and\\nExercises \\n| \\n235 together with the classifiers it forms a stacking ensemble! Now evaluate the\\nensemble on the test set. For each image in the test set, make predictions with all\\nyour classifiers, then feed the predictions to the blender to get the ensemble’s pre‐\\ndictions. How does it compare to the voting classifier you trained earlier? Now\\ntry again using a StackingClassifier instead. Do you get better performance? If\\nso, why?\\nSolutions to these exercises are available at the end of this chapter’s notebook, at\\nhttps://homl.info/colab3.\\n236 \\n| \\nChapter 7: Ensemble Learning and Random Forests CHAPTER 8\\nDimensionality Reduction\\nMany machine learning problems involve thousands or even millions of features for\\neach training instance. Not only do all these features make training extremely slow,\\nbut they can also make it much harder to find a good solution, as you will see. This\\nproblem is often referred to as the curse of dimensionality.\\nFortunately, in real-world '),\n",
              " Document(id='3058d1c1-ea7c-4bf9-ad7c-a263c954bb3f', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 463, 'total_chunks': 1778}, page_content='7-3. The law of large numbers\\nSimilarly, suppose you build an ensemble containing 1,000 classifiers that are indi‐\\nvidually correct only 51% of the time (barely better than random guessing). If you\\npredict the majority voted class, you can hope for up to 75% accuracy! However, this\\nis only true if all classifiers are perfectly independent, making uncorrelated errors,\\nwhich is clearly not the case because they are trained on the same data. They are likely\\nto make the same types of errors, so there will be many majority votes for the wrong\\nclass, reducing the ensemble’s accuracy.\\nVoting Classifiers \\n| \\n213 Ensemble methods work best when the predictors are as independ‐\\nent from one another as possible. One way to get diverse classifiers\\nis to train them using very different algorithms. This increases the\\nchance that they will make very different types of errors, improving\\nthe ensemble’s accuracy.\\nScikit-Learn provides a VotingClassifier class that’s quite easy to use: just give\\nit a list'),\n",
              " Document(id='a2a21152-04f1-4cda-92be-8e40da8b726a', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 461, 'total_chunks': 1778}, page_content='Suppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nYou may have a logistic regression classifier, an SVM classifier, a random forest\\nclassifier, a k-nearest neighbors classifier, and perhaps a few more (see Figure 7-1).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions\\nof each classifier: the class that gets the most votes is the ensemble’s prediction. This\\nmajority-vote classifier is called a hard voting classifier (see Figure 7-2).\\nFigure 7-2. Hard voting classifier predictions\\n212 \\n| \\nChapter 7: Ensemble Learning and Random Forests Somewhat surprisingly, this voting classifier often achieves a higher accuracy than\\nthe best classifier in the ensemble. In fact, even if each classifier is a weak learner\\n(meaning it does only slightly better than random guessing), the ensemble can still be\\na strong learner (achieving high accuracy), provided there are a sufficient number of\\nweak '),\n",
              " Document(id='6aa87b20-9a6d-481d-9436-471d82678903', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 468, 'total_chunks': 1778}, page_content='t replacement, it is called\\npasting.4\\nBagging and Pasting \\n| \\n215 5 Bias and variance were introduced in Chapter 4.\\nIn other words, both bagging and pasting allow training instances to be sampled\\nseveral times across multiple predictors, but only bagging allows training instances to\\nbe sampled several times for the same predictor. This sampling and training process\\nis represented in Figure 7-4.\\nFigure 7-4. Bagging and pasting involve training several predictors on different random\\nsamples of the training set\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode for classification (i.e., the most frequent\\nprediction, just like with a hard voting classifier), or the average for regression. Each\\nindividual predictor has a higher bias than if it were trained on the original training\\nset, but aggregation reduces both bias and variance.5 General'),\n",
              " Document(id='5f79121d-0c20-42b9-8713-8037ea1188bc', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 459, 'total_chunks': 1778}, page_content='r the test set.\\nd. Evaluate these predictions on the test set: you should obtain a slightly higher\\nd.\\naccuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\\nyou have trained a random forest classifier!\\nSolutions to these exercises are available at the end of this chapter’s notebook, at\\nhttps://homl.info/colab3.\\nExercises \\n| \\n209  CHAPTER 7\\nEnsemble Learning and Random Forests\\nSuppose you pose a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of\\npredictors is called an ensemble; thus, this technique is called ensemble learning, and\\nan ensemble learning algorithm is called an ensemble method.\\nAs an example of an ensembl'),\n",
              " Document(id='b7a82052-bc9b-418e-bd5f-fdaf936ab8ef', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 253, 'total_chunks': 1778}, page_content=' several rounds, the final\\ncandidates are evaluated using full resources. This may save you some time tuning\\nhyperparameters.\\n94 \\n| \\nChapter 2: End-to-End Machine Learning Project Ensemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel—just like random forests perform better than the individual decision trees\\nthey rely on—especially if the individual models make very different types of errors.\\nFor example, you could train and fine-tune a k-nearest neighbors model, then create\\nan ensemble model that just predicts the mean of the random forest prediction and\\nthat model’s prediction. We will cover this topic in more detail in Chapter 7.\\nAnalyzing the Best Models and Their Errors\\nYou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor can indicate the relative importance of each\\nattribute for makin'),\n",
              " Document(id='281571f2-af2d-4b32-bc6d-8b8012cb1e14', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 501, 'total_chunks': 1778}, page_content='an help push your system’s performance to its limits.\\nExercises\\n1. If you have trained five different models on the exact same training data, and\\n1.\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2. What is the difference between hard and soft voting classifiers?\\n2.\\n3. Is it possible to speed up training of a bagging ensemble by distributing it across\\n3.\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4. What is the benefit of out-of-bag evaluation?\\n4.\\n5. What makes extra-trees ensembles more random than regular random forests?\\n5.\\nHow can this extra randomness help? Are extra-trees classifiers slower or faster\\nthan regular random forests?\\n6. If your AdaBoost ensemble underfits the training data, which hyperparameters\\n6.\\nshould you tweak, and how?\\n7. If your gradient boosting ensemble overfits the training set, should you increase\\n7.\\nor decrea'),\n",
              " Document(id='d888e708-34ba-4bef-9156-5eb9f4795739', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 497, 'total_chunks': 1778}, page_content='onal\\nfeatures, including GPU acceleration; you should definitely check\\nthem out! Moreover, the TensorFlow Random Forests library pro‐\\nvides optimized implementations of a variety of random forest\\nalgorithms, including plain random forests, extra-trees, GBRT, and\\nseveral more.\\nBoosting \\n| \\n231 18 David H. Wolpert, “Stacked Generalization”, Neural Networks 5, no. 2 (1992): 241–259.\\nStacking\\nThe last ensemble method we will discuss in this chapter is called stacking (short for\\nstacked generalization).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don’t we train a model to perform this aggregation? Figure 7-11 shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor\\n(called a blender, or a meta learner) takes these predictions as inputs and makes the\\nfinal '),\n",
              " Document(id='c8918bdc-c859-400e-8a5d-71487cdf1bfd', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 498, 'total_chunks': 1778}, page_content='prediction (3.0).\\nFigure 7-11. Aggregating predictions using a blending predictor\\nTo train the blender, you first need to build the blending training set. You can\\nuse cross_val_predict() on every predictor in the ensemble to get out-of-sample\\npredictions for each instance in the original training set (Figure 7-12), and use these\\n232 \\n| \\nChapter 7: Ensemble Learning and Random Forests can be used as the input features to train the blender; and the targets can simply be\\ncopied from the original training set. Note that regardless of the number of features\\nin the original training set (just one in this example), the blending training set will\\ncontain one input feature per predictor (three in this example). Once the blender is\\ntrained, the base predictors are retrained one last time on the full original training set.\\nFigure 7-12. Training the blender in a stacking ensemble\\nIt is actually possible to train several different blenders this way (e.g., one using linear\\nregression, another using '),\n",
              " Document(id='c3cdd1f3-29a2-4c70-9439-be6478f4fa3a', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 469, 'total_chunks': 1778}, page_content='ly, the net result is that\\nthe ensemble has a similar bias but a lower variance than a single predictor trained on\\nthe original training set.\\nAs you can see in Figure 7-4, predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons bagging and pasting are such popular methods: they scale\\nvery well.\\n216 \\n| \\nChapter 7: Ensemble Learning and Random Forests 6 max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of sampled\\ninstances is equal to the size of the training set times max_samples.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting: BaggingClassifier\\nclass (or BaggingRegressor for regression). The following code trains an ensemble\\nof 500 decision tree classifiers:6 each is trained on 100 training instances randomly\\nsampled from the training set with replacement (this is an example of'),\n",
              " Document(id='f346811c-80c1-437a-a2f2-5438f94e6210', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 489, 'total_chunks': 1778}, page_content='ntaining three trees. It can make predictions on a new\\ninstance simply by adding up the predictions of all the trees:\\n>>> X_new = np.array([[-0.4], [0.], [0.5]])\\n>>> sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\\narray([0.49484029, 0.04021166, 0.75026781])\\nFigure 7-9 represents the predictions of these three trees in the left column, and the\\nensemble’s predictions in the right column. In the first row, the ensemble has just one\\ntree, so its predictions are exactly the same as the first tree’s predictions. In the second\\nrow, a new tree is trained on the residual errors of the first tree. On the right you can\\nsee that the ensemble’s predictions are equal to the sum of the predictions of the first\\ntwo trees. Similarly, in the third row another tree is trained on the residual errors of\\nthe second tree. You can see that the ensemble’s predictions gradually get better as\\ntrees are added to the ensemble.\\nYou can use Scikit-Learn’s GradientBoostingRegressor class to tra'),\n",
              " Document(id='32b3a6f4-1972-4ace-abc5-658642db0df2', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 500, 'total_chunks': 1778}, page_content='\\nstacking_clf.fit(X_train, y_train)\\nFor each predictor, the stacking classifier will call predict_proba() if available; if not\\nit will fall back to decision_function() or, as a last resort, call predict(). If you\\ndon’t provide a final estimator, StackingClassifier will use LogisticRegression\\nand StackingRegressor will use RidgeCV.\\n234 \\n| \\nChapter 7: Ensemble Learning and Random Forests If you evaluate this stacking model on the test set, you will find 92.8% accuracy,\\nwhich is a bit better than the voting classifier using soft voting, which got 92%.\\nIn conclusion, ensemble methods are versatile, powerful, and fairly simple to use.\\nRandom forests, AdaBoost, and GBRT are among the first models you should test for\\nmost machine learning tasks, and they particularly shine with heterogeneous tabular\\ndata. Moreover, as they require very little preprocessing, they’re great for getting a\\nprototype up and running quickly. Lastly, ensemble methods like voting classifiers\\nand stacking classifiers c'),\n",
              " Document(id='1c22561b-d06c-424b-a0db-c1c21cc472bb', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 499, 'total_chunks': 1778}, page_content=\"random forest regression) to get a whole layer of blenders,\\nand then add another blender on top of that to produce the final prediction, as shown\\nin Figure 7-13. You may be able to squeeze out a few more drops of performance by\\ndoing this, but it will cost you in both training time and system complexity.\\nStacking \\n| \\n233 Figure 7-13. Predictions in a multilayer stacking ensemble\\nScikit-Learn provides two classes for stacking ensembles: StackingClassifier and\\nStackingRegressor. For example, we can replace the VotingClassifier we used at\\nthe beginning of this chapter on the moons dataset with a StackingClassifier:\\nfrom sklearn.ensemble import StackingClassifier\\nstacking_clf = StackingClassifier(\\n    estimators=[\\n        ('lr', LogisticRegression(random_state=42)),\\n        ('rf', RandomForestClassifier(random_state=42)),\\n        ('svc', SVC(probability=True, random_state=42))\\n    ],\\n    final_estimator=RandomForestClassifier(random_state=43),\\n    cv=5  # number of cross-validation folds\\n)\"),\n",
              " Document(id='bb38efe1-f261-4b09-9c5a-a633d74e58bc', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 472, 'total_chunks': 1778}, page_content='ven\\npredictor, while others may not be sampled at all. By default a BaggingClassifier\\nsamples m training instances with replacement (bootstrap=True), where m is the\\nsize of the training set. With this process, it can be shown mathematically that only\\nabout 63% of the training instances are sampled on average for each predictor.7 The\\nremaining 37% of the training instances that are not sampled are called out-of-bag\\n(OOB) instances. Note that they are not the same 37% for all predictors.\\nA bagging ensemble can be evaluated using OOB instances, without the need for\\na separate validation set: indeed, if there are enough estimators, then each instance\\nin the training set will likely be an OOB instance of several estimators, so these\\nestimators can be used to make a fair ensemble prediction for that instance. Once\\nyou have a prediction for each instance, you can compute the ensemble’s prediction\\naccuracy (or any other metric).\\nIn Scikit-Learn, you can set oob_score=True when creating a Baggi'),\n",
              " Document(id='011f8b85-bd41-429e-bc3e-ce9b63f09d9e', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 470, 'total_chunks': 1778}, page_content=' bagging, but\\nif you want to use pasting instead, just set bootstrap=False). The n_jobs parameter\\ntells Scikit-Learn the number of CPU cores to use for training and predictions, and\\n–1 tells Scikit-Learn to use all available cores:\\nfrom sklearn.ensemble import BaggingClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nbag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\\n                            max_samples=100, n_jobs=-1, random_state=42)\\nbag_clf.fit(X_train, y_train)\\nA BaggingClassifier automatically performs soft voting instead\\nof hard voting if the base classifier can estimate class probabilities\\n(i.e., if it has a predict_proba() method), which is the case with\\ndecision tree classifiers.\\nFigure 7-5 compares the decision boundary of a single decision tree with the decision\\nboundary of a bagging ensemble of 500 trees (from the preceding code), both trained\\non the moons dataset. As you can see, the ensemble’s predictions will likely generalize\\nmuch better '),\n",
              " Document(id='a316ef27-a51a-4694-be5c-316ce3dbca6f', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 475, 'total_chunks': 1778}, page_content='ad\\nof instance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis technique is particularly useful when you are dealing with high-dimensional\\ninputs (such as images), as it can considerably speed up training. Sampling both\\ntraining instances and features is called the random patches method.8 Keeping all\\ntraining instances (by setting bootstrap=False and max_samples=1.0) but sampling\\nfeatures (by setting bootstrap_features to True and/or max_features to a value\\nsmaller than 1.0) is called the random subspaces method.9\\nSampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nBagging and Pasting \\n| \\n219 10 Tin Kam Ho, “Random Decision Forests”, Proceedings of the Third International Conference on Document\\nAnalysis and Recognition 1 (1995): 278.\\n11 The BaggingClassifier class remains useful if you want a bag of something other than decision trees.\\nRandom Forests\\nAs we have discussed, a random forest'),\n",
              " Document(id='91a65997-0aaa-4426-a711-07810e875fef', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 1757, 'total_chunks': 1778}, page_content='earn, xvi\\nbagging and pasting in, 217\\nCART algorithm, 199, 205\\ncross-validation, 89-91\\ndesign principles, 70-71\\nPCA implementation, 246\\nPipeline constructor, 84-87\\nsklearn.base.BaseEstimator, 80\\nsklearn.base.clone(), 163\\nsklearn.base.TransformerMixin, 80\\nsklearn.cluster.DBSCAN, 279\\nsklearn.cluster.KMeans, 81, 263\\nsklearn.cluster.MiniBatchKMeans, 268\\nsklearn.compose.ColumnTransformer, 85\\nsklearn.compose.TransformedTargetRegres‐\\nsor, 79\\nsklearn.datasets.load_iris(), 167\\nsklearn.datasets.make_moons(), 179\\nsklearn.decomposition.IncrementalPCA,\\n251\\nsklearn.decomposition.PCA, 246\\nsklearn.ensemble.AdaBoostClassifier, 226\\nsklearn.ensemble.BaggingClassifier, 217\\nsklearn.ensemble.GradientBoostingRegres‐\\nsor, 227-230\\nsklearn.ensemble.HistGradientBoosting‐\\nClassifier, 230\\nsklearn.ensemble.HistGradientBoostingRe‐\\ngressor, 230\\nsklearn.ensemble.RandomForestClassifier,\\n117-119, 214, 220, 221, 248\\nsklearn.ensemble.RandomForestRegressor,\\n90, 220\\nsklearn.ensemble.StackingClassifier, 234\\nsklearn.ensemble.'),\n",
              " Document(id='4d5ede7f-adf0-4423-97cc-36f62cf01ddf', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 481, 'total_chunks': 1778}, page_content='tance, you get the image represented in\\nFigure 7-6.\\nFigure 7-6. MNIST pixel importance (according to a random forest classifier)\\nRandom forests are very handy to get a quick understanding of what features actually\\nmatter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting (originally called hypothesis boosting) refers to any ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede‐\\ncessor. There are many boosting methods available, but by far the most popular\\nare AdaBoost13 (short for adaptive boosting) and gradient boosting. Let’s start with\\nAdaBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfit. This results in new predictors\\nfocusing more and more on the hard cases. This is the technique used by AdaBoost.\\n222 \\n| \\nChapter 7: E'),\n",
              " Document(id='230bf426-57e6-4be5-89c8-ab024bd8a881', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 462, 'total_chunks': 1778}, page_content='learners in the ensemble and they are sufficiently diverse.\\nHow is this possible? The following analogy can help shed some light on this mystery.\\nSuppose you have a slightly biased coin that has a 51% chance of coming up heads\\nand 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\\nmore or less 510 heads and 490 tails, and hence a majority of heads. If you do the\\nmath, you will find that the probability of obtaining a majority of heads after 1,000\\ntosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,\\nwith 10,000 tosses, the probability climbs over 97%). This is due to the law of large\\nnumbers: as you keep tossing the coin, the ratio of heads gets closer and closer to\\nthe probability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. You\\ncan see that as the number of tosses increases, the ratio of heads approaches 51%.\\nEventually all 10 series end up so close to 51% that they are consistently above 50%.\\nFigure '),\n",
              " Document(id='2b548653-f3aa-4ed9-bf9b-c00a4dbe03c6', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 1727, 'total_chunks': 1778}, page_content=' learning, 692\\nexponential linear unit (ELU), 363-366\\nexponential scheduling, 389, 392\\nexport_graphviz(), 196\\nextra-trees, random forest, 220\\nextremely randomized trees ensemble (extra-\\ntrees), 221\\nF\\nface-recognition classifier, 125\\nfalse negatives, confusion matrix, 109\\nfalse positive rate (FPR) or fall-out, 115\\nfalse positives, confusion matrix, 109\\nfan-in/fan-out, 359\\nfast-MCD, 293\\nFCNs (fully convolutional networks), 525-526,\\n532\\nfeature engineering, 30, 262\\nfeature extraction, 12, 30\\nfeature maps, 485-487, 488, 508, 511\\nfeature scaling, 75-79, 141, 176\\nfeature selection, 30, 95, 159, 221\\nfeature vectors, 45, 133, 186\\nfeatures, 11\\nfederated learning, 746\\nfeedforward neural network (FNN), 309, 538\\nfetch_openml(), 104\\nfillna(), 68\\nfilters, convolutional layers, 484, 487, 497, 509\\nfirst moment (mean of gradient), 384\\nfirst-order partial derivatives (Jacobians), 386\\nfit()\\nand custom transformers, 80, 84\\ndata cleaning, 69\\nversus partial_fit(), 148\\nusing only with training set, 75\\nfitnes'),\n",
              " Document(id='1f500b11-8d3e-40fe-99e8-d84e24c7550e', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 471, 'total_chunks': 1778}, page_content='than the single decision tree’s predictions: the ensemble has a compara‐\\nble bias but a smaller variance (it makes roughly the same number of errors on the\\ntraining set, but the decision boundary is less irregular).\\nBagging introduces a bit more diversity in the subsets that each predictor is trained\\non, so bagging ends up with a slightly higher bias than pasting; but the extra diversity\\nalso means that the predictors end up being less correlated, so the ensemble’s variance\\nis reduced. Overall, bagging often results in better models, which explains why\\nit’s generally preferred. But if you have spare time and CPU power, you can use\\ncross-validation to evaluate both bagging and pasting and select the one that works\\nbest.\\nBagging and Pasting \\n| \\n217 7 As m grows, this ratio approaches 1 – exp(–1) ≈ 63%.\\nFigure 7-5. A single decision tree (left) versus a bagging ensemble of 500 trees (right)\\nOut-of-Bag Evaluation\\nWith bagging, some training instances may be sampled several times for any gi'),\n",
              " Document(id='af1da992-dcf9-44af-9262-7330a06996a5', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 467, 'total_chunks': 1778}, page_content='ameter to \"soft\", and ensure that all classifiers can estimate\\nclass probabilities. This is not the case for the SVC class by default, so you need\\nto set its probability hyperparameter to True (this will make the SVC class use\\ncross-validation to estimate class probabilities, slowing down training, and it will add\\na predict_proba() method). Let’s try that:\\n>>> voting_clf.voting = \"soft\"\\n>>> voting_clf.named_estimators[\"svc\"].probability = True\\n>>> voting_clf.fit(X_train, y_train)\\n>>> voting_clf.score(X_test, y_test)\\n0.92\\nWe reach 92% accuracy simply by using soft voting—not bad!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor but train them on different random subsets of the training set. When\\nsampling is performed with replacement,1 this method is called bagging2 (short for\\nbootstrap aggregating3). When sampling is performed withou'),\n",
              " Document(id='e45622a5-0df0-42a2-8117-79e28dab8dd4', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 490, 'total_chunks': 1778}, page_content='in GBRT ensem‐\\nbles more easily (there’s also a GradientBoostingClassifier class for classifica‐\\ntion). Much like the RandomForestRegressor class, it has hyperparameters to\\ncontrol the growth of decision trees (e.g., max_depth, min_samples_leaf), as well\\nas hyperparameters to control the ensemble training, such as the number of trees\\n(n_estimators). The following code creates the same ensemble as the previous one:\\nfrom sklearn.ensemble import GradientBoostingRegressor\\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\\n                                 learning_rate=1.0, random_state=42)\\ngbrt.fit(X, y)\\nBoosting \\n| \\n227 Figure 7-9. In this depiction of gradient boosting, the first predictor (top left) is trained\\nnormally, then each consecutive predictor (middle left and lower left) is trained on the\\nprevious predictor’s residuals; the right column shows the resulting ensemble’s predictions\\nThe learning_rate hyperparameter scales the contribution of each tree. If you set\\nit to a'),\n",
              " Document(id='018b18b8-9fc7-48a1-9604-beff10f07c3c', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 502, 'total_chunks': 1778}, page_content='se the learning rate?\\n8. Load the MNIST dataset (introduced in Chapter 3), and split it into a training\\n8.\\nset, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000\\nfor validation, and 10,000 for testing). Then train various classifiers, such as a\\nrandom forest classifier, an extra-trees classifier, and an SVM classifier. Next, try\\nto combine them into an ensemble that outperforms each individual classifier\\non the validation set, using soft or hard voting. Once you have found one, try\\nit on the test set. How much better does it perform compared to the individual\\nclassifiers?\\n9. Run the individual classifiers from the previous exercise to make predictions on\\n9.\\nthe validation set, and create a new training set with the resulting predictions:\\neach training instance is a vector containing the set of predictions from all your\\nclassifiers for an image, and the target is the image’s class. Train a classifier\\non this new training set. Congratulations—you have just'),\n",
              " Document(id='0f367f16-ad00-4150-a2ed-fd1d7e25c2bc', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 18, 'total_chunks': 1778}, page_content='                             199\\nComputational Complexity                                                                                        200\\nGini Impurity or Entropy?                                                                                         201\\nRegularization Hyperparameters                                                                              201\\nRegression                                                                                                                     204\\nSensitivity to Axis Orientation                                                                                   206\\nDecision Trees Have a High Variance                                                                       207\\nExercises                                                                                                                        208\\n7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\\nVoting Clas'),\n",
              " Document(id='fb379f9d-3076-4d52-ba76-7b13d6f33911', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 1758, 'total_chunks': 1778}, page_content='StackingRegressor, 234\\nsklearn.ensemble.VotingClassifier, 214\\nsklearn.externals.joblib, 97-98\\nsklearn.feature_selection.SelectFromModel,\\n95\\nsklearn.impute.IterativeImputer, 69\\nsklearn.impute.KNNImputer, 69\\nsklearn.impute.SimpleImputer, 68\\nsklearn.linear_model.ElasticNet, 162\\nsklearn.linear_model.Lasso, 161\\nsklearn.linear_model.LinearRegression, 25,\\n71, 79, 137, 138, 150\\nsklearn.linear_model.LogisticRegression,\\n168, 172, 275\\nsklearn.linear_model.Perceptron, 307\\nsklearn.linear_model.Ridge, 157\\nsklearn.linear_model.RidgeCV, 158\\nsklearn.linear_model.SGDClassifier, 106,\\n111, 112, 117-119, 121, 183, 188, 307\\nsklearn.linear_model.SGDRegressor, 147,\\n163\\nsklearn.manifold.LocallyLinearEmbedding,\\n254\\nsklearn.metrics.ConfusionMatrixDisplay,\\n122\\nsklearn.metrics.confusion_matrix(), 109,\\n122\\nsklearn.metrics.f1_score(), 111, 126\\nsklearn.metrics.mean_squared_error(), 88\\nsklearn.metrics.precision_recall_curve(),\\n113, 117\\nsklearn.metrics.precision_score(), 110\\nsklearn.metrics.recall_score(), 110\\nsklearn.'),\n",
              " Document(id='05c97852-85b5-4183-814e-7d04630289a0', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 243, 'total_chunks': 1778}, page_content='edictions. Such models composed of many\\nother models are called ensembles: they are capable of boosting the performance of\\n90 \\n| \\nChapter 2: End-to-End Machine Learning Project the underlying model (in this case, decision trees). The code is much the same as\\nearlier:\\nfrom sklearn.ensemble import RandomForestRegressor\\nforest_reg = make_pipeline(preprocessing,\\n                           RandomForestRegressor(random_state=42))\\nforest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\\n                                scoring=\"neg_root_mean_squared_error\", cv=10)\\nLet’s look at the scores:\\n>>> pd.Series(forest_rmses).describe()\\ncount       10.000000\\nmean     47019.561281\\nstd       1033.957120\\nmin      45458.112527\\n25%      46464.031184\\n50%      46967.596354\\n75%      47325.694987\\nmax      49243.765795\\ndtype: float64\\nWow, this is much better: random forests really look very promising for this task!\\nHowever, if you train a RandomForest and measure the RMSE on the training set,\\nyou wil'),\n",
              " Document(id='fd23caee-d0ad-42ff-ab1e-5225efb9d51a', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 10, 'total_chunks': 1778}, page_content='                                                                 79\\nTransformation Pipelines                                                                                          83\\nSelect and Train a Model                                                                                               88\\nTrain and Evaluate on the Training Set                                                                   88\\nBetter Evaluation Using Cross-Validation                                                              89\\nFine-Tune Your Model                                                                                                  91\\nGrid Search                                                                                                                 91\\nRandomized Search                                                                                                   93\\nEnsemble Methods                                                                                                 '),\n",
              " Document(id='9e062702-e6e8-4581-940e-806068fa5858', metadata={'title': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow', 'source_file': 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf', 'chunk_index': 0, 'total_chunks': 1778}, page_content='Aurélien Géron\\nHands-On \\nMachine Learning \\nwith Scikit-Learn, \\nKeras & TensorFlow\\nConcepts, Tools, and Techniques \\nto Build Intelligent Systems\\nThird\\nEdition\\nTM DATA SCIENCE / MACHINE LEARNING\\n“An exceptional \\nresource to study \\nmachine learning. You \\nwill find clear-minded, \\nintuitive explanations, \\nand a wealth of \\npractical tips.” \\n—François Chollet\\nAuthor of Keras, author of \\nDeep Learning with Python\\n“This book is a great \\nintroduction to the \\ntheory and practice \\nof solving problems \\nwith neural networks; \\nI recommend it to \\nanyone interested \\nin learning about \\npractical ML.” \\n—Pete Warden\\nMobile Lead for TensorFlow\\nHands-On Machine Learning \\nwith Scikit-Learn, Keras, and TensorFlow\\nISBN: 978-1-098-12597-4\\nUS $79.99\\t\\n CAN $99.99\\nThrough a recent series of breakthroughs, deep learning has \\nboosted the entire field of machine learning. Now, even program\\xad\\nmers who know close to nothing about this technology can use \\nsimple, efficient tools to implement programs capable of learning ')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt):\n",
        "    \"\"\"Generate response using Google Generative AI (Gemini) directly\"\"\"\n",
        "\n",
        "    # ✅ Configure API key securely\n",
        "    genai.configure(api_key=\"AIzaSyCEH5R4yYziJZlRmpClwLE7FGTrlvnUCdk\")\n",
        "\n",
        "    # ✅ Load the Gemini model\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")  # or \"gemini-1.5-pro\"\n",
        "\n",
        "    # ✅ Generate response\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "7BXje0WjRtfa"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_topic_slides_from_doc(doc_id: str, full_context: str, topic: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant tasked with creating an educational slide deck from a document.\n",
        "\n",
        "📄 Document Title: {doc_id}\n",
        "\n",
        "🎯 Topic of Focus: {topic}\n",
        "\n",
        "🔍 Content:\n",
        "\\\"\\\"\\\"\n",
        "{full_context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "---\n",
        "📝 Instructions:\n",
        "Based on the content above, create exactly 10 well-formatted slides that explain the topic **\"{topic}\"**.\n",
        "\n",
        "Each slide should have:\n",
        "- A clear and informative **title**\n",
        "- A short and focused **bullet-point summary** (3–5 bullets max)\n",
        "- Do **not** make up facts — only use what's explicitly in the document\n",
        "- Format clearly for readability\n",
        "\n",
        "---\n",
        "🎓 Output Format (Markdown Style):\n",
        "\n",
        "Slide 1️⃣\n",
        "**Title:** [Slide Title]\n",
        "- Bullet point 1\n",
        "- Bullet point 2\n",
        "...\n",
        "\n",
        "Slide 2️⃣\n",
        "**Title:** [Slide Title]\n",
        "- Bullet point 1\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "Slide 🔟\n",
        "**Title:** [Slide Title]\n",
        "- Bullet point 1\n",
        "...\n",
        "\n",
        "Make it easy to read and present.\n",
        "\"\"\"\n",
        "\n",
        "    response = generate_response(prompt)  # Replace this with your LLM call\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "f61-fpr1RtqU"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_id = \"Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow\"\n",
        "\n",
        "relevant_text = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "slides_output = generate_topic_slides_from_doc(doc_id, relevant_text, topic)\n",
        "print(slides_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bQ6S_VbfTjKO",
        "outputId": "22d2a1b4-220b-495f-f1d1-1c3dcd490a34"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slide 1️⃣\n",
            "**Title:** Ensemble Learning: The Wisdom of the Crowd\n",
            "\n",
            "- Aggregates predictions from multiple predictors (classifiers or regressors).\n",
            "- Often yields better predictions than the best individual predictor.\n",
            "- Analogous to the \"wisdom of the crowd\" effect.\n",
            "- A powerful technique used in many winning machine learning competitions (e.g., Netflix Prize).\n",
            "\n",
            "\n",
            "Slide 2️⃣\n",
            "**Title:** Voting Classifiers: Simple Ensemble Methods\n",
            "\n",
            "- A simple way to combine predictions from multiple classifiers.\n",
            "- \"Hard voting\":  The class with the most votes wins.\n",
            "- \"Soft voting\":  Aggregates class probabilities, often yielding higher accuracy.\n",
            "- Scikit-Learn provides the `VotingClassifier` class for easy implementation.\n",
            "\n",
            "\n",
            "Slide 3️⃣\n",
            "**Title:** Bagging and Pasting Ensembles\n",
            "\n",
            "- Train multiple predictors on different random subsets of the training set.\n",
            "- Bagging: Sampling with replacement (bootstrap=True). About 63% of instances are sampled on average for each predictor.\n",
            "- Pasting: Sampling without replacement (bootstrap=False).\n",
            "- Reduces both bias and variance compared to a single predictor.\n",
            "\n",
            "\n",
            "Slide 4️⃣\n",
            "**Title:** Out-of-Bag Evaluation\n",
            "\n",
            "- Bagging allows evaluation using out-of-bag (OOB) instances.\n",
            "- OOB instances are not used in training a particular predictor.\n",
            "- Provides a free validation set, eliminating the need for a separate validation set.\n",
            "- Set `oob_score=True` in `BaggingClassifier` for OOB evaluation in Scikit-Learn.\n",
            "\n",
            "\n",
            "Slide 5️⃣\n",
            "**Title:** Random Forests: Powerful Bagging Ensembles\n",
            "\n",
            "- An ensemble of decision trees, trained on different random subsets of the training set.\n",
            "-  Each tree is trained on a random subset of features.\n",
            "- Simple yet powerful algorithm.\n",
            "- Handy for feature selection by examining feature importance.\n",
            "\n",
            "\n",
            "Slide 6️⃣\n",
            "**Title:** Boosting: Sequential Ensemble Learning\n",
            "\n",
            "- Trains predictors sequentially, each correcting its predecessor.\n",
            "- AdaBoost: Pays more attention to instances that predecessors misclassified.\n",
            "- Gradient Boosting:  Trains predictors to minimize the residual errors of the previous predictors.\n",
            "- Popular methods include AdaBoost and Gradient Boosting.\n",
            "\n",
            "\n",
            "Slide 7️⃣\n",
            "**Title:** Gradient Boosting Regression Trees (GBRT)\n",
            "\n",
            "- Uses decision trees, trained sequentially on residual errors of previous trees.\n",
            "- `GradientBoostingRegressor` class in Scikit-Learn for regression tasks.\n",
            "- `GradientBoostingClassifier` class for classification tasks.\n",
            "- `learning_rate` hyperparameter scales contribution of each tree.\n",
            "\n",
            "\n",
            "Slide 8️⃣\n",
            "**Title:** Stacking Ensembles: Meta-Learners\n",
            "\n",
            "- Trains a model (a \"blender\" or \"meta-learner\") to aggregate predictions from multiple base predictors.\n",
            "- Uses out-of-sample predictions from base predictors to train the blender.\n",
            "- Can create multilayer stacking ensembles for potentially improved performance.\n",
            "- Scikit-Learn provides `StackingClassifier` and `StackingRegressor` classes.\n",
            "\n",
            "\n",
            "Slide 9️⃣\n",
            "**Title:** Stacking in Scikit-Learn\n",
            "\n",
            "- `StackingClassifier` and `StackingRegressor` simplify stacking implementation.\n",
            "-  Uses `predict_proba()`, `decision_function()`, or `predict()` from base predictors.\n",
            "- Uses `LogisticRegression` (default) or specified `final_estimator` for the blender in classification.\n",
            "- Uses `RidgeCV` (default) or specified `final_estimator` for the blender in regression.\n",
            "\n",
            "\n",
            "Slide 🔟\n",
            "**Title:** Ensemble Methods: Summary\n",
            "\n",
            "- Ensemble methods combine multiple predictors to improve accuracy.\n",
            "- Various techniques exist, including voting, bagging, pasting, boosting, and stacking.\n",
            "- Scikit-Learn provides easy-to-use classes for implementing ensemble methods.\n",
            "- Effective for many machine learning tasks, especially with heterogeneous data.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xthEnt65e60u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(relevant_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cY6rPR-e7AA",
        "outputId": "2edda129-ac0c-47a4-d796-2a28d87fd8ed"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e method, you can train a group of decision tree\n",
            "classifiers, each on a different random subset of the training set. You can then obtain\n",
            "the predictions of all the individual trees, and the class that gets the most votes is\n",
            "the ensemble’s prediction (see the last exercise in Chapter 6). Such an ensemble of\n",
            "decision trees is called a random forest, and despite its simplicity, this is one of the\n",
            "most powerful machine learning algorithms available today.\n",
            "As discussed in Chapter 2, you will often use ensemble methods near the end of\n",
            "a project, once you have already built a few good predictors, to combine them\n",
            "into an even better predictor. In fact, the winning solutions in machine learning\n",
            "competitions often involve several ensemble methods—most famously in the Netflix\n",
            "Prize competition.\n",
            "In this chapter we will examine the most popular ensemble methods, including\n",
            "voting classifiers, bagging and pasting ensembles, random forests, and boosting, and\n",
            "stacking ensembles.\n",
            "211 Voting Classifiers\n",
            "\n",
            "\n",
            " trained a blender, and\n",
            "Exercises \n",
            "| \n",
            "235 together with the classifiers it forms a stacking ensemble! Now evaluate the\n",
            "ensemble on the test set. For each image in the test set, make predictions with all\n",
            "your classifiers, then feed the predictions to the blender to get the ensemble’s pre‐\n",
            "dictions. How does it compare to the voting classifier you trained earlier? Now\n",
            "try again using a StackingClassifier instead. Do you get better performance? If\n",
            "so, why?\n",
            "Solutions to these exercises are available at the end of this chapter’s notebook, at\n",
            "https://homl.info/colab3.\n",
            "236 \n",
            "| \n",
            "Chapter 7: Ensemble Learning and Random Forests CHAPTER 8\n",
            "Dimensionality Reduction\n",
            "Many machine learning problems involve thousands or even millions of features for\n",
            "each training instance. Not only do all these features make training extremely slow,\n",
            "but they can also make it much harder to find a good solution, as you will see. This\n",
            "problem is often referred to as the curse of dimensionality.\n",
            "Fortunately, in real-world \n",
            "\n",
            "7-3. The law of large numbers\n",
            "Similarly, suppose you build an ensemble containing 1,000 classifiers that are indi‐\n",
            "vidually correct only 51% of the time (barely better than random guessing). If you\n",
            "predict the majority voted class, you can hope for up to 75% accuracy! However, this\n",
            "is only true if all classifiers are perfectly independent, making uncorrelated errors,\n",
            "which is clearly not the case because they are trained on the same data. They are likely\n",
            "to make the same types of errors, so there will be many majority votes for the wrong\n",
            "class, reducing the ensemble’s accuracy.\n",
            "Voting Classifiers \n",
            "| \n",
            "213 Ensemble methods work best when the predictors are as independ‐\n",
            "ent from one another as possible. One way to get diverse classifiers\n",
            "is to train them using very different algorithms. This increases the\n",
            "chance that they will make very different types of errors, improving\n",
            "the ensemble’s accuracy.\n",
            "Scikit-Learn provides a VotingClassifier class that’s quite easy to use: just give\n",
            "it a list\n",
            "\n",
            "Suppose you have trained a few classifiers, each one achieving about 80% accuracy.\n",
            "You may have a logistic regression classifier, an SVM classifier, a random forest\n",
            "classifier, a k-nearest neighbors classifier, and perhaps a few more (see Figure 7-1).\n",
            "Figure 7-1. Training diverse classifiers\n",
            "A very simple way to create an even better classifier is to aggregate the predictions\n",
            "of each classifier: the class that gets the most votes is the ensemble’s prediction. This\n",
            "majority-vote classifier is called a hard voting classifier (see Figure 7-2).\n",
            "Figure 7-2. Hard voting classifier predictions\n",
            "212 \n",
            "| \n",
            "Chapter 7: Ensemble Learning and Random Forests Somewhat surprisingly, this voting classifier often achieves a higher accuracy than\n",
            "the best classifier in the ensemble. In fact, even if each classifier is a weak learner\n",
            "(meaning it does only slightly better than random guessing), the ensemble can still be\n",
            "a strong learner (achieving high accuracy), provided there are a sufficient number of\n",
            "weak \n",
            "\n",
            "t replacement, it is called\n",
            "pasting.4\n",
            "Bagging and Pasting \n",
            "| \n",
            "215 5 Bias and variance were introduced in Chapter 4.\n",
            "In other words, both bagging and pasting allow training instances to be sampled\n",
            "several times across multiple predictors, but only bagging allows training instances to\n",
            "be sampled several times for the same predictor. This sampling and training process\n",
            "is represented in Figure 7-4.\n",
            "Figure 7-4. Bagging and pasting involve training several predictors on different random\n",
            "samples of the training set\n",
            "Once all predictors are trained, the ensemble can make a prediction for a new\n",
            "instance by simply aggregating the predictions of all predictors. The aggregation\n",
            "function is typically the statistical mode for classification (i.e., the most frequent\n",
            "prediction, just like with a hard voting classifier), or the average for regression. Each\n",
            "individual predictor has a higher bias than if it were trained on the original training\n",
            "set, but aggregation reduces both bias and variance.5 General\n",
            "\n",
            "r the test set.\n",
            "d. Evaluate these predictions on the test set: you should obtain a slightly higher\n",
            "d.\n",
            "accuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\n",
            "you have trained a random forest classifier!\n",
            "Solutions to these exercises are available at the end of this chapter’s notebook, at\n",
            "https://homl.info/colab3.\n",
            "Exercises \n",
            "| \n",
            "209  CHAPTER 7\n",
            "Ensemble Learning and Random Forests\n",
            "Suppose you pose a complex question to thousands of random people, then aggregate\n",
            "their answers. In many cases you will find that this aggregated answer is better than\n",
            "an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate\n",
            "the predictions of a group of predictors (such as classifiers or regressors), you will\n",
            "often get better predictions than with the best individual predictor. A group of\n",
            "predictors is called an ensemble; thus, this technique is called ensemble learning, and\n",
            "an ensemble learning algorithm is called an ensemble method.\n",
            "As an example of an ensembl\n",
            "\n",
            " several rounds, the final\n",
            "candidates are evaluated using full resources. This may save you some time tuning\n",
            "hyperparameters.\n",
            "94 \n",
            "| \n",
            "Chapter 2: End-to-End Machine Learning Project Ensemble Methods\n",
            "Another way to fine-tune your system is to try to combine the models that perform\n",
            "best. The group (or “ensemble”) will often perform better than the best individual\n",
            "model—just like random forests perform better than the individual decision trees\n",
            "they rely on—especially if the individual models make very different types of errors.\n",
            "For example, you could train and fine-tune a k-nearest neighbors model, then create\n",
            "an ensemble model that just predicts the mean of the random forest prediction and\n",
            "that model’s prediction. We will cover this topic in more detail in Chapter 7.\n",
            "Analyzing the Best Models and Their Errors\n",
            "You will often gain good insights on the problem by inspecting the best models. For\n",
            "example, the RandomForestRegressor can indicate the relative importance of each\n",
            "attribute for makin\n",
            "\n",
            "an help push your system’s performance to its limits.\n",
            "Exercises\n",
            "1. If you have trained five different models on the exact same training data, and\n",
            "1.\n",
            "they all achieve 95% precision, is there any chance that you can combine these\n",
            "models to get better results? If so, how? If not, why?\n",
            "2. What is the difference between hard and soft voting classifiers?\n",
            "2.\n",
            "3. Is it possible to speed up training of a bagging ensemble by distributing it across\n",
            "3.\n",
            "multiple servers? What about pasting ensembles, boosting ensembles, random\n",
            "forests, or stacking ensembles?\n",
            "4. What is the benefit of out-of-bag evaluation?\n",
            "4.\n",
            "5. What makes extra-trees ensembles more random than regular random forests?\n",
            "5.\n",
            "How can this extra randomness help? Are extra-trees classifiers slower or faster\n",
            "than regular random forests?\n",
            "6. If your AdaBoost ensemble underfits the training data, which hyperparameters\n",
            "6.\n",
            "should you tweak, and how?\n",
            "7. If your gradient boosting ensemble overfits the training set, should you increase\n",
            "7.\n",
            "or decrea\n",
            "\n",
            "onal\n",
            "features, including GPU acceleration; you should definitely check\n",
            "them out! Moreover, the TensorFlow Random Forests library pro‐\n",
            "vides optimized implementations of a variety of random forest\n",
            "algorithms, including plain random forests, extra-trees, GBRT, and\n",
            "several more.\n",
            "Boosting \n",
            "| \n",
            "231 18 David H. Wolpert, “Stacked Generalization”, Neural Networks 5, no. 2 (1992): 241–259.\n",
            "Stacking\n",
            "The last ensemble method we will discuss in this chapter is called stacking (short for\n",
            "stacked generalization).18 It is based on a simple idea: instead of using trivial functions\n",
            "(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\n",
            "why don’t we train a model to perform this aggregation? Figure 7-11 shows such an\n",
            "ensemble performing a regression task on a new instance. Each of the bottom three\n",
            "predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor\n",
            "(called a blender, or a meta learner) takes these predictions as inputs and makes the\n",
            "final \n",
            "\n",
            "prediction (3.0).\n",
            "Figure 7-11. Aggregating predictions using a blending predictor\n",
            "To train the blender, you first need to build the blending training set. You can\n",
            "use cross_val_predict() on every predictor in the ensemble to get out-of-sample\n",
            "predictions for each instance in the original training set (Figure 7-12), and use these\n",
            "232 \n",
            "| \n",
            "Chapter 7: Ensemble Learning and Random Forests can be used as the input features to train the blender; and the targets can simply be\n",
            "copied from the original training set. Note that regardless of the number of features\n",
            "in the original training set (just one in this example), the blending training set will\n",
            "contain one input feature per predictor (three in this example). Once the blender is\n",
            "trained, the base predictors are retrained one last time on the full original training set.\n",
            "Figure 7-12. Training the blender in a stacking ensemble\n",
            "It is actually possible to train several different blenders this way (e.g., one using linear\n",
            "regression, another using \n",
            "\n",
            "ly, the net result is that\n",
            "the ensemble has a similar bias but a lower variance than a single predictor trained on\n",
            "the original training set.\n",
            "As you can see in Figure 7-4, predictors can all be trained in parallel, via different\n",
            "CPU cores or even different servers. Similarly, predictions can be made in parallel.\n",
            "This is one of the reasons bagging and pasting are such popular methods: they scale\n",
            "very well.\n",
            "216 \n",
            "| \n",
            "Chapter 7: Ensemble Learning and Random Forests 6 max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of sampled\n",
            "instances is equal to the size of the training set times max_samples.\n",
            "Bagging and Pasting in Scikit-Learn\n",
            "Scikit-Learn offers a simple API for both bagging and pasting: BaggingClassifier\n",
            "class (or BaggingRegressor for regression). The following code trains an ensemble\n",
            "of 500 decision tree classifiers:6 each is trained on 100 training instances randomly\n",
            "sampled from the training set with replacement (this is an example of\n",
            "\n",
            "ntaining three trees. It can make predictions on a new\n",
            "instance simply by adding up the predictions of all the trees:\n",
            ">>> X_new = np.array([[-0.4], [0.], [0.5]])\n",
            ">>> sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
            "array([0.49484029, 0.04021166, 0.75026781])\n",
            "Figure 7-9 represents the predictions of these three trees in the left column, and the\n",
            "ensemble’s predictions in the right column. In the first row, the ensemble has just one\n",
            "tree, so its predictions are exactly the same as the first tree’s predictions. In the second\n",
            "row, a new tree is trained on the residual errors of the first tree. On the right you can\n",
            "see that the ensemble’s predictions are equal to the sum of the predictions of the first\n",
            "two trees. Similarly, in the third row another tree is trained on the residual errors of\n",
            "the second tree. You can see that the ensemble’s predictions gradually get better as\n",
            "trees are added to the ensemble.\n",
            "You can use Scikit-Learn’s GradientBoostingRegressor class to tra\n",
            "\n",
            "\n",
            "stacking_clf.fit(X_train, y_train)\n",
            "For each predictor, the stacking classifier will call predict_proba() if available; if not\n",
            "it will fall back to decision_function() or, as a last resort, call predict(). If you\n",
            "don’t provide a final estimator, StackingClassifier will use LogisticRegression\n",
            "and StackingRegressor will use RidgeCV.\n",
            "234 \n",
            "| \n",
            "Chapter 7: Ensemble Learning and Random Forests If you evaluate this stacking model on the test set, you will find 92.8% accuracy,\n",
            "which is a bit better than the voting classifier using soft voting, which got 92%.\n",
            "In conclusion, ensemble methods are versatile, powerful, and fairly simple to use.\n",
            "Random forests, AdaBoost, and GBRT are among the first models you should test for\n",
            "most machine learning tasks, and they particularly shine with heterogeneous tabular\n",
            "data. Moreover, as they require very little preprocessing, they’re great for getting a\n",
            "prototype up and running quickly. Lastly, ensemble methods like voting classifiers\n",
            "and stacking classifiers c\n",
            "\n",
            "random forest regression) to get a whole layer of blenders,\n",
            "and then add another blender on top of that to produce the final prediction, as shown\n",
            "in Figure 7-13. You may be able to squeeze out a few more drops of performance by\n",
            "doing this, but it will cost you in both training time and system complexity.\n",
            "Stacking \n",
            "| \n",
            "233 Figure 7-13. Predictions in a multilayer stacking ensemble\n",
            "Scikit-Learn provides two classes for stacking ensembles: StackingClassifier and\n",
            "StackingRegressor. For example, we can replace the VotingClassifier we used at\n",
            "the beginning of this chapter on the moons dataset with a StackingClassifier:\n",
            "from sklearn.ensemble import StackingClassifier\n",
            "stacking_clf = StackingClassifier(\n",
            "    estimators=[\n",
            "        ('lr', LogisticRegression(random_state=42)),\n",
            "        ('rf', RandomForestClassifier(random_state=42)),\n",
            "        ('svc', SVC(probability=True, random_state=42))\n",
            "    ],\n",
            "    final_estimator=RandomForestClassifier(random_state=43),\n",
            "    cv=5  # number of cross-validation folds\n",
            ")\n",
            "\n",
            "ven\n",
            "predictor, while others may not be sampled at all. By default a BaggingClassifier\n",
            "samples m training instances with replacement (bootstrap=True), where m is the\n",
            "size of the training set. With this process, it can be shown mathematically that only\n",
            "about 63% of the training instances are sampled on average for each predictor.7 The\n",
            "remaining 37% of the training instances that are not sampled are called out-of-bag\n",
            "(OOB) instances. Note that they are not the same 37% for all predictors.\n",
            "A bagging ensemble can be evaluated using OOB instances, without the need for\n",
            "a separate validation set: indeed, if there are enough estimators, then each instance\n",
            "in the training set will likely be an OOB instance of several estimators, so these\n",
            "estimators can be used to make a fair ensemble prediction for that instance. Once\n",
            "you have a prediction for each instance, you can compute the ensemble’s prediction\n",
            "accuracy (or any other metric).\n",
            "In Scikit-Learn, you can set oob_score=True when creating a Baggi\n",
            "\n",
            " bagging, but\n",
            "if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter\n",
            "tells Scikit-Learn the number of CPU cores to use for training and predictions, and\n",
            "–1 tells Scikit-Learn to use all available cores:\n",
            "from sklearn.ensemble import BaggingClassifier\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
            "                            max_samples=100, n_jobs=-1, random_state=42)\n",
            "bag_clf.fit(X_train, y_train)\n",
            "A BaggingClassifier automatically performs soft voting instead\n",
            "of hard voting if the base classifier can estimate class probabilities\n",
            "(i.e., if it has a predict_proba() method), which is the case with\n",
            "decision tree classifiers.\n",
            "Figure 7-5 compares the decision boundary of a single decision tree with the decision\n",
            "boundary of a bagging ensemble of 500 trees (from the preceding code), both trained\n",
            "on the moons dataset. As you can see, the ensemble’s predictions will likely generalize\n",
            "much better \n",
            "\n",
            "ad\n",
            "of instance sampling. Thus, each predictor will be trained on a random subset of the\n",
            "input features.\n",
            "This technique is particularly useful when you are dealing with high-dimensional\n",
            "inputs (such as images), as it can considerably speed up training. Sampling both\n",
            "training instances and features is called the random patches method.8 Keeping all\n",
            "training instances (by setting bootstrap=False and max_samples=1.0) but sampling\n",
            "features (by setting bootstrap_features to True and/or max_features to a value\n",
            "smaller than 1.0) is called the random subspaces method.9\n",
            "Sampling features results in even more predictor diversity, trading a bit more bias for\n",
            "a lower variance.\n",
            "Bagging and Pasting \n",
            "| \n",
            "219 10 Tin Kam Ho, “Random Decision Forests”, Proceedings of the Third International Conference on Document\n",
            "Analysis and Recognition 1 (1995): 278.\n",
            "11 The BaggingClassifier class remains useful if you want a bag of something other than decision trees.\n",
            "Random Forests\n",
            "As we have discussed, a random forest\n",
            "\n",
            "earn, xvi\n",
            "bagging and pasting in, 217\n",
            "CART algorithm, 199, 205\n",
            "cross-validation, 89-91\n",
            "design principles, 70-71\n",
            "PCA implementation, 246\n",
            "Pipeline constructor, 84-87\n",
            "sklearn.base.BaseEstimator, 80\n",
            "sklearn.base.clone(), 163\n",
            "sklearn.base.TransformerMixin, 80\n",
            "sklearn.cluster.DBSCAN, 279\n",
            "sklearn.cluster.KMeans, 81, 263\n",
            "sklearn.cluster.MiniBatchKMeans, 268\n",
            "sklearn.compose.ColumnTransformer, 85\n",
            "sklearn.compose.TransformedTargetRegres‐\n",
            "sor, 79\n",
            "sklearn.datasets.load_iris(), 167\n",
            "sklearn.datasets.make_moons(), 179\n",
            "sklearn.decomposition.IncrementalPCA,\n",
            "251\n",
            "sklearn.decomposition.PCA, 246\n",
            "sklearn.ensemble.AdaBoostClassifier, 226\n",
            "sklearn.ensemble.BaggingClassifier, 217\n",
            "sklearn.ensemble.GradientBoostingRegres‐\n",
            "sor, 227-230\n",
            "sklearn.ensemble.HistGradientBoosting‐\n",
            "Classifier, 230\n",
            "sklearn.ensemble.HistGradientBoostingRe‐\n",
            "gressor, 230\n",
            "sklearn.ensemble.RandomForestClassifier,\n",
            "117-119, 214, 220, 221, 248\n",
            "sklearn.ensemble.RandomForestRegressor,\n",
            "90, 220\n",
            "sklearn.ensemble.StackingClassifier, 234\n",
            "sklearn.ensemble.\n",
            "\n",
            "tance, you get the image represented in\n",
            "Figure 7-6.\n",
            "Figure 7-6. MNIST pixel importance (according to a random forest classifier)\n",
            "Random forests are very handy to get a quick understanding of what features actually\n",
            "matter, in particular if you need to perform feature selection.\n",
            "Boosting\n",
            "Boosting (originally called hypothesis boosting) refers to any ensemble method that\n",
            "can combine several weak learners into a strong learner. The general idea of most\n",
            "boosting methods is to train predictors sequentially, each trying to correct its prede‐\n",
            "cessor. There are many boosting methods available, but by far the most popular\n",
            "are AdaBoost13 (short for adaptive boosting) and gradient boosting. Let’s start with\n",
            "AdaBoost.\n",
            "AdaBoost\n",
            "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
            "to the training instances that the predecessor underfit. This results in new predictors\n",
            "focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
            "222 \n",
            "| \n",
            "Chapter 7: E\n",
            "\n",
            "learners in the ensemble and they are sufficiently diverse.\n",
            "How is this possible? The following analogy can help shed some light on this mystery.\n",
            "Suppose you have a slightly biased coin that has a 51% chance of coming up heads\n",
            "and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\n",
            "more or less 510 heads and 490 tails, and hence a majority of heads. If you do the\n",
            "math, you will find that the probability of obtaining a majority of heads after 1,000\n",
            "tosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,\n",
            "with 10,000 tosses, the probability climbs over 97%). This is due to the law of large\n",
            "numbers: as you keep tossing the coin, the ratio of heads gets closer and closer to\n",
            "the probability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. You\n",
            "can see that as the number of tosses increases, the ratio of heads approaches 51%.\n",
            "Eventually all 10 series end up so close to 51% that they are consistently above 50%.\n",
            "Figure \n",
            "\n",
            " learning, 692\n",
            "exponential linear unit (ELU), 363-366\n",
            "exponential scheduling, 389, 392\n",
            "export_graphviz(), 196\n",
            "extra-trees, random forest, 220\n",
            "extremely randomized trees ensemble (extra-\n",
            "trees), 221\n",
            "F\n",
            "face-recognition classifier, 125\n",
            "false negatives, confusion matrix, 109\n",
            "false positive rate (FPR) or fall-out, 115\n",
            "false positives, confusion matrix, 109\n",
            "fan-in/fan-out, 359\n",
            "fast-MCD, 293\n",
            "FCNs (fully convolutional networks), 525-526,\n",
            "532\n",
            "feature engineering, 30, 262\n",
            "feature extraction, 12, 30\n",
            "feature maps, 485-487, 488, 508, 511\n",
            "feature scaling, 75-79, 141, 176\n",
            "feature selection, 30, 95, 159, 221\n",
            "feature vectors, 45, 133, 186\n",
            "features, 11\n",
            "federated learning, 746\n",
            "feedforward neural network (FNN), 309, 538\n",
            "fetch_openml(), 104\n",
            "fillna(), 68\n",
            "filters, convolutional layers, 484, 487, 497, 509\n",
            "first moment (mean of gradient), 384\n",
            "first-order partial derivatives (Jacobians), 386\n",
            "fit()\n",
            "and custom transformers, 80, 84\n",
            "data cleaning, 69\n",
            "versus partial_fit(), 148\n",
            "using only with training set, 75\n",
            "fitnes\n",
            "\n",
            "than the single decision tree’s predictions: the ensemble has a compara‐\n",
            "ble bias but a smaller variance (it makes roughly the same number of errors on the\n",
            "training set, but the decision boundary is less irregular).\n",
            "Bagging introduces a bit more diversity in the subsets that each predictor is trained\n",
            "on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity\n",
            "also means that the predictors end up being less correlated, so the ensemble’s variance\n",
            "is reduced. Overall, bagging often results in better models, which explains why\n",
            "it’s generally preferred. But if you have spare time and CPU power, you can use\n",
            "cross-validation to evaluate both bagging and pasting and select the one that works\n",
            "best.\n",
            "Bagging and Pasting \n",
            "| \n",
            "217 7 As m grows, this ratio approaches 1 – exp(–1) ≈ 63%.\n",
            "Figure 7-5. A single decision tree (left) versus a bagging ensemble of 500 trees (right)\n",
            "Out-of-Bag Evaluation\n",
            "With bagging, some training instances may be sampled several times for any gi\n",
            "\n",
            "ameter to \"soft\", and ensure that all classifiers can estimate\n",
            "class probabilities. This is not the case for the SVC class by default, so you need\n",
            "to set its probability hyperparameter to True (this will make the SVC class use\n",
            "cross-validation to estimate class probabilities, slowing down training, and it will add\n",
            "a predict_proba() method). Let’s try that:\n",
            ">>> voting_clf.voting = \"soft\"\n",
            ">>> voting_clf.named_estimators[\"svc\"].probability = True\n",
            ">>> voting_clf.fit(X_train, y_train)\n",
            ">>> voting_clf.score(X_test, y_test)\n",
            "0.92\n",
            "We reach 92% accuracy simply by using soft voting—not bad!\n",
            "Bagging and Pasting\n",
            "One way to get a diverse set of classifiers is to use very different training algorithms,\n",
            "as just discussed. Another approach is to use the same training algorithm for every\n",
            "predictor but train them on different random subsets of the training set. When\n",
            "sampling is performed with replacement,1 this method is called bagging2 (short for\n",
            "bootstrap aggregating3). When sampling is performed withou\n",
            "\n",
            "in GBRT ensem‐\n",
            "bles more easily (there’s also a GradientBoostingClassifier class for classifica‐\n",
            "tion). Much like the RandomForestRegressor class, it has hyperparameters to\n",
            "control the growth of decision trees (e.g., max_depth, min_samples_leaf), as well\n",
            "as hyperparameters to control the ensemble training, such as the number of trees\n",
            "(n_estimators). The following code creates the same ensemble as the previous one:\n",
            "from sklearn.ensemble import GradientBoostingRegressor\n",
            "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3,\n",
            "                                 learning_rate=1.0, random_state=42)\n",
            "gbrt.fit(X, y)\n",
            "Boosting \n",
            "| \n",
            "227 Figure 7-9. In this depiction of gradient boosting, the first predictor (top left) is trained\n",
            "normally, then each consecutive predictor (middle left and lower left) is trained on the\n",
            "previous predictor’s residuals; the right column shows the resulting ensemble’s predictions\n",
            "The learning_rate hyperparameter scales the contribution of each tree. If you set\n",
            "it to a\n",
            "\n",
            "se the learning rate?\n",
            "8. Load the MNIST dataset (introduced in Chapter 3), and split it into a training\n",
            "8.\n",
            "set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000\n",
            "for validation, and 10,000 for testing). Then train various classifiers, such as a\n",
            "random forest classifier, an extra-trees classifier, and an SVM classifier. Next, try\n",
            "to combine them into an ensemble that outperforms each individual classifier\n",
            "on the validation set, using soft or hard voting. Once you have found one, try\n",
            "it on the test set. How much better does it perform compared to the individual\n",
            "classifiers?\n",
            "9. Run the individual classifiers from the previous exercise to make predictions on\n",
            "9.\n",
            "the validation set, and create a new training set with the resulting predictions:\n",
            "each training instance is a vector containing the set of predictions from all your\n",
            "classifiers for an image, and the target is the image’s class. Train a classifier\n",
            "on this new training set. Congratulations—you have just\n",
            "\n",
            "                             199\n",
            "Computational Complexity                                                                                        200\n",
            "Gini Impurity or Entropy?                                                                                         201\n",
            "Regularization Hyperparameters                                                                              201\n",
            "Regression                                                                                                                     204\n",
            "Sensitivity to Axis Orientation                                                                                   206\n",
            "Decision Trees Have a High Variance                                                                       207\n",
            "Exercises                                                                                                                        208\n",
            "7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\n",
            "Voting Clas\n",
            "\n",
            "StackingRegressor, 234\n",
            "sklearn.ensemble.VotingClassifier, 214\n",
            "sklearn.externals.joblib, 97-98\n",
            "sklearn.feature_selection.SelectFromModel,\n",
            "95\n",
            "sklearn.impute.IterativeImputer, 69\n",
            "sklearn.impute.KNNImputer, 69\n",
            "sklearn.impute.SimpleImputer, 68\n",
            "sklearn.linear_model.ElasticNet, 162\n",
            "sklearn.linear_model.Lasso, 161\n",
            "sklearn.linear_model.LinearRegression, 25,\n",
            "71, 79, 137, 138, 150\n",
            "sklearn.linear_model.LogisticRegression,\n",
            "168, 172, 275\n",
            "sklearn.linear_model.Perceptron, 307\n",
            "sklearn.linear_model.Ridge, 157\n",
            "sklearn.linear_model.RidgeCV, 158\n",
            "sklearn.linear_model.SGDClassifier, 106,\n",
            "111, 112, 117-119, 121, 183, 188, 307\n",
            "sklearn.linear_model.SGDRegressor, 147,\n",
            "163\n",
            "sklearn.manifold.LocallyLinearEmbedding,\n",
            "254\n",
            "sklearn.metrics.ConfusionMatrixDisplay,\n",
            "122\n",
            "sklearn.metrics.confusion_matrix(), 109,\n",
            "122\n",
            "sklearn.metrics.f1_score(), 111, 126\n",
            "sklearn.metrics.mean_squared_error(), 88\n",
            "sklearn.metrics.precision_recall_curve(),\n",
            "113, 117\n",
            "sklearn.metrics.precision_score(), 110\n",
            "sklearn.metrics.recall_score(), 110\n",
            "sklearn.\n",
            "\n",
            "edictions. Such models composed of many\n",
            "other models are called ensembles: they are capable of boosting the performance of\n",
            "90 \n",
            "| \n",
            "Chapter 2: End-to-End Machine Learning Project the underlying model (in this case, decision trees). The code is much the same as\n",
            "earlier:\n",
            "from sklearn.ensemble import RandomForestRegressor\n",
            "forest_reg = make_pipeline(preprocessing,\n",
            "                           RandomForestRegressor(random_state=42))\n",
            "forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n",
            "                                scoring=\"neg_root_mean_squared_error\", cv=10)\n",
            "Let’s look at the scores:\n",
            ">>> pd.Series(forest_rmses).describe()\n",
            "count       10.000000\n",
            "mean     47019.561281\n",
            "std       1033.957120\n",
            "min      45458.112527\n",
            "25%      46464.031184\n",
            "50%      46967.596354\n",
            "75%      47325.694987\n",
            "max      49243.765795\n",
            "dtype: float64\n",
            "Wow, this is much better: random forests really look very promising for this task!\n",
            "However, if you train a RandomForest and measure the RMSE on the training set,\n",
            "you wil\n",
            "\n",
            "                                                                 79\n",
            "Transformation Pipelines                                                                                          83\n",
            "Select and Train a Model                                                                                               88\n",
            "Train and Evaluate on the Training Set                                                                   88\n",
            "Better Evaluation Using Cross-Validation                                                              89\n",
            "Fine-Tune Your Model                                                                                                  91\n",
            "Grid Search                                                                                                                 91\n",
            "Randomized Search                                                                                                   93\n",
            "Ensemble Methods                                                                                                 \n",
            "\n",
            "Aurélien Géron\n",
            "Hands-On \n",
            "Machine Learning \n",
            "with Scikit-Learn, \n",
            "Keras & TensorFlow\n",
            "Concepts, Tools, and Techniques \n",
            "to Build Intelligent Systems\n",
            "Third\n",
            "Edition\n",
            "TM DATA SCIENCE / MACHINE LEARNING\n",
            "“An exceptional \n",
            "resource to study \n",
            "machine learning. You \n",
            "will find clear-minded, \n",
            "intuitive explanations, \n",
            "and a wealth of \n",
            "practical tips.” \n",
            "—François Chollet\n",
            "Author of Keras, author of \n",
            "Deep Learning with Python\n",
            "“This book is a great \n",
            "introduction to the \n",
            "theory and practice \n",
            "of solving problems \n",
            "with neural networks; \n",
            "I recommend it to \n",
            "anyone interested \n",
            "in learning about \n",
            "practical ML.” \n",
            "—Pete Warden\n",
            "Mobile Lead for TensorFlow\n",
            "Hands-On Machine Learning \n",
            "with Scikit-Learn, Keras, and TensorFlow\n",
            "ISBN: 978-1-098-12597-4\n",
            "US $79.99\t\n",
            " CAN $99.99\n",
            "Through a recent series of breakthroughs, deep learning has \n",
            "boosted the entire field of machine learning. Now, even program­\n",
            "mers who know close to nothing about this technology can use \n",
            "simple, efficient tools to implement programs capable of learning \n"
          ]
        }
      ]
    }
  ]
}