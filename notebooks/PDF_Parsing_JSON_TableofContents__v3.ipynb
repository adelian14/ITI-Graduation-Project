{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AIWNL-6VE3uw"
      },
      "outputs": [],
      "source": [
        "# !pip install google-generativeai\n",
        "# !pip install pypdf\n",
        "# !pip install pdf2image\n",
        "# !pip install pillow\n",
        "# !pip install python-dotenv\n",
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IHfPGz4vmSQ9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "from datetime import datetime\n",
        "from IPython.display import JSON\n",
        "import json\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "FOqwdovSE3oH"
      },
      "outputs": [],
      "source": [
        "# Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBcIP9qbfhrCOz41t4udX8RjxplyOrZa3U\"\n",
        "\n",
        "# Configure genai with the API key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Load Gemini model\n",
        "model = genai.GenerativeModel(\n",
        "    \"gemini-2.5-flash\", #\"gemini-2.5-flash-preview-04-17\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ocq6nF5KE3lO"
      },
      "outputs": [],
      "source": [
        "def convert_pdf_to_images(pdf_path, output_folder, dpi=300):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(pdf_path, dpi=dpi)\n",
        "\n",
        "    # Save images to the output folder\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        image_path = os.path.join(output_folder, f'page_{i+1}.jpg')\n",
        "        image.save(image_path, 'JPEG')\n",
        "        image_paths.append(image_path)\n",
        "\n",
        "    return image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-wLBkweGHLEx"
      },
      "outputs": [],
      "source": [
        "pdf_path = '/content/Machine Learning Fundamentals.pdf'\n",
        "output_folder = '/content/images'\n",
        "# image_paths = convert_pdf_to_images(pdf_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /content/images"
      ],
      "metadata": {
        "id": "ru_yVt1Iq02_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6hv8lZ09E3iH"
      },
      "outputs": [],
      "source": [
        "def batch_images(image_paths, batch_size=10):\n",
        "    \"\"\"Group images into batches for processing\"\"\"\n",
        "    for i in range(0, len(image_paths), batch_size):\n",
        "        yield image_paths[i:i + batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "75tmxDbTE3ff"
      },
      "outputs": [],
      "source": [
        "def ocr_with_gemini(image_paths, instruction):\n",
        "    images = [Image.open(path) for path in image_paths]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "    You are an expert document analysis AI with exceptional OCR capabilities.\n",
        "    Your task is to extract ALL textual content from the provided document images with perfect accuracy.\n",
        "\n",
        "    CRITICAL REQUIREMENTS:\n",
        "    1. ACCURACY: Every word, number, symbol, and punctuation mark must be captured exactly as shown\n",
        "    2. STRUCTURE: Maintain the original document structure, hierarchy, and formatting\n",
        "    3. COMPLETENESS: Do not skip any content, including headers, footers, page numbers, footnotes, watermarks, or marginalia\n",
        "    4. CONTEXT: Understand the document context to resolve ambiguous characters\n",
        "\n",
        "    {instruction}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = model.generate_content([prompt, *images])\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9y0_Idq_E3ca"
      },
      "outputs": [],
      "source": [
        "def ocr_complex_document(image_paths):\n",
        "    instruction = \"\"\"\n",
        "    Extract ALL text content from these document pages.\n",
        "    For Tables:\n",
        "    - Use markdown table format with proper alignment\n",
        "    - Include all headers, subheaders, and merged cells\n",
        "    - Preserve numerical precision and units\n",
        "    - Note any table notes or footnotes\n",
        "\n",
        "    For Multi-column Text:\n",
        "    - Process columns in natural reading order (left to right, top to bottom)\n",
        "    - Clearly separate column content with appropriate breaks\n",
        "    - Maintain column-specific formatting\n",
        "\n",
        "    For Charts/Graphs:\n",
        "    - Describe chart type and purpose\n",
        "    - Extract all axis labels, legends, and data points\n",
        "    - Capture titles, captions, and source information\n",
        "    - Note any trends or key insights visible in the visual\n",
        "\n",
        "    For Special Elements:\n",
        "    - Preserve bullet points, numbered lists, and indentation\n",
        "    - Maintain emphasis (bold, italic, underline) using markdown\n",
        "    - Capture all hyperlinks and cross-references\n",
        "    - Include page numbers and section breaks\n",
        "\n",
        "    QUALITY ASSURANCE:\n",
        "    - Double-check all numerical data for accuracy\n",
        "    - Verify proper names, technical terms, and specialized vocabulary\n",
        "    - Ensure logical flow and coherence in extracted text\n",
        "    - Flag any unclear or potentially misread content with [UNCERTAIN: text]\n",
        "    Preserve all headers, footers, page numbers, and footnotes.\n",
        "    \"\"\"\n",
        "\n",
        "    return ocr_with_gemini(image_paths, instruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bNYiZFaHE3W_"
      },
      "outputs": [],
      "source": [
        "def process_large_pdf(pdf_path, output_folder):\n",
        "    # Convert PDF to images\n",
        "    image_paths = convert_pdf_to_images(pdf_path, output_folder)\n",
        "\n",
        "    # Create batches of images (e.g., by chapter or section)\n",
        "    batches = batch_images(image_paths, 30)\n",
        "\n",
        "    full_text = \"\"\n",
        "    for i, batch in enumerate(batches):\n",
        "        print(f\"Processing batch {i+1}...\")\n",
        "        batch_text = ocr_with_gemini(batch, \"Extract all text, maintaining document structure\")\n",
        "        full_text += f\"\\n\\n--- BATCH {i+1} ---\\n\\n{batch_text}\"\n",
        "\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "bfVSXatimSRA",
        "outputId": "12b74c3a-ef66-49eb-d194-89cd54066a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1...\n"
          ]
        }
      ],
      "source": [
        "text = process_large_pdf(pdf_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "1N2Amq2vLxxw"
      },
      "outputs": [],
      "source": [
        "def text_to_json_with_gemini(text_content):\n",
        "  \"\"\"Converts text content to JSON format using the Gemini model.\"\"\"\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    You are a document structuring expert. Convert the provided text into a comprehensive, well-organized JSON structure.\n",
        "\n",
        "    REQUIREMENTS:\n",
        "    1. Create a logical, hierarchical JSON structure that reflects the document's organization\n",
        "    2. Preserve all content while organizing it meaningfully\n",
        "    3. Include metadata about the document structure and content\n",
        "\n",
        "  Convert the following text content into a structured JSON object.\n",
        "  Identify key sections, headings, paragraphs, tables, and other elements\n",
        "  and represent them appropriately in the JSON structure.\n",
        "\n",
        "\n",
        "  Text content to convert:\n",
        "\n",
        "  {}\n",
        "\n",
        "    JSON STRUCTURE TEMPLATE:\n",
        "    {{\n",
        "        \"document_metadata\": {{\n",
        "            \"title\": \"Document title if available\",\n",
        "            \"subsection number\": \"Subsection number if available\",\n",
        "            \"document_type\": \"academic|technical|report|manual|other\",\n",
        "            \"total_pages\": \"number of pages processed\",\n",
        "            \"language\": \"primary language detected\",\n",
        "            \"has_tables\": true/false,\n",
        "            \"has_charts\": true/false,\n",
        "            \"has_images\": true/false,\n",
        "\n",
        "        }},\n",
        "        \"document_structure\": {{\n",
        "          \"chapters\": [\n",
        "            {{\n",
        "            \"Chapter title\": \"Chapter title if available\",\n",
        "            \"Chapter number\": \"Chapter number if available\",\n",
        "            \"sections\": [\n",
        "                {{\n",
        "                    \"section_number\": \"1\",\n",
        "                    \"title\": \"Section Title\",\n",
        "                    \"subsections\": [\n",
        "                        {{\n",
        "                            \"subsection_number\": \"1.1\",\n",
        "                            \"title\": \"Subsection Title\",\n",
        "                            \"Chapter title\": \"Chapter title if available\",\n",
        "                            \"Chapter number\": \"Chapter number if available\",\n",
        "                            \"Full content\": \"Full text content from this section must inculdeed and type of each one\",\n",
        "                            \"paragraphs\": [\"paragraph 1\", \"paragraph 2\"],\n",
        "                            \"lists\": [\"list items if any\"],\n",
        "                            \"tables\": [\n",
        "                                {{\n",
        "                                    \"table_number\": \"Table 1\",\n",
        "                                    \"caption\": \"Table caption\",\n",
        "                                    \"headers\": [\"Column 1\", \"Column 2\"],\n",
        "                                    \"rows\": [[\"Data 1\", \"Data 2\"]]\n",
        "                                }}\n",
        "                            ],\n",
        "                            \"figures\": [\n",
        "                                {{\n",
        "                                    \"figure_number\": \"Figure 1\",\n",
        "                                    \"caption\": \"Figure caption\",\n",
        "                                    \"description\": \"Description of visual content\"\n",
        "                                }}\n",
        "                            ]\n",
        "                        }}\n",
        "                    ]\n",
        "                    }}\n",
        "                    ]\n",
        "                }}\n",
        "            ]\n",
        "        }},\n",
        "    }}\n",
        "\n",
        "    ADAPTATION RULES:\n",
        "    - If document doesn't have clear sections, organize by pages or logical breaks\n",
        "    - For tables without clear structure, preserve as formatted text\n",
        "    - For charts/graphs, include detailed descriptions in figures array\n",
        "    - Adapt structure to match document type (academic papers, reports, manuals, etc.)\n",
        "    - Ensure all content is preserved even if structure is unclear\n",
        "\n",
        "    OUTPUT: Valid JSON only, no additional text or explanations. and don't add ```json```\n",
        "  \"\"\".format(text_content)\n",
        "\n",
        "  response = model.generate_content([prompt])\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "yQJVL6X2L1Xb"
      },
      "outputs": [],
      "source": [
        "# Convert the extracted text to JSON\n",
        "json_output = text_to_json_with_gemini(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(json_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCGr3h2VsElm",
        "outputId": "9f14d0da-7ece-48f5-8854-61a93ba0e60d"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"document_metadata\": {\n",
            "        \"title\": \"Machine Learning Fundamentals\",\n",
            "        \"document_type\": \"technical\",\n",
            "        \"total_pages\": 1,\n",
            "        \"language\": \"en\",\n",
            "        \"has_tables\": false,\n",
            "        \"has_charts\": false,\n",
            "        \"has_images\": false\n",
            "    },\n",
            "    \"document_structure\": {\n",
            "        \"chapters\": [\n",
            "            {\n",
            "                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                \"Chapter number\": \"1\",\n",
            "                \"sections\": [\n",
            "                    {\n",
            "                        \"section_number\": \"1.1\",\n",
            "                        \"title\": \"Introduction to Machine Learning\",\n",
            "                        \"subsections\": [\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.1.1.1\",\n",
            "                                \"title\": \"Definition and Scope\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Machine Learning (ML) is a subset of artificial intelligence that enables\\ncomputers to learn and improve from experience without being explicitly\\nprogrammed. It involves the development of algorithms and statistical\\nmodels that computer systems use to perform specific tasks effectively\\nwithout using explicit instructions.\\n\\nThe core principle of machine learning is to build mathematical models\\nbased on training data to make predictions or decisions without being\\nexplicitly programmed for each scenario. This paradigm shift from traditional\\nprogramming approaches allows systems to adapt and improve their\\nperformance over time.\\n\\nNote: Machine learning is fundamentally different from traditional\\nprogramming where we write explicit rules. In ML, we provide data and\\nexpected outputs, and the algorithm learns the rules.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It involves the development of algorithms and statistical models that computer systems use to perform specific tasks effectively without using explicit instructions.\",\n",
            "                                    \"The core principle of machine learning is to build mathematical models based on training data to make predictions or decisions without being explicitly programmed for each scenario. This paradigm shift from traditional programming approaches allows systems to adapt and improve their performance over time.\",\n",
            "                                    \"Note: Machine learning is fundamentally different from traditional programming where we write explicit rules. In ML, we provide data and expected outputs, and the algorithm learns the rules.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.1.1.2\",\n",
            "                                \"title\": \"Historical Context\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"The concept of machine learning has its roots in the 1940s and 1950s, with\\nearly pioneers like Alan Turing proposing the idea of machines that could\\nlearn. The field has evolved through several key periods:\\n\\nThe symbolic AI period (1950s-1980s) focused on rule-based systems and\\nexpert systems. The connectionist revival (1980s-1990s) brought neural\\nnetworks back into prominence. The statistical learning era (1990s-2000s)\\nemphasized probabilistic models and support vector machines.\\n\\nThe current deep learning revolution (2010s-present) has been enabled by\\nincreased computational power, large datasets, and improved algorithms,\\nleading to breakthroughs in computer vision, natural language processing,\\nand other domains.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"The concept of machine learning has its roots in the 1940s and 1950s, with early pioneers like Alan Turing proposing the idea of machines that could learn. The field has evolved through several key periods:\",\n",
            "                                    \"The symbolic AI period (1950s-1980s) focused on rule-based systems and expert systems. The connectionist revival (1980s-1990s) brought neural networks back into prominence. The statistical learning era (1990s-2000s) emphasized probabilistic models and support vector machines.\",\n",
            "                                    \"The current deep learning revolution (2010s-present) has been enabled by increased computational power, large datasets, and improved algorithms, leading to breakthroughs in computer vision, natural language processing, and other domains.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.1.1.3\",\n",
            "                                \"title\": \"Modern Applications\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Machine learning has found applications across numerous industries and\\ndomains. In healthcare, ML algorithms assist in medical diagnosis, drug\\ndiscovery, and personalized treatment plans. Financial services use ML for\\nfraud detection, algorithmic trading, and risk assessment.\\n\\nTechnology companies leverage ML for recommendation systems, search\\nengines, and autonomous vehicles. Other applications include natural\\nlanguage processing for chatbots and translation services, computer vision\\nfor image recognition and analysis, and predictive maintenance in\\nmanufacturing.\\n\\nImportant: The success of ML applications depends heavily on data quality,\\nappropriate algorithm selection, and proper evaluation metrics.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Machine learning has found applications across numerous industries and domains. In healthcare, ML algorithms assist in medical diagnosis, drug discovery, and personalized treatment plans. Financial services use ML for fraud detection, algorithmic trading, and risk assessment.\",\n",
            "                                    \"Technology companies leverage ML for recommendation systems, search engines, and autonomous vehicles. Other applications include natural language processing for chatbots and translation services, computer vision for image recognition and analysis, and predictive maintenance in manufacturing.\",\n",
            "                                    \"Important: The success of ML applications depends heavily on data quality, appropriate algorithm selection, and proper evaluation metrics.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.1.2.1\",\n",
            "                                \"title\": \"Supervised Learning\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Supervised learning involves training algorithms on labeled datasets where\\nboth input features and corresponding output labels are provided. The goal is\\nto learn a mapping function from inputs to outputs that can generalize to\\nnew, unseen data.\\n\\nClassification tasks involve predicting discrete categorical labels, such as\\nemail spam detection or image recognition. Regression tasks involve\\npredicting continuous numerical values, such as house prices or stock\\nmarket predictions.\\n\\n# Example of supervised learning structure\\nX_train = [[feature1, feature2, ...],\\n...] # Training features\\ny_train = [label1, label2, ...] # Training labels\\nmodel.fit(X_train, y_train) # Learn from data\\npredictions = model.predict(X_test) # Make predictions\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Supervised learning involves training algorithms on labeled datasets where both input features and corresponding output labels are provided. The goal is to learn a mapping function from inputs to outputs that can generalize to new, unseen data.\",\n",
            "                                    \"Classification tasks involve predicting discrete categorical labels, such as email spam detection or image recognition. Regression tasks involve predicting continuous numerical values, such as house prices or stock market predictions.\",\n",
            "                                    \"# Example of supervised learning structure\\nX_train = [[feature1, feature2, ...],\\n...] # Training features\\ny_train = [label1, label2, ...] # Training labels\\nmodel.fit(X_train, y_train) # Learn from data\\npredictions = model.predict(X_test) # Make predictions\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.1.2.2\",\n",
            "                                \"title\": \"Unsupervised Learning\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Unsupervised learning works with unlabeled data to discover hidden\\npatterns, structures, or relationships within the dataset. Without explicit\\ntarget variables, these algorithms must identify meaningful patterns\\nindependently.\\n\\nCommon unsupervised learning tasks include clustering (grouping similar\\ndata points), dimensionality reduction (reducing the number of features\\nwhile preserving important information), and anomaly detection (identifying\\noutliers or unusual patterns).\\nApplications include customer segmentation, data compression, and\\nexploratory data analysis. These techniques are particularly valuable for\\nunderstanding complex datasets and preprocessing data for supervised\\nlearning tasks.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Unsupervised learning works with unlabeled data to discover hidden patterns, structures, or relationships within the dataset. Without explicit target variables, these algorithms must identify meaningful patterns independently.\",\n",
            "                                    \"Common unsupervised learning tasks include clustering (grouping similar data points), dimensionality reduction (reducing the number of features while preserving important information), and anomaly detection (identifying outliers or unusual patterns).\\nApplications include customer segmentation, data compression, and exploratory data analysis. These techniques are particularly valuable for understanding complex datasets and preprocessing data for supervised learning tasks.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.1.2.3\",\n",
            "                                \"title\": \"Reinforcement Learning\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Reinforcement learning involves training agents to make sequential\\ndecisions in an environment to maximize cumulative rewards. The agent\\nlearns through interaction with the environment, receiving feedback in the\\nform of rewards or penalties.\\n\\nKey components include the agent (decision maker), environment (world in\\nwhich the agent operates), actions (choices available to the agent), states\\n(current situation), and rewards (feedback signal). The agent learns an\\noptimal policy that maps states to actions.\\n\\nApplications include game playing (chess, Go), autonomous navigation,\\nrobotics, and resource allocation. Notable successes include AlphaGo,\\nautonomous vehicles, and recommendation systems that adapt to user\\npreferences over time.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Reinforcement learning involves training agents to make sequential decisions in an environment to maximize cumulative rewards. The agent learns through interaction with the environment, receiving feedback in the form of rewards or penalties.\",\n",
            "                                    \"Key components include the agent (decision maker), environment (world in which the agent operates), actions (choices available to the agent), states (current situation), and rewards (feedback signal). The agent learns an optimal policy that maps states to actions.\",\n",
            "                                    \"Applications include game playing (chess, Go), autonomous navigation, robotics, and resource allocation. Notable successes include AlphaGo, autonomous vehicles, and recommendation systems that adapt to user preferences over time.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            }\n",
            "                        ]\n",
            "                    },\n",
            "                    {\n",
            "                        \"section_number\": \"1.2\",\n",
            "                        \"title\": \"Mathematical Foundations\",\n",
            "                        \"subsections\": [\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.2.1.1\",\n",
            "                                \"title\": \"Vectors and Matrices\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Vectors and matrices are fundamental mathematical structures in machine\\nlearning. A vector is an ordered collection of numbers, representing features\\nor data points in n-dimensional space. Matrices are rectangular arrays of\\nnumbers, often used to represent datasets or transformation operations.\\n\\nVector operations include addition, scalar multiplication, and dot products.\\nMatrix operations include multiplication, transposition, and inversion. These\\noperations form the basis for many machine learning algorithms.\\n\\nVector dot product: a $\\\\cdot$ b = $\\\\Sigma$(a$_{i}$ $\\\\times$ b$_{i}$)\\nMatrix multiplication: C = A $\\\\times$ B, where C$_{ij}$ = $\\\\Sigma$(A$_{ik}$ $\\\\times$ B$_{kj}$)\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Vectors and matrices are fundamental mathematical structures in machine learning. A vector is an ordered collection of numbers, representing features or data points in n-dimensional space. Matrices are rectangular arrays of numbers, often used to represent datasets or transformation operations.\",\n",
            "                                    \"Vector operations include addition, scalar multiplication, and dot products. Matrix operations include multiplication, transposition, and inversion. These operations form the basis for many machine learning algorithms.\",\n",
            "                                    \"Vector dot product: a $\\\\cdot$ b = $\\\\Sigma$(a$_{i}$ $\\\\times$ b$_{i}$)\",\n",
            "                                    \"Matrix multiplication: C = A $\\\\times$ B, where C$_{ij}$ = $\\\\Sigma$(A$_{ik}$ $\\\\times$ B$_{kj}$)\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.2.1.2\",\n",
            "                                \"title\": \"Eigenvalues and Eigenvectors\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Eigenvalues and eigenvectors are crucial concepts in linear algebra with\\nsignificant applications in machine learning. An eigenvector of a matrix A is a\\nnon-zero vector v such that Av = $\\\\lambda$v, where $\\\\lambda$ is the corresponding eigenvalue.\\n\\nThese concepts are fundamental to dimensionality reduction techniques like\\nPrincipal Component Analysis (PCA), where eigenvectors represent principal\\ncomponents and eigenvalues indicate their importance. They also play roles\\nin spectral clustering and graph-based algorithms.\\n\\nComputing eigenvalues and eigenvectors helps understand the intrinsic\\nproperties of data transformations and can reveal important structural\\ninformation about datasets.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Eigenvalues and eigenvectors are crucial concepts in linear algebra with significant applications in machine learning. An eigenvector of a matrix A is a non-zero vector v such that Av = $\\\\lambda$v, where $\\\\lambda$ is the corresponding eigenvalue.\",\n",
            "                                    \"These concepts are fundamental to dimensionality reduction techniques like Principal Component Analysis (PCA), where eigenvectors represent principal components and eigenvalues indicate their importance. They also play roles in spectral clustering and graph-based algorithms.\",\n",
            "                                    \"Computing eigenvalues and eigenvectors helps understand the intrinsic properties of data transformations and can reveal important structural information about datasets.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.2.2.1\",\n",
            "                                \"title\": \"Probability Distributions\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Probability distributions describe how probabilities are distributed over possible\\noutcomes of a random variable. Understanding distributions is essential for\\nmodeling uncertainty in machine learning systems.\\n\\nCommon distributions include the normal (Gaussian) distribution for continuous\\nvariables, binomial distribution for binary outcomes, and Poisson distribution for\\ncount data. Each distribution has specific parameters that characterize its shape\\nand properties.\\n\\nMachine learning algorithms often make assumptions about data distributions. For\\nexample, linear regression assumes normally distributed residuals, while Naive\\nBayes assumes conditional independence of features given the class.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Probability distributions describe how probabilities are distributed over possible outcomes of a random variable. Understanding distributions is essential for modeling uncertainty in machine learning systems.\",\n",
            "                                    \"Common distributions include the normal (Gaussian) distribution for continuous variables, binomial distribution for binary outcomes, and Poisson distribution for count data. Each distribution has specific parameters that characterize its shape and properties.\",\n",
            "                                    \"Machine learning algorithms often make assumptions about data distributions. For example, linear regression assumes normally distributed residuals, while Naive Bayes assumes conditional independence of features given the class.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.2.2.2\",\n",
            "                                \"title\": \"Bayes' Theorem\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Bayes' theorem is a fundamental principle in probability theory that describes how\\nto update beliefs based on new evidence. It forms the foundation for Bayesian\\nmachine learning approaches.\\n\\nP(A|B) = P(B|A) $\\\\times$ P(A) / P(B)\\n\\nIn machine learning contexts, Bayes' theorem is used in classification algorithms\\nlike Naive Bayes, Bayesian networks, and Bayesian optimization. It provides a\\nprincipled way to incorporate prior knowledge and update predictions as new data\\nbecomes available.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Bayes' theorem is a fundamental principle in probability theory that describes how to update beliefs based on new evidence. It forms the foundation for Bayesian machine learning approaches.\",\n",
            "                                    \"P(A|B) = P(B|A) $\\\\times$ P(A) / P(B)\",\n",
            "                                    \"In machine learning contexts, Bayes' theorem is used in classification algorithms like Naive Bayes, Bayesian networks, and Bayesian optimization. It provides a principled way to incorporate prior knowledge and update predictions as new data becomes available.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            }\n",
            "                        ]\n",
            "                    },\n",
            "                    {\n",
            "                        \"section_number\": \"1.3\",\n",
            "                        \"title\": \"Data Processing and Feature Engineering\",\n",
            "                        \"subsections\": [\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.3.1.1\",\n",
            "                                \"title\": \"Data Cleaning\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Data cleaning is the process of identifying and correcting errors,\\ninconsistencies, and inaccuracies in datasets. Real-world data often\\ncontains noise, outliers, duplicate records, and formatting issues that can\\nnegatively impact model performance.\\n\\nCommon data cleaning tasks include removing duplicates, handling\\ninconsistent formatting, correcting typos, and dealing with outliers.\\nAutomated tools and manual inspection are often combined to ensure data\\nquality.\\n\\n# Example data cleaning operations\\ndf = df.drop_duplicates() # Remove duplicate rows\\ndf['column'] = df['column'].str.lower() # Standardize text case\\ndf = df[df['value'] < threshold] # Remove outliers\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Data cleaning is the process of identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Real-world data often contains noise, outliers, duplicate records, and formatting issues that can negatively impact model performance.\",\n",
            "                                    \"Common data cleaning tasks include removing duplicates, handling inconsistent formatting, correcting typos, and dealing with outliers. Automated tools and manual inspection are often combined to ensure data quality.\",\n",
            "                                    \"# Example data cleaning operations\\ndf = df.drop_duplicates() # Remove duplicate rows\\ndf['column'] = df['column'].str.lower() # Standardize text case\\ndf = df[df['value'] < threshold] # Remove outliers\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.3.1.2\",\n",
            "                                \"title\": \"Missing Data Handling\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Missing data is a common challenge in machine learning projects. Different\\nstrategies exist for handling missing values, including deletion (removing\\nrows or columns with missing values), imputation (filling in missing values),\\nand using algorithms that can handle missing data directly.\\n\\nImputation methods include mean/median/mode imputation for simple\\ncases, regression imputation for more sophisticated approaches, and\\nmultiple imputation for maintaining uncertainty estimates. The choice\\ndepends on the nature of the missing data and the specific application.\\n\\nUnderstanding whether data is missing completely at random (MCAR),\\nmissing at random (MAR), or missing not at random (MNAR) is crucial for\\nchoosing appropriate handling strategies.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Missing data is a common challenge in machine learning projects. Different strategies exist for handling missing values, including deletion (removing rows or columns with missing values), imputation (filling in missing values), and using algorithms that can handle missing data directly.\",\n",
            "                                    \"Imputation methods include mean/median/mode imputation for simple cases, regression imputation for more sophisticated approaches, and multiple imputation for maintaining uncertainty estimates. The choice depends on the nature of the missing data and the specific application.\",\n",
            "                                    \"Understanding whether data is missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR) is crucial for choosing appropriate handling strategies.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.3.2.1\",\n",
            "                                \"title\": \"Dimensionality Reduction\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Dimensionality reduction techniques aim to reduce the number of features while\\npreserving the most important information. This helps combat the curse of\\ndimensionality, improves computational efficiency, and can enhance model\\ninterpretability.\\nPrincipal Component Analysis (PCA) is a popular linear dimensionality reduction\\ntechnique that finds orthogonal projections that maximize variance. Non-linear\\nmethods like t-SNE and UMAP are useful for visualization and capturing complex\\ndata structures.\\n\\nFeature selection methods include filter methods (based on statistical measures),\\nwrapper methods (using model performance), and embedded methods (integrated\\ninto the learning algorithm).\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Dimensionality reduction techniques aim to reduce the number of features while preserving the most important information. This helps combat the curse of dimensionality, improves computational efficiency, and can enhance model interpretability.\",\n",
            "                                    \"Principal Component Analysis (PCA) is a popular linear dimensionality reduction technique that finds orthogonal projections that maximize variance. Non-linear methods like t-SNE and UMAP are useful for visualization and capturing complex data structures.\",\n",
            "                                    \"Feature selection methods include filter methods (based on statistical measures), wrapper methods (using model performance), and embedded methods (integrated into the learning algorithm).\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"1.3.2.2\",\n",
            "                                \"title\": \"Feature Scaling\",\n",
            "                                \"Chapter title\": \"Foundations of Machine Learning\",\n",
            "                                \"Chapter number\": \"1\",\n",
            "                                \"Full content\": \"Feature scaling ensures that all features contribute equally to the learning process,\\npreventing features with larger scales from dominating the algorithm. Different\\nscaling methods are appropriate for different situations.\\n\\nMin-max scaling transforms features to a fixed range (typically 0-1), while\\nstandardization (z-score normalization) centers features around zero with unit\\nvariance. Robust scaling uses median and interquartile range to handle outliers\\nbetter.\\n\\nMin-max scaling: X' = (X - min(X)) / (max(X) - min(X))\\nStandardization: X' = (X - $\\\\mu$) / $\\\\sigma$\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Feature scaling ensures that all features contribute equally to the learning process, preventing features with larger scales from dominating the algorithm. Different scaling methods are appropriate for different situations.\",\n",
            "                                    \"Min-max scaling transforms features to a fixed range (typically 0-1), while standardization (z-score normalization) centers features around zero with unit variance. Robust scaling uses median and interquartile range to handle outliers better.\",\n",
            "                                    \"Min-max scaling: X' = (X - min(X)) / (max(X) - min(X))\",\n",
            "                                    \"Standardization: X' = (X - $\\\\mu$) / $\\\\sigma$\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            }\n",
            "                        ]\n",
            "                    }\n",
            "                ]\n",
            "            },\n",
            "            {\n",
            "                \"Chapter title\": \"Core Machine Learning Algorithms\",\n",
            "                \"Chapter number\": \"2\",\n",
            "                \"sections\": [\n",
            "                    {\n",
            "                        \"section_number\": \"2.1\",\n",
            "                        \"title\": \"Supervised Learning Algorithms\",\n",
            "                        \"subsections\": [\n",
            "                            {\n",
            "                                \"subsection_number\": \"2.1.1.1\",\n",
            "                                \"title\": \"Linear Regression\",\n",
            "                                \"Chapter title\": \"Core Machine Learning Algorithms\",\n",
            "                                \"Chapter number\": \"2\",\n",
            "                                \"Full content\": \"Linear regression is a fundamental supervised learning algorithm that\\nmodels the relationship between a dependent variable and independent\\nvariables by fitting a linear equation to the observed data. It assumes a linear\\nrelationship between features and the target variable.\\n\\nThe algorithm finds the best-fitting line through the data points by minimizing\\nthe sum of squared residuals. The resulting model can be expressed as y = $\\\\beta$$_{0}$\\n+ $\\\\beta$$_{1}$x$_{1}$ + $\\\\beta$$_{2}$x$_{2}$ + ... + $\\\\beta$$_{n}$x$_{n}$, where $\\\\beta$ coefficients represent the weights of each\\nfeature.\\n\\n# Linear regression implementation concept\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n# Create and train model\\nmodel = LinearRegression().fit(X_train, y_train)\\npredictions = model.predict(X_test) # Make predictions\\n\\nLinear regression is interpretable, computationally efficient, and serves as a\\nbaseline for many problems. However, it assumes linearity and can be\\nsensitive to outliers and multicollinearity.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Linear regression is a fundamental supervised learning algorithm that models the relationship between a dependent variable and independent variables by fitting a linear equation to the observed data. It assumes a linear relationship between features and the target variable.\",\n",
            "                                    \"The algorithm finds the best-fitting line through the data points by minimizing the sum of squared residuals. The resulting model can be expressed as y = $\\\\beta$$_{0}$ + $\\\\beta$$_{1}$x$_{1}$ + $\\\\beta$$_{2}$x$_{2}$ + ... + $\\\\beta$$_{n}$x$_{n}$, where $\\\\beta$ coefficients represent the weights of each feature.\",\n",
            "                                    \"# Linear regression implementation concept\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n# Create and train model\\nmodel = LinearRegression().fit(X_train, y_train)\\npredictions = model.predict(X_test) # Make predictions\",\n",
            "                                    \"Linear regression is interpretable, computationally efficient, and serves as a baseline for many problems. However, it assumes linearity and can be sensitive to outliers and multicollinearity.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"2.1.1.2\",\n",
            "                                \"title\": \"Logistic Regression\",\n",
            "                                \"Chapter title\": \"Core Machine Learning Algorithms\",\n",
            "                                \"Chapter number\": \"2\",\n",
            "                                \"Full content\": \"Logistic regression extends linear regression to classification problems by\\nusing the logistic function to model the probability of class membership.\\nDespite its name, it's a classification algorithm that outputs probabilities\\nbetween 0 and 1.\\n\\nThe logistic function (sigmoid) transforms the linear combination of features\\ninto a probability: P(y=1|x) = 1 / (1 + e$^{-(}\\\\beta$$_{0}$ + $\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\beta$$_{n}$x$_{n}^{)}$)). This\\nensures outputs are always between 0 and 1.\\n\\nLogistic function: $\\\\sigma$(z) = 1 / (1 + e$^{-z}$)\\nLog-odds: ln(p/(1-p)) = $\\\\beta$$_{0}$ + $\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\beta$$_{n}$x$_{n}$\\n\\nLogistic regression is widely used for binary classification and can be\\nextended to multi-class problems. It provides probabilistic outputs and\\ncoefficient interpretability, making it valuable for many applications.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Logistic regression extends linear regression to classification problems by using the logistic function to model the probability of class membership. Despite its name, it's a classification algorithm that outputs probabilities between 0 and 1.\",\n",
            "                                    \"The logistic function (sigmoid) transforms the linear combination of features into a probability: P(y=1|x) = 1 / (1 + e$^{-(}\\\\beta$$_{0}$ + $\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\beta$$_{n}$x$_{n}^{)}$)). This ensures outputs are always between 0 and 1.\",\n",
            "                                    \"Logistic function: $\\\\sigma$(z) = 1 / (1 + e$^{-z}$)\",\n",
            "                                    \"Log-odds: ln(p/(1-p)) = $\\\\beta$$_{0}$ + $\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\beta$$_{n}$x$_{n}$\",\n",
            "                                    \"Logistic regression is widely used for binary classification and can be extended to multi-class problems. It provides probabilistic outputs and coefficient interpretability, making it valuable for many applications.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            },\n",
            "                            {\n",
            "                                \"subsection_number\": \"2.1.1.3\",\n",
            "                                \"title\": \"Regularization Techniques\",\n",
            "                                \"Chapter title\": \"Core Machine Learning Algorithms\",\n",
            "                                \"Chapter number\": \"2\",\n",
            "                                \"Full content\": \"Regularization techniques prevent overfitting by adding penalty terms to the\\nloss function. They constrain model complexity and improve generalization\\nto unseen data, especially important when dealing with high-dimensional\\ndatasets.\\n\\nL1 regularization (Lasso) adds the sum of absolute values of coefficients as a\\npenalty, promoting sparsity and automatic feature selection. L2\\nregularization (Ridge) adds the sum of squared coefficients, shrinking\\ncoefficients towards zero but keeping all features.\\n\\nElastic Net combines L1 and L2 regularization.\",\n",
            "                                \"paragraphs\": [\n",
            "                                    \"Regularization techniques prevent overfitting by adding penalty terms to the loss function. They constrain model complexity and improve generalization to unseen data, especially important when dealing with high-dimensional datasets.\",\n",
            "                                    \"L1 regularization (Lasso) adds the sum of absolute values of coefficients as a penalty, promoting sparsity and automatic feature selection. L2 regularization (Ridge) adds the sum of squared coefficients, shrinking coefficients towards zero but keeping all features.\",\n",
            "                                    \"Elastic Net combines L1 and L2 regularization.\"\n",
            "                                ],\n",
            "                                \"lists\": [],\n",
            "                                \"tables\": [],\n",
            "                                \"figures\": []\n",
            "                            }\n",
            "                        ]\n",
            "                    }\n",
            "                ]\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------"
      ],
      "metadata": {
        "id": "RoLIlwkD3E9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_json_content = re.sub(r'(?<!\\\\)\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', json_output)"
      ],
      "metadata": {
        "id": "zdYl9BSg609e"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove triple backticks and optional \"json\"\n",
        "cleaned_json_content = cleaned_json_content.strip().removeprefix(\"```json\").removeprefix(\"```\").removesuffix(\"```\").strip()"
      ],
      "metadata": {
        "id": "Fwoc6MA-86ha"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_json_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "NhoGTVc08nMG",
        "outputId": "d33b6706-5932-466e-babc-343e922190bc"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n    \"document_metadata\": {\\n        \"title\": \"Machine Learning Fundamentals\",\\n        \"document_type\": \"technical\",\\n        \"total_pages\": 1,\\n        \"language\": \"en\",\\n        \"has_tables\": false,\\n        \"has_charts\": false,\\n        \"has_images\": false\\n    },\\n    \"document_structure\": {\\n        \"chapters\": [\\n            {\\n                \"Chapter title\": \"Foundations of Machine Learning\",\\n                \"Chapter number\": \"1\",\\n                \"sections\": [\\n                    {\\n                        \"section_number\": \"1.1\",\\n                        \"title\": \"Introduction to Machine Learning\",\\n                        \"subsections\": [\\n                            {\\n                                \"subsection_number\": \"1.1.1.1\",\\n                                \"title\": \"Definition and Scope\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Machine Learning (ML) is a subset of artificial intelligence that enables\\\\ncomputers to learn and improve from experience without being explicitly\\\\nprogrammed. It involves the development of algorithms and statistical\\\\nmodels that computer systems use to perform specific tasks effectively\\\\nwithout using explicit instructions.\\\\n\\\\nThe core principle of machine learning is to build mathematical models\\\\nbased on training data to make predictions or decisions without being\\\\nexplicitly programmed for each scenario. This paradigm shift from traditional\\\\nprogramming approaches allows systems to adapt and improve their\\\\nperformance over time.\\\\n\\\\nNote: Machine learning is fundamentally different from traditional\\\\nprogramming where we write explicit rules. In ML, we provide data and\\\\nexpected outputs, and the algorithm learns the rules.\",\\n                                \"paragraphs\": [\\n                                    \"Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed. It involves the development of algorithms and statistical models that computer systems use to perform specific tasks effectively without using explicit instructions.\",\\n                                    \"The core principle of machine learning is to build mathematical models based on training data to make predictions or decisions without being explicitly programmed for each scenario. This paradigm shift from traditional programming approaches allows systems to adapt and improve their performance over time.\",\\n                                    \"Note: Machine learning is fundamentally different from traditional programming where we write explicit rules. In ML, we provide data and expected outputs, and the algorithm learns the rules.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.1.1.2\",\\n                                \"title\": \"Historical Context\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"The concept of machine learning has its roots in the 1940s and 1950s, with\\\\nearly pioneers like Alan Turing proposing the idea of machines that could\\\\nlearn. The field has evolved through several key periods:\\\\n\\\\nThe symbolic AI period (1950s-1980s) focused on rule-based systems and\\\\nexpert systems. The connectionist revival (1980s-1990s) brought neural\\\\nnetworks back into prominence. The statistical learning era (1990s-2000s)\\\\nemphasized probabilistic models and support vector machines.\\\\n\\\\nThe current deep learning revolution (2010s-present) has been enabled by\\\\nincreased computational power, large datasets, and improved algorithms,\\\\nleading to breakthroughs in computer vision, natural language processing,\\\\nand other domains.\",\\n                                \"paragraphs\": [\\n                                    \"The concept of machine learning has its roots in the 1940s and 1950s, with early pioneers like Alan Turing proposing the idea of machines that could learn. The field has evolved through several key periods:\",\\n                                    \"The symbolic AI period (1950s-1980s) focused on rule-based systems and expert systems. The connectionist revival (1980s-1990s) brought neural networks back into prominence. The statistical learning era (1990s-2000s) emphasized probabilistic models and support vector machines.\",\\n                                    \"The current deep learning revolution (2010s-present) has been enabled by increased computational power, large datasets, and improved algorithms, leading to breakthroughs in computer vision, natural language processing, and other domains.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.1.1.3\",\\n                                \"title\": \"Modern Applications\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Machine learning has found applications across numerous industries and\\\\ndomains. In healthcare, ML algorithms assist in medical diagnosis, drug\\\\ndiscovery, and personalized treatment plans. Financial services use ML for\\\\nfraud detection, algorithmic trading, and risk assessment.\\\\n\\\\nTechnology companies leverage ML for recommendation systems, search\\\\nengines, and autonomous vehicles. Other applications include natural\\\\nlanguage processing for chatbots and translation services, computer vision\\\\nfor image recognition and analysis, and predictive maintenance in\\\\nmanufacturing.\\\\n\\\\nImportant: The success of ML applications depends heavily on data quality,\\\\nappropriate algorithm selection, and proper evaluation metrics.\",\\n                                \"paragraphs\": [\\n                                    \"Machine learning has found applications across numerous industries and domains. In healthcare, ML algorithms assist in medical diagnosis, drug discovery, and personalized treatment plans. Financial services use ML for fraud detection, algorithmic trading, and risk assessment.\",\\n                                    \"Technology companies leverage ML for recommendation systems, search engines, and autonomous vehicles. Other applications include natural language processing for chatbots and translation services, computer vision for image recognition and analysis, and predictive maintenance in manufacturing.\",\\n                                    \"Important: The success of ML applications depends heavily on data quality, appropriate algorithm selection, and proper evaluation metrics.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.1.2.1\",\\n                                \"title\": \"Supervised Learning\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Supervised learning involves training algorithms on labeled datasets where\\\\nboth input features and corresponding output labels are provided. The goal is\\\\nto learn a mapping function from inputs to outputs that can generalize to\\\\nnew, unseen data.\\\\n\\\\nClassification tasks involve predicting discrete categorical labels, such as\\\\nemail spam detection or image recognition. Regression tasks involve\\\\npredicting continuous numerical values, such as house prices or stock\\\\nmarket predictions.\\\\n\\\\n# Example of supervised learning structure\\\\nX_train = [[feature1, feature2, ...],\\\\n...] # Training features\\\\ny_train = [label1, label2, ...] # Training labels\\\\nmodel.fit(X_train, y_train) # Learn from data\\\\npredictions = model.predict(X_test) # Make predictions\",\\n                                \"paragraphs\": [\\n                                    \"Supervised learning involves training algorithms on labeled datasets where both input features and corresponding output labels are provided. The goal is to learn a mapping function from inputs to outputs that can generalize to new, unseen data.\",\\n                                    \"Classification tasks involve predicting discrete categorical labels, such as email spam detection or image recognition. Regression tasks involve predicting continuous numerical values, such as house prices or stock market predictions.\",\\n                                    \"# Example of supervised learning structure\\\\nX_train = [[feature1, feature2, ...],\\\\n...] # Training features\\\\ny_train = [label1, label2, ...] # Training labels\\\\nmodel.fit(X_train, y_train) # Learn from data\\\\npredictions = model.predict(X_test) # Make predictions\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.1.2.2\",\\n                                \"title\": \"Unsupervised Learning\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Unsupervised learning works with unlabeled data to discover hidden\\\\npatterns, structures, or relationships within the dataset. Without explicit\\\\ntarget variables, these algorithms must identify meaningful patterns\\\\nindependently.\\\\n\\\\nCommon unsupervised learning tasks include clustering (grouping similar\\\\ndata points), dimensionality reduction (reducing the number of features\\\\nwhile preserving important information), and anomaly detection (identifying\\\\noutliers or unusual patterns).\\\\nApplications include customer segmentation, data compression, and\\\\nexploratory data analysis. These techniques are particularly valuable for\\\\nunderstanding complex datasets and preprocessing data for supervised\\\\nlearning tasks.\",\\n                                \"paragraphs\": [\\n                                    \"Unsupervised learning works with unlabeled data to discover hidden patterns, structures, or relationships within the dataset. Without explicit target variables, these algorithms must identify meaningful patterns independently.\",\\n                                    \"Common unsupervised learning tasks include clustering (grouping similar data points), dimensionality reduction (reducing the number of features while preserving important information), and anomaly detection (identifying outliers or unusual patterns).\\\\nApplications include customer segmentation, data compression, and exploratory data analysis. These techniques are particularly valuable for understanding complex datasets and preprocessing data for supervised learning tasks.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.1.2.3\",\\n                                \"title\": \"Reinforcement Learning\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Reinforcement learning involves training agents to make sequential\\\\ndecisions in an environment to maximize cumulative rewards. The agent\\\\nlearns through interaction with the environment, receiving feedback in the\\\\nform of rewards or penalties.\\\\n\\\\nKey components include the agent (decision maker), environment (world in\\\\nwhich the agent operates), actions (choices available to the agent), states\\\\n(current situation), and rewards (feedback signal). The agent learns an\\\\noptimal policy that maps states to actions.\\\\n\\\\nApplications include game playing (chess, Go), autonomous navigation,\\\\nrobotics, and resource allocation. Notable successes include AlphaGo,\\\\nautonomous vehicles, and recommendation systems that adapt to user\\\\npreferences over time.\",\\n                                \"paragraphs\": [\\n                                    \"Reinforcement learning involves training agents to make sequential decisions in an environment to maximize cumulative rewards. The agent learns through interaction with the environment, receiving feedback in the form of rewards or penalties.\",\\n                                    \"Key components include the agent (decision maker), environment (world in which the agent operates), actions (choices available to the agent), states (current situation), and rewards (feedback signal). The agent learns an optimal policy that maps states to actions.\",\\n                                    \"Applications include game playing (chess, Go), autonomous navigation, robotics, and resource allocation. Notable successes include AlphaGo, autonomous vehicles, and recommendation systems that adapt to user preferences over time.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            }\\n                        ]\\n                    },\\n                    {\\n                        \"section_number\": \"1.2\",\\n                        \"title\": \"Mathematical Foundations\",\\n                        \"subsections\": [\\n                            {\\n                                \"subsection_number\": \"1.2.1.1\",\\n                                \"title\": \"Vectors and Matrices\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Vectors and matrices are fundamental mathematical structures in machine\\\\nlearning. A vector is an ordered collection of numbers, representing features\\\\nor data points in n-dimensional space. Matrices are rectangular arrays of\\\\nnumbers, often used to represent datasets or transformation operations.\\\\n\\\\nVector operations include addition, scalar multiplication, and dot products.\\\\nMatrix operations include multiplication, transposition, and inversion. These\\\\noperations form the basis for many machine learning algorithms.\\\\n\\\\nVector dot product: a $\\\\\\\\cdot$ b = $\\\\\\\\Sigma$(a$_{i}$ $\\\\\\\\times$ b$_{i}$)\\\\nMatrix multiplication: C = A $\\\\\\\\times$ B, where C$_{ij}$ = $\\\\\\\\Sigma$(A$_{ik}$ $\\\\\\\\times$ B$_{kj}$)\",\\n                                \"paragraphs\": [\\n                                    \"Vectors and matrices are fundamental mathematical structures in machine learning. A vector is an ordered collection of numbers, representing features or data points in n-dimensional space. Matrices are rectangular arrays of numbers, often used to represent datasets or transformation operations.\",\\n                                    \"Vector operations include addition, scalar multiplication, and dot products. Matrix operations include multiplication, transposition, and inversion. These operations form the basis for many machine learning algorithms.\",\\n                                    \"Vector dot product: a $\\\\\\\\cdot$ b = $\\\\\\\\Sigma$(a$_{i}$ $\\\\\\\\times$ b$_{i}$)\",\\n                                    \"Matrix multiplication: C = A $\\\\\\\\times$ B, where C$_{ij}$ = $\\\\\\\\Sigma$(A$_{ik}$ $\\\\\\\\times$ B$_{kj}$)\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.2.1.2\",\\n                                \"title\": \"Eigenvalues and Eigenvectors\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Eigenvalues and eigenvectors are crucial concepts in linear algebra with\\\\nsignificant applications in machine learning. An eigenvector of a matrix A is a\\\\nnon-zero vector v such that Av = $\\\\\\\\lambda$v, where $\\\\\\\\lambda$ is the corresponding eigenvalue.\\\\n\\\\nThese concepts are fundamental to dimensionality reduction techniques like\\\\nPrincipal Component Analysis (PCA), where eigenvectors represent principal\\\\ncomponents and eigenvalues indicate their importance. They also play roles\\\\nin spectral clustering and graph-based algorithms.\\\\n\\\\nComputing eigenvalues and eigenvectors helps understand the intrinsic\\\\nproperties of data transformations and can reveal important structural\\\\ninformation about datasets.\",\\n                                \"paragraphs\": [\\n                                    \"Eigenvalues and eigenvectors are crucial concepts in linear algebra with significant applications in machine learning. An eigenvector of a matrix A is a non-zero vector v such that Av = $\\\\\\\\lambda$v, where $\\\\\\\\lambda$ is the corresponding eigenvalue.\",\\n                                    \"These concepts are fundamental to dimensionality reduction techniques like Principal Component Analysis (PCA), where eigenvectors represent principal components and eigenvalues indicate their importance. They also play roles in spectral clustering and graph-based algorithms.\",\\n                                    \"Computing eigenvalues and eigenvectors helps understand the intrinsic properties of data transformations and can reveal important structural information about datasets.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.2.2.1\",\\n                                \"title\": \"Probability Distributions\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Probability distributions describe how probabilities are distributed over possible\\\\noutcomes of a random variable. Understanding distributions is essential for\\\\nmodeling uncertainty in machine learning systems.\\\\n\\\\nCommon distributions include the normal (Gaussian) distribution for continuous\\\\nvariables, binomial distribution for binary outcomes, and Poisson distribution for\\\\ncount data. Each distribution has specific parameters that characterize its shape\\\\nand properties.\\\\n\\\\nMachine learning algorithms often make assumptions about data distributions. For\\\\nexample, linear regression assumes normally distributed residuals, while Naive\\\\nBayes assumes conditional independence of features given the class.\",\\n                                \"paragraphs\": [\\n                                    \"Probability distributions describe how probabilities are distributed over possible outcomes of a random variable. Understanding distributions is essential for modeling uncertainty in machine learning systems.\",\\n                                    \"Common distributions include the normal (Gaussian) distribution for continuous variables, binomial distribution for binary outcomes, and Poisson distribution for count data. Each distribution has specific parameters that characterize its shape and properties.\",\\n                                    \"Machine learning algorithms often make assumptions about data distributions. For example, linear regression assumes normally distributed residuals, while Naive Bayes assumes conditional independence of features given the class.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.2.2.2\",\\n                                \"title\": \"Bayes\\' Theorem\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Bayes\\' theorem is a fundamental principle in probability theory that describes how\\\\nto update beliefs based on new evidence. It forms the foundation for Bayesian\\\\nmachine learning approaches.\\\\n\\\\nP(A|B) = P(B|A) $\\\\\\\\times$ P(A) / P(B)\\\\n\\\\nIn machine learning contexts, Bayes\\' theorem is used in classification algorithms\\\\nlike Naive Bayes, Bayesian networks, and Bayesian optimization. It provides a\\\\nprincipled way to incorporate prior knowledge and update predictions as new data\\\\nbecomes available.\",\\n                                \"paragraphs\": [\\n                                    \"Bayes\\' theorem is a fundamental principle in probability theory that describes how to update beliefs based on new evidence. It forms the foundation for Bayesian machine learning approaches.\",\\n                                    \"P(A|B) = P(B|A) $\\\\\\\\times$ P(A) / P(B)\",\\n                                    \"In machine learning contexts, Bayes\\' theorem is used in classification algorithms like Naive Bayes, Bayesian networks, and Bayesian optimization. It provides a principled way to incorporate prior knowledge and update predictions as new data becomes available.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            }\\n                        ]\\n                    },\\n                    {\\n                        \"section_number\": \"1.3\",\\n                        \"title\": \"Data Processing and Feature Engineering\",\\n                        \"subsections\": [\\n                            {\\n                                \"subsection_number\": \"1.3.1.1\",\\n                                \"title\": \"Data Cleaning\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Data cleaning is the process of identifying and correcting errors,\\\\ninconsistencies, and inaccuracies in datasets. Real-world data often\\\\ncontains noise, outliers, duplicate records, and formatting issues that can\\\\nnegatively impact model performance.\\\\n\\\\nCommon data cleaning tasks include removing duplicates, handling\\\\ninconsistent formatting, correcting typos, and dealing with outliers.\\\\nAutomated tools and manual inspection are often combined to ensure data\\\\nquality.\\\\n\\\\n# Example data cleaning operations\\\\ndf = df.drop_duplicates() # Remove duplicate rows\\\\ndf[\\'column\\'] = df[\\'column\\'].str.lower() # Standardize text case\\\\ndf = df[df[\\'value\\'] < threshold] # Remove outliers\",\\n                                \"paragraphs\": [\\n                                    \"Data cleaning is the process of identifying and correcting errors, inconsistencies, and inaccuracies in datasets. Real-world data often contains noise, outliers, duplicate records, and formatting issues that can negatively impact model performance.\",\\n                                    \"Common data cleaning tasks include removing duplicates, handling inconsistent formatting, correcting typos, and dealing with outliers. Automated tools and manual inspection are often combined to ensure data quality.\",\\n                                    \"# Example data cleaning operations\\\\ndf = df.drop_duplicates() # Remove duplicate rows\\\\ndf[\\'column\\'] = df[\\'column\\'].str.lower() # Standardize text case\\\\ndf = df[df[\\'value\\'] < threshold] # Remove outliers\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.3.1.2\",\\n                                \"title\": \"Missing Data Handling\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Missing data is a common challenge in machine learning projects. Different\\\\nstrategies exist for handling missing values, including deletion (removing\\\\nrows or columns with missing values), imputation (filling in missing values),\\\\nand using algorithms that can handle missing data directly.\\\\n\\\\nImputation methods include mean/median/mode imputation for simple\\\\ncases, regression imputation for more sophisticated approaches, and\\\\nmultiple imputation for maintaining uncertainty estimates. The choice\\\\ndepends on the nature of the missing data and the specific application.\\\\n\\\\nUnderstanding whether data is missing completely at random (MCAR),\\\\nmissing at random (MAR), or missing not at random (MNAR) is crucial for\\\\nchoosing appropriate handling strategies.\",\\n                                \"paragraphs\": [\\n                                    \"Missing data is a common challenge in machine learning projects. Different strategies exist for handling missing values, including deletion (removing rows or columns with missing values), imputation (filling in missing values), and using algorithms that can handle missing data directly.\",\\n                                    \"Imputation methods include mean/median/mode imputation for simple cases, regression imputation for more sophisticated approaches, and multiple imputation for maintaining uncertainty estimates. The choice depends on the nature of the missing data and the specific application.\",\\n                                    \"Understanding whether data is missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR) is crucial for choosing appropriate handling strategies.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.3.2.1\",\\n                                \"title\": \"Dimensionality Reduction\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Dimensionality reduction techniques aim to reduce the number of features while\\\\npreserving the most important information. This helps combat the curse of\\\\ndimensionality, improves computational efficiency, and can enhance model\\\\ninterpretability.\\\\nPrincipal Component Analysis (PCA) is a popular linear dimensionality reduction\\\\ntechnique that finds orthogonal projections that maximize variance. Non-linear\\\\nmethods like t-SNE and UMAP are useful for visualization and capturing complex\\\\ndata structures.\\\\n\\\\nFeature selection methods include filter methods (based on statistical measures),\\\\nwrapper methods (using model performance), and embedded methods (integrated\\\\ninto the learning algorithm).\",\\n                                \"paragraphs\": [\\n                                    \"Dimensionality reduction techniques aim to reduce the number of features while preserving the most important information. This helps combat the curse of dimensionality, improves computational efficiency, and can enhance model interpretability.\",\\n                                    \"Principal Component Analysis (PCA) is a popular linear dimensionality reduction technique that finds orthogonal projections that maximize variance. Non-linear methods like t-SNE and UMAP are useful for visualization and capturing complex data structures.\",\\n                                    \"Feature selection methods include filter methods (based on statistical measures), wrapper methods (using model performance), and embedded methods (integrated into the learning algorithm).\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"1.3.2.2\",\\n                                \"title\": \"Feature Scaling\",\\n                                \"Chapter title\": \"Foundations of Machine Learning\",\\n                                \"Chapter number\": \"1\",\\n                                \"Full content\": \"Feature scaling ensures that all features contribute equally to the learning process,\\\\npreventing features with larger scales from dominating the algorithm. Different\\\\nscaling methods are appropriate for different situations.\\\\n\\\\nMin-max scaling transforms features to a fixed range (typically 0-1), while\\\\nstandardization (z-score normalization) centers features around zero with unit\\\\nvariance. Robust scaling uses median and interquartile range to handle outliers\\\\nbetter.\\\\n\\\\nMin-max scaling: X\\' = (X - min(X)) / (max(X) - min(X))\\\\nStandardization: X\\' = (X - $\\\\\\\\mu$) / $\\\\\\\\sigma$\",\\n                                \"paragraphs\": [\\n                                    \"Feature scaling ensures that all features contribute equally to the learning process, preventing features with larger scales from dominating the algorithm. Different scaling methods are appropriate for different situations.\",\\n                                    \"Min-max scaling transforms features to a fixed range (typically 0-1), while standardization (z-score normalization) centers features around zero with unit variance. Robust scaling uses median and interquartile range to handle outliers better.\",\\n                                    \"Min-max scaling: X\\' = (X - min(X)) / (max(X) - min(X))\",\\n                                    \"Standardization: X\\' = (X - $\\\\\\\\mu$) / $\\\\\\\\sigma$\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            }\\n                        ]\\n                    }\\n                ]\\n            },\\n            {\\n                \"Chapter title\": \"Core Machine Learning Algorithms\",\\n                \"Chapter number\": \"2\",\\n                \"sections\": [\\n                    {\\n                        \"section_number\": \"2.1\",\\n                        \"title\": \"Supervised Learning Algorithms\",\\n                        \"subsections\": [\\n                            {\\n                                \"subsection_number\": \"2.1.1.1\",\\n                                \"title\": \"Linear Regression\",\\n                                \"Chapter title\": \"Core Machine Learning Algorithms\",\\n                                \"Chapter number\": \"2\",\\n                                \"Full content\": \"Linear regression is a fundamental supervised learning algorithm that\\\\nmodels the relationship between a dependent variable and independent\\\\nvariables by fitting a linear equation to the observed data. It assumes a linear\\\\nrelationship between features and the target variable.\\\\n\\\\nThe algorithm finds the best-fitting line through the data points by minimizing\\\\nthe sum of squared residuals. The resulting model can be expressed as y = $\\\\\\\\beta$$_{0}$\\\\n+ $\\\\\\\\beta$$_{1}$x$_{1}$ + $\\\\\\\\beta$$_{2}$x$_{2}$ + ... + $\\\\\\\\beta$$_{n}$x$_{n}$, where $\\\\\\\\beta$ coefficients represent the weights of each\\\\nfeature.\\\\n\\\\n# Linear regression implementation concept\\\\nimport numpy as np\\\\nfrom sklearn.linear_model import LinearRegression\\\\n# Create and train model\\\\nmodel = LinearRegression().fit(X_train, y_train)\\\\npredictions = model.predict(X_test) # Make predictions\\\\n\\\\nLinear regression is interpretable, computationally efficient, and serves as a\\\\nbaseline for many problems. However, it assumes linearity and can be\\\\nsensitive to outliers and multicollinearity.\",\\n                                \"paragraphs\": [\\n                                    \"Linear regression is a fundamental supervised learning algorithm that models the relationship between a dependent variable and independent variables by fitting a linear equation to the observed data. It assumes a linear relationship between features and the target variable.\",\\n                                    \"The algorithm finds the best-fitting line through the data points by minimizing the sum of squared residuals. The resulting model can be expressed as y = $\\\\\\\\beta$$_{0}$ + $\\\\\\\\beta$$_{1}$x$_{1}$ + $\\\\\\\\beta$$_{2}$x$_{2}$ + ... + $\\\\\\\\beta$$_{n}$x$_{n}$, where $\\\\\\\\beta$ coefficients represent the weights of each feature.\",\\n                                    \"# Linear regression implementation concept\\\\nimport numpy as np\\\\nfrom sklearn.linear_model import LinearRegression\\\\n# Create and train model\\\\nmodel = LinearRegression().fit(X_train, y_train)\\\\npredictions = model.predict(X_test) # Make predictions\",\\n                                    \"Linear regression is interpretable, computationally efficient, and serves as a baseline for many problems. However, it assumes linearity and can be sensitive to outliers and multicollinearity.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"2.1.1.2\",\\n                                \"title\": \"Logistic Regression\",\\n                                \"Chapter title\": \"Core Machine Learning Algorithms\",\\n                                \"Chapter number\": \"2\",\\n                                \"Full content\": \"Logistic regression extends linear regression to classification problems by\\\\nusing the logistic function to model the probability of class membership.\\\\nDespite its name, it\\'s a classification algorithm that outputs probabilities\\\\nbetween 0 and 1.\\\\n\\\\nThe logistic function (sigmoid) transforms the linear combination of features\\\\ninto a probability: P(y=1|x) = 1 / (1 + e$^{-(}\\\\\\\\beta$$_{0}$ + $\\\\\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\\\\\beta$$_{n}$x$_{n}^{)}$)). This\\\\nensures outputs are always between 0 and 1.\\\\n\\\\nLogistic function: $\\\\\\\\sigma$(z) = 1 / (1 + e$^{-z}$)\\\\nLog-odds: ln(p/(1-p)) = $\\\\\\\\beta$$_{0}$ + $\\\\\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\\\\\beta$$_{n}$x$_{n}$\\\\n\\\\nLogistic regression is widely used for binary classification and can be\\\\nextended to multi-class problems. It provides probabilistic outputs and\\\\ncoefficient interpretability, making it valuable for many applications.\",\\n                                \"paragraphs\": [\\n                                    \"Logistic regression extends linear regression to classification problems by using the logistic function to model the probability of class membership. Despite its name, it\\'s a classification algorithm that outputs probabilities between 0 and 1.\",\\n                                    \"The logistic function (sigmoid) transforms the linear combination of features into a probability: P(y=1|x) = 1 / (1 + e$^{-(}\\\\\\\\beta$$_{0}$ + $\\\\\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\\\\\beta$$_{n}$x$_{n}^{)}$)). This ensures outputs are always between 0 and 1.\",\\n                                    \"Logistic function: $\\\\\\\\sigma$(z) = 1 / (1 + e$^{-z}$)\",\\n                                    \"Log-odds: ln(p/(1-p)) = $\\\\\\\\beta$$_{0}$ + $\\\\\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\\\\\beta$$_{n}$x$_{n}$\",\\n                                    \"Logistic regression is widely used for binary classification and can be extended to multi-class problems. It provides probabilistic outputs and coefficient interpretability, making it valuable for many applications.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            },\\n                            {\\n                                \"subsection_number\": \"2.1.1.3\",\\n                                \"title\": \"Regularization Techniques\",\\n                                \"Chapter title\": \"Core Machine Learning Algorithms\",\\n                                \"Chapter number\": \"2\",\\n                                \"Full content\": \"Regularization techniques prevent overfitting by adding penalty terms to the\\\\nloss function. They constrain model complexity and improve generalization\\\\nto unseen data, especially important when dealing with high-dimensional\\\\ndatasets.\\\\n\\\\nL1 regularization (Lasso) adds the sum of absolute values of coefficients as a\\\\npenalty, promoting sparsity and automatic feature selection. L2\\\\nregularization (Ridge) adds the sum of squared coefficients, shrinking\\\\ncoefficients towards zero but keeping all features.\\\\n\\\\nElastic Net combines L1 and L2 regularization.\",\\n                                \"paragraphs\": [\\n                                    \"Regularization techniques prevent overfitting by adding penalty terms to the loss function. They constrain model complexity and improve generalization to unseen data, especially important when dealing with high-dimensional datasets.\",\\n                                    \"L1 regularization (Lasso) adds the sum of absolute values of coefficients as a penalty, promoting sparsity and automatic feature selection. L2 regularization (Ridge) adds the sum of squared coefficients, shrinking coefficients towards zero but keeping all features.\",\\n                                    \"Elastic Net combines L1 and L2 regularization.\"\\n                                ],\\n                                \"lists\": [],\\n                                \"tables\": [],\\n                                \"figures\": []\\n                            }\\n                        ]\\n                    }\\n                ]\\n            }\\n        ]\\n    }\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_json = json.loads(cleaned_json_content)"
      ],
      "metadata": {
        "id": "cKiwHln_8knn"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_json\n",
        "\n",
        "# Recursive function to print sections and subsections\n",
        "def print_sections(sections, indent=1):\n",
        "    for sec in sections:\n",
        "        number = sec.get(\"section_number\") or sec.get(\"subsection_number\")\n",
        "        title = sec.get(\"title\", \"\")\n",
        "        print(\"  \" * indent + f\"{number} {title}\")\n",
        "        if \"subsections\" in sec:\n",
        "            print_sections(sec[\"subsections\"], indent + 1)\n",
        "\n",
        "# Start from chapters\n",
        "for chapter in data[\"document_structure\"][\"chapters\"]:\n",
        "    print(f'{chapter[\"Chapter number\"]} {chapter[\"Chapter title\"]}')\n",
        "    print_sections(chapter[\"sections\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s86R5q0R0Ham",
        "outputId": "deee66b3-b26c-46bc-c5ba-c8b433005a6c"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Foundations of Machine Learning\n",
            "  1.1 Introduction to Machine Learning\n",
            "    1.1.1.1 Definition and Scope\n",
            "    1.1.1.2 Historical Context\n",
            "    1.1.1.3 Modern Applications\n",
            "    1.1.2.1 Supervised Learning\n",
            "    1.1.2.2 Unsupervised Learning\n",
            "    1.1.2.3 Reinforcement Learning\n",
            "  1.2 Mathematical Foundations\n",
            "    1.2.1.1 Vectors and Matrices\n",
            "    1.2.1.2 Eigenvalues and Eigenvectors\n",
            "    1.2.2.1 Probability Distributions\n",
            "    1.2.2.2 Bayes' Theorem\n",
            "  1.3 Data Processing and Feature Engineering\n",
            "    1.3.1.1 Data Cleaning\n",
            "    1.3.1.2 Missing Data Handling\n",
            "    1.3.2.1 Dimensionality Reduction\n",
            "    1.3.2.2 Feature Scaling\n",
            "2 Core Machine Learning Algorithms\n",
            "  2.1 Supervised Learning Algorithms\n",
            "    2.1.1.1 Linear Regression\n",
            "    2.1.1.2 Logistic Regression\n",
            "    2.1.1.3 Regularization Techniques\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJ2UP5hJ0HX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czW5G5DQ0HVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}