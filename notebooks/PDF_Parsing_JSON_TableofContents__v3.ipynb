{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AIWNL-6VE3uw"
      },
      "outputs": [],
      "source": [
        "# !pip install google-generativeai\n",
        "# !pip install pypdf\n",
        "# !pip install pdf2image\n",
        "# !pip install pillow\n",
        "# !pip install python-dotenv\n",
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IHfPGz4vmSQ9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "from datetime import datetime\n",
        "from IPython.display import JSON\n",
        "import json\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOqwdovSE3oH"
      },
      "outputs": [],
      "source": [
        "# Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key_here\"\n",
        "\n",
        "# Configure genai with the API key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "# Load Gemini model\n",
        "model = genai.GenerativeModel(\n",
        "    \"gemini-2.5-flash-preview-04-17\", #\"gemini-2.5-flash-preview-04-17\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ocq6nF5KE3lO"
      },
      "outputs": [],
      "source": [
        "def convert_pdf_to_images(pdf_path, output_folder, dpi=300):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(pdf_path, dpi=dpi)\n",
        "\n",
        "    # Save images to the output folder\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        image_path = os.path.join(output_folder, f'page_{i+1}.jpg')\n",
        "        image.save(image_path, 'JPEG')\n",
        "        image_paths.append(image_path)\n",
        "\n",
        "    return image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-wLBkweGHLEx"
      },
      "outputs": [],
      "source": [
        "pdf_path = '/content/Machine Learning Fundamentals.pdf'\n",
        "output_folder = '/content/images'\n",
        "# image_paths = convert_pdf_to_images(pdf_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru_yVt1Iq02_"
      },
      "outputs": [],
      "source": [
        "# !rm -r /content/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6hv8lZ09E3iH"
      },
      "outputs": [],
      "source": [
        "def batch_images(image_paths, batch_size=10):\n",
        "    \"\"\"Group images into batches for processing\"\"\"\n",
        "    for i in range(0, len(image_paths), batch_size):\n",
        "        yield image_paths[i:i + batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "75tmxDbTE3ff"
      },
      "outputs": [],
      "source": [
        "def ocr_with_gemini(image_paths, instruction):\n",
        "    images = [Image.open(path) for path in image_paths]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "    You are an expert document analysis AI with exceptional OCR capabilities.\n",
        "    Your task is to extract ALL textual content from the provided document images with perfect accuracy.\n",
        "\n",
        "    CRITICAL REQUIREMENTS:\n",
        "    1. ACCURACY: Every word, number, symbol, and punctuation mark must be captured exactly as shown\n",
        "    2. STRUCTURE: Maintain the original document structure, hierarchy, and formatting\n",
        "    3. COMPLETENESS: Do not skip any content, including headers, footers, page numbers, footnotes, watermarks, or marginalia\n",
        "    4. CONTEXT: Understand the document context to resolve ambiguous characters\n",
        "\n",
        "    {instruction}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = model.generate_content([prompt, *images])\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9y0_Idq_E3ca"
      },
      "outputs": [],
      "source": [
        "def ocr_complex_document(image_paths):\n",
        "    instruction = \"\"\"\n",
        "    Extract ALL text content from these document pages.\n",
        "    For Tables:\n",
        "    - Use markdown table format with proper alignment\n",
        "    - Include all headers, subheaders, and merged cells\n",
        "    - Preserve numerical precision and units\n",
        "    - Note any table notes or footnotes\n",
        "\n",
        "    For Multi-column Text:\n",
        "    - Process columns in natural reading order (left to right, top to bottom)\n",
        "    - Clearly separate column content with appropriate breaks\n",
        "    - Maintain column-specific formatting\n",
        "\n",
        "    For Charts/Graphs:\n",
        "    - Describe chart type and purpose\n",
        "    - Extract all axis labels, legends, and data points\n",
        "    - Capture titles, captions, and source information\n",
        "    - Note any trends or key insights visible in the visual\n",
        "\n",
        "    For Special Elements:\n",
        "    - Preserve bullet points, numbered lists, and indentation\n",
        "    - Maintain emphasis (bold, italic, underline) using markdown\n",
        "    - Capture all hyperlinks and cross-references\n",
        "    - Include page numbers and section breaks\n",
        "\n",
        "    QUALITY ASSURANCE:\n",
        "    - Double-check all numerical data for accuracy\n",
        "    - Verify proper names, technical terms, and specialized vocabulary\n",
        "    - Ensure logical flow and coherence in extracted text\n",
        "    - Flag any unclear or potentially misread content with [UNCERTAIN: text]\n",
        "    Preserve all headers, footers, page numbers, and footnotes.\n",
        "    \"\"\"\n",
        "\n",
        "    return ocr_with_gemini(image_paths, instruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bNYiZFaHE3W_"
      },
      "outputs": [],
      "source": [
        "def process_large_pdf(pdf_path, output_folder):\n",
        "    # Convert PDF to images\n",
        "    image_paths = convert_pdf_to_images(pdf_path, output_folder)\n",
        "\n",
        "    # Create batches of images (e.g., by chapter or section)\n",
        "    batches = batch_images(image_paths, 30)\n",
        "\n",
        "    full_text = \"\"\n",
        "    for i, batch in enumerate(batches):\n",
        "        print(f\"Processing batch {i+1}...\")\n",
        "        batch_text = ocr_with_gemini(batch, \"Extract all text, maintaining document structure\")\n",
        "        full_text += f\"\\n\\n--- BATCH {i+1} ---\\n\\n{batch_text}\"\n",
        "\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "bfVSXatimSRA",
        "outputId": "12b74c3a-ef66-49eb-d194-89cd54066a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1...\n"
          ]
        }
      ],
      "source": [
        "text = process_large_pdf(pdf_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "1N2Amq2vLxxw"
      },
      "outputs": [],
      "source": [
        "def text_to_json_with_gemini(text_content):\n",
        "  \"\"\"Converts text content to JSON format using the Gemini model.\"\"\"\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    You are a document structuring expert. Convert the provided text into a comprehensive, well-organized JSON structure.\n",
        "\n",
        "    REQUIREMENTS:\n",
        "    1. Create a logical, hierarchical JSON structure that reflects the document's organization\n",
        "    2. Preserve all content while organizing it meaningfully\n",
        "    3. Include metadata about the document structure and content\n",
        "\n",
        "  Convert the following text content into a structured JSON object.\n",
        "  Identify key sections, headings, paragraphs, tables, and other elements\n",
        "  and represent them appropriately in the JSON structure.\n",
        "\n",
        "\n",
        "  Text content to convert:\n",
        "\n",
        "  {}\n",
        "\n",
        "    JSON STRUCTURE TEMPLATE:\n",
        "    {{\n",
        "        \"document_metadata\": {{\n",
        "            \"title\": \"Document title if available\",\n",
        "            \"subsection number\": \"Subsection number if available\",\n",
        "            \"document_type\": \"academic|technical|report|manual|other\",\n",
        "            \"total_pages\": \"number of pages processed\",\n",
        "            \"language\": \"primary language detected\",\n",
        "            \"has_tables\": true/false,\n",
        "            \"has_charts\": true/false,\n",
        "            \"has_images\": true/false,\n",
        "\n",
        "        }},\n",
        "        \"document_structure\": {{\n",
        "            \"Chapter title\": \"Chapter title if available\",\n",
        "            \"Chapter number\": \"Chapter number if available\",\n",
        "            \"sections\": [\n",
        "                {{\n",
        "                    \"section_number\": \"1\",\n",
        "                    \"title\": \"Section Title\",\n",
        "                    \"subsections\": [\n",
        "                        {{\n",
        "                            \"subsection_number\": \"1.1\",\n",
        "                            \"title\": \"Subsection Title\",\n",
        "                            \"Chapter title\": \"Chapter title if available\",\n",
        "                            \"Chapter number\": \"Chapter number if available\",\n",
        "                            \"content\": \"Full text content from this section must inculdeed and type of each one\",\n",
        "                            \"lists\": [\"list items if any\"],\n",
        "                            \"tables\": [\n",
        "                                {{\n",
        "                                    \"table_number\": \"Table 1\",\n",
        "                                    \"caption\": \"Table caption\",\n",
        "                                    \"headers\": [\"Column 1\", \"Column 2\"],\n",
        "                                    \"rows\": [[\"Data 1\", \"Data 2\"]]\n",
        "                                }}\n",
        "                            ],\n",
        "                            \"figures\": [\n",
        "                                {{\n",
        "                                    \"figure_number\": \"Figure 1\",\n",
        "                                    \"caption\": \"Figure caption\",\n",
        "                                    \"description\": \"Description of visual content\"\n",
        "                                }}\n",
        "                            ]\n",
        "                        }}\n",
        "                    ]\n",
        "                }}\n",
        "            ]\n",
        "        }},\n",
        "    }}\n",
        "\n",
        "    ADAPTATION RULES:\n",
        "    - If document doesn't have clear sections, organize by pages or logical breaks\n",
        "    - For tables without clear structure, preserve as formatted text\n",
        "    - For charts/graphs, include detailed descriptions in figures array\n",
        "    - Adapt structure to match document type (academic papers, reports, manuals, etc.)\n",
        "    - Ensure all content is preserved even if structure is unclear\n",
        "\n",
        "    OUTPUT: Valid JSON only, no additional text or explanations. and don't add ```json```\n",
        "  \"\"\".format(text_content)\n",
        "\n",
        "  response = model.generate_content([prompt])\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "yQJVL6X2L1Xb"
      },
      "outputs": [],
      "source": [
        "# Convert the extracted text to JSON\n",
        "json_output = text_to_json_with_gemini(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCGr3h2VsElm",
        "outputId": "4feba720-4013-4ecb-83e2-d85d8423abc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"document_metadata\": {\n",
            "    \"title\": \"Machine Learning Fundamentals\",\n",
            "    \"document_type\": \"technical\",\n",
            "    \"total_pages\": \"N/A\",\n",
            "    \"language\": \"en\",\n",
            "    \"has_tables\": false,\n",
            "    \"has_charts\": false,\n",
            "    \"has_images\": false\n",
            "  },\n",
            "  \"document_structure\": {\n",
            "    \"chapters\": [\n",
            "      {\n",
            "        \"chapter_number\": \"1\",\n",
            "        \"title\": \"Foundations of Machine Learning\",\n",
            "        \"sections\": [\n",
            "          {\n",
            "            \"section_number\": \"1.1\",\n",
            "            \"title\": \"Introduction to Machine Learning\",\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"subsection_number\": \"1.1.1\",\n",
            "                \"title\": \"What is Machine Learning?\",\n",
            "                \"content_elements\": [\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.1.1\",\n",
            "                    \"source_heading_title\": \"Definition and Scope\",\n",
            "                    \"text\": \"Machine Learning (ML) is a subset of artificial intelligence that enables\\ncomputers to learn and improve from experience without being explicitly\\nprogrammed. It involves the development of algorithms and statistical\\nmodels that computer systems use to perform specific tasks effectively\\nwithout using explicit instructions.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.1.1\",\n",
            "                    \"source_heading_title\": \"Definition and Scope\",\n",
            "                    \"text\": \"The core principle of machine learning is to build mathematical models\\nbased on training data to make predictions or decisions without being\\nexplicitly programmed for each scenario. This paradigm shift from traditional\\nprogramming approaches allows systems to adapt and improve their\\nperformance over time.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"note\",\n",
            "                    \"source_heading_number\": \"1.1.1.1\",\n",
            "                    \"source_heading_title\": \"Definition and Scope\",\n",
            "                    \"text\": \"Machine learning is fundamentally different from traditional\\nprogramming where we write explicit rules. In ML, we provide data and\\nexpected outputs, and the algorithm learns the rules.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.1.2\",\n",
            "                    \"source_heading_title\": \"Historical Context\",\n",
            "                    \"text\": \"The concept of machine learning has its roots in the 1940s and 1950s, with\\nearly pioneers like Alan Turing proposing the idea of machines that could\\nlearn. The field has evolved through several key periods:\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.1.2\",\n",
            "                    \"source_heading_title\": \"Historical Context\",\n",
            "                    \"text\": \"The symbolic AI period (1950s-1980s) focused on rule-based systems and\\nexpert systems. The connectionist revival (1980s-1990s) brought neural\\nnetworks back into prominence. The statistical learning era (1990s-2000s)\\nemphasized probabilistic models and support vector machines.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.1.2\",\n",
            "                    \"source_heading_title\": \"Historical Context\",\n",
            "                    \"text\": \"The current deep learning revolution (2010s-present) has been enabled by\\nincreased computational power, large datasets, and improved algorithms,\\nleading to breakthroughs in computer vision, natural language processing,\\nand other domains.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.1.3\",\n",
            "                    \"source_heading_title\": \"Modern Applications\",\n",
            "                    \"text\": \"Machine learning has found applications across numerous industries and\\ndomains. In healthcare, ML algorithms assist in medical diagnosis, drug\\ndiscovery, and personalized treatment plans. Financial services use ML for\\nfraud detection, algorithmic trading, and risk assessment.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.1.3\",\n",
            "                    \"source_heading_title\": \"Modern Applications\",\n",
            "                    \"text\": \"Technology companies leverage ML for recommendation systems, search\\nengines, and autonomous vehicles. Other applications include natural\\nlanguage processing for chatbots and translation services, computer vision\\nfor image recognition and analysis, and predictive maintenance in\\nmanufacturing.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"callout\",\n",
            "                    \"source_heading_number\": \"1.1.1.3\",\n",
            "                    \"source_heading_title\": \"Modern Applications\",\n",
            "                    \"text\": \"The success of ML applications depends heavily on data quality,\\nappropriate algorithm selection, and proper evaluation metrics.\"\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"subsection_number\": \"1.1.2\",\n",
            "                \"title\": \"Types of Machine Learning\",\n",
            "                \"content_elements\": [\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.2.1\",\n",
            "                    \"source_heading_title\": \"Supervised Learning\",\n",
            "                    \"text\": \"Supervised learning involves training algorithms on labeled datasets where\\nboth input features and corresponding output labels are provided. The goal is\\nto learn a mapping function from inputs to outputs that can generalize to\\nnew, unseen data.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.2.1\",\n",
            "                    \"source_heading_title\": \"Supervised Learning\",\n",
            "                    \"text\": \"Classification tasks involve predicting discrete categorical labels, such as\\nemail spam detection or image recognition. Regression tasks involve\\npredicting continuous numerical values, such as house prices or stock\\nmarket predictions.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"code\",\n",
            "                    \"source_heading_number\": \"1.1.2.1\",\n",
            "                    \"source_heading_title\": \"Supervised Learning\",\n",
            "                    \"text\": \"# Example of supervised learning structure\\nX_train = [[feature1, feature2, ...],\\n...] # Training features\\ny_train = [label1, label2, ...] # Training labels\\nmodel.fit(X_train, y_train) # Learn from data\\npredictions = model.predict(X_test) # Make predictions\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.2.2\",\n",
            "                    \"source_heading_title\": \"Unsupervised Learning\",\n",
            "                    \"text\": \"Unsupervised learning works with unlabeled data to discover hidden\\npatterns, structures, or relationships within the dataset. Without explicit\\ntarget variables, these algorithms must identify meaningful patterns\\nindependently.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.2.2\",\n",
            "                    \"source_heading_title\": \"Unsupervised Learning\",\n",
            "                    \"text\": \"Common unsupervised learning tasks include clustering (grouping similar\\ndata points), dimensionality reduction (reducing the number of features\\nwhile preserving important information), and anomaly detection (identifying\\noutliers or unusual patterns).\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.2.2\",\n",
            "                    \"source_heading_title\": \"Unsupervised Learning\",\n",
            "                    \"text\": \"Applications include customer segmentation, data compression, and\\nexploratory data analysis. These techniques are particularly valuable for\\nunderstanding complex datasets and preprocessing data for supervised\\nlearning tasks.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.2.3\",\n",
            "                    \"source_heading_title\": \"Reinforcement Learning\",\n",
            "                    \"text\": \"Reinforcement learning involves training agents to make sequential\\ndecisions in an environment to maximize cumulative rewards. The agent\\nlearns through interaction with the environment, receiving feedback in the\\nform of rewards or penalties.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.2.3\",\n",
            "                    \"source_heading_title\": \"Reinforcement Learning\",\n",
            "                    \"text\": \"Key components include the agent (decision maker), environment (world in\\nwhich the agent operates), actions (choices available to the agent), states\\n(current situation), and rewards (feedback signal). The agent learns an\\noptimal policy that maps states to actions.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.1.2.3\",\n",
            "                    \"source_heading_title\": \"Reinforcement Learning\",\n",
            "                    \"text\": \"Applications include game playing (chess, Go), autonomous navigation,\\nrobotics, and resource allocation. Notable successes include AlphaGo,\\nautonomous vehicles, and recommendation systems that adapt to user\\npreferences over time.\"\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"section_number\": \"1.2\",\n",
            "            \"title\": \"Mathematical Foundations\",\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"subsection_number\": \"1.2.1\",\n",
            "                \"title\": \"Linear Algebra\",\n",
            "                \"content_elements\": [\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.1.1\",\n",
            "                    \"source_heading_title\": \"Vectors and Matrices\",\n",
            "                    \"text\": \"Vectors and matrices are fundamental mathematical structures in machine\\nlearning. A vector is an ordered collection of numbers, representing features\\nor data points in n-dimensional space. Matrices are rectangular arrays of\\nnumbers, often used to represent datasets or transformation operations.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.1.1\",\n",
            "                    \"source_heading_title\": \"Vectors and Matrices\",\n",
            "                    \"text\": \"Vector operations include addition, scalar multiplication, and dot products.\\nMatrix operations include multiplication, transposition, and inversion. These\\noperations form the basis for many machine learning algorithms.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"formula\",\n",
            "                    \"source_heading_number\": \"1.2.1.1\",\n",
            "                    \"source_heading_title\": \"Vectors and Matrices\",\n",
            "                    \"text\": \"Vector dot product: a $\\cdot$ b = $\\Sigma$(a$_{i}$ $\\times$ b$_{i}$)\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"formula\",\n",
            "                     \"source_heading_number\": \"1.2.1.1\",\n",
            "                    \"source_heading_title\": \"Vectors and Matrices\",\n",
            "                    \"text\": \"Matrix multiplication: C = A $\\times$ B, where C$_{ij}$ = $\\Sigma$(A$_{ik}$ $\\times$ B$_{kj}$)\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.1.2\",\n",
            "                    \"source_heading_title\": \"Eigenvalues and Eigenvectors\",\n",
            "                    \"text\": \"Eigenvalues and eigenvectors are crucial concepts in linear algebra with\\nsignificant applications in machine learning. An eigenvector of a matrix A is a\\nnon-zero vector v such that Av = $\\lambda$v, where $\\lambda$ is the corresponding eigenvalue.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.1.2\",\n",
            "                    \"source_heading_title\": \"Eigenvalues and Eigenvectors\",\n",
            "                    \"text\": \"These concepts are fundamental to dimensionality reduction techniques like\\nPrincipal Component Analysis (PCA), where eigenvectors represent principal\\ncomponents and eigenvalues indicate their importance. They also play roles\\nin spectral clustering and graph-based algorithms.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.1.2\",\n",
            "                    \"source_heading_title\": \"Eigenvalues and Eigenvectors\",\n",
            "                    \"text\": \"Computing eigenvalues and eigenvectors helps understand the intrinsic\\nproperties of data transformations and can reveal important structural\\ninformation about datasets.\"\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"subsection_number\": \"1.2.2\",\n",
            "                \"title\": \"Statistics and Probability\",\n",
            "                \"content_elements\": [\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.2.1\",\n",
            "                    \"source_heading_title\": \"Probability Distributions\",\n",
            "                    \"text\": \"Probability distributions describe how probabilities are distributed over possible\\noutcomes of a random variable. Understanding distributions is essential for\\nmodeling uncertainty in machine learning systems.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.2.1\",\n",
            "                    \"source_heading_title\": \"Probability Distributions\",\n",
            "                    \"text\": \"Common distributions include the normal (Gaussian) distribution for continuous\\nvariables, binomial distribution for binary outcomes, and Poisson distribution for\\ncount data. Each distribution has specific parameters that characterize its shape\\nand properties.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.2.1\",\n",
            "                    \"source_heading_title\": \"Probability Distributions\",\n",
            "                    \"text\": \"Machine learning algorithms often make assumptions about data distributions. For\\nexample, linear regression assumes normally distributed residuals, while Naive\\nBayes assumes conditional independence of features given the class.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.2.2\",\n",
            "                    \"source_heading_title\": \"Bayes' Theorem\",\n",
            "                    \"text\": \"Bayes' theorem is a fundamental principle in probability theory that describes how\\nto update beliefs based on new evidence. It forms the foundation for Bayesian\\nmachine learning approaches.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"formula\",\n",
            "                    \"source_heading_number\": \"1.2.2.2\",\n",
            "                    \"source_heading_title\": \"Bayes' Theorem\",\n",
            "                    \"text\": \"P(A|B) = P(B|A) $\\times$ P(A) / P(B)\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.2.2.2\",\n",
            "                    \"source_heading_title\": \"Bayes' Theorem\",\n",
            "                    \"text\": \"In machine learning contexts, Bayes' theorem is used in classification algorithms\\nlike Naive Bayes, Bayesian networks, and Bayesian optimization. It provides a\\nprincipled way to incorporate prior knowledge and update predictions as new data\\nbecomes available.\"\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"section_number\": \"1.3\",\n",
            "            \"title\": \"Data Processing and Feature Engineering\",\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"subsection_number\": \"1.3.1\",\n",
            "                \"title\": \"Data Preprocessing\",\n",
            "                \"content_elements\": [\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.1.1\",\n",
            "                    \"source_heading_title\": \"Data Cleaning\",\n",
            "                    \"text\": \"Data cleaning is the process of identifying and correcting errors,\\ninconsistencies, and inaccuracies in datasets. Real-world data often\\ncontains noise, outliers, duplicate records, and formatting issues that can\\nnegatively impact model performance.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.1.1\",\n",
            "                    \"source_heading_title\": \"Data Cleaning\",\n",
            "                    \"text\": \"Common data cleaning tasks include removing duplicates, handling\\ninconsistent formatting, correcting typos, and dealing with outliers.\\nAutomated tools and manual inspection are often combined to ensure data\\nquality.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"code\",\n",
            "                    \"source_heading_number\": \"1.3.1.1\",\n",
            "                    \"source_heading_title\": \"Data Cleaning\",\n",
            "                    \"text\": \"# Example data cleaning operations\\ndf = df.drop_duplicates() # Remove duplicate rows\\ndf['column'] = df['column'].str.lower() # Standardize text case\\ndf = df[df['value'] < threshold] # Remove outliers\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.1.2\",\n",
            "                    \"source_heading_title\": \"Missing Data Handling\",\n",
            "                    \"text\": \"Missing data is a common challenge in machine learning projects. Different\\nstrategies exist for handling missing values, including deletion (removing\\nrows or columns with missing values), imputation (filling in missing values),\\nand using algorithms that can handle missing data directly.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.1.2\",\n",
            "                    \"source_heading_title\": \"Missing Data Handling\",\n",
            "                    \"text\": \"Imputation methods include mean/median/mode imputation for simple\\ncases, regression imputation for more sophisticated approaches, and\\nmultiple imputation for maintaining uncertainty estimates. The choice\\ndepends on the nature of the missing data and the specific application.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.1.2\",\n",
            "                    \"source_heading_title\": \"Missing Data Handling\",\n",
            "                    \"text\": \"Understanding whether data is missing completely at random (MCAR),\\nmissing at random (MAR), or missing not at random (MNAR) is crucial for\\nchoosing appropriate handling strategies.\"\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"subsection_number\": \"1.3.2\",\n",
            "                \"title\": \"Feature Selection and Extraction\",\n",
            "                \"content_elements\": [\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.2.1\",\n",
            "                    \"source_heading_title\": \"Dimensionality Reduction\",\n",
            "                    \"text\": \"Dimensionality reduction techniques aim to reduce the number of features while\\npreserving the most important information. This helps combat the curse of\\ndimensionality, improves computational efficiency, and can enhance model\\ninterpretability.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.2.1\",\n",
            "                    \"source_heading_title\": \"Dimensionality Reduction\",\n",
            "                    \"text\": \"Principal Component Analysis (PCA) is a popular linear dimensionality reduction\\ntechnique that finds orthogonal projections that maximize variance. Non-linear\\nmethods like t-SNE and UMAP are useful for visualization and capturing complex\\ndata structures.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.2.1\",\n",
            "                    \"source_heading_title\": \"Dimensionality Reduction\",\n",
            "                    \"text\": \"Feature selection methods include filter methods (based on statistical measures),\\nwrapper methods (using model performance), and embedded methods (integrated\\ninto the learning algorithm).\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.2.2\",\n",
            "                    \"source_heading_title\": \"Feature Scaling\",\n",
            "                    \"text\": \"Feature scaling ensures that all features contribute equally to the learning process,\\npreventing features with larger scales from dominating the algorithm. Different\\nscaling methods are appropriate for different situations.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"1.3.2.2\",\n",
            "                    \"source_heading_title\": \"Feature Scaling\",\n",
            "                    \"text\": \"Min-max scaling transforms features to a fixed range (typically 0-1), while\\nstandardization (z-score normalization) centers features around zero with unit\\nvariance. Robust scaling uses median and interquartile range to handle outliers\\nbetter.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"formula\",\n",
            "                     \"source_heading_number\": \"1.3.2.2\",\n",
            "                    \"source_heading_title\": \"Feature Scaling\",\n",
            "                    \"text\": \"Min-max scaling: X' = (X - min(X)) / (max(X) - min(X))\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"formula\",\n",
            "                    \"source_heading_number\": \"1.3.2.2\",\n",
            "                    \"source_heading_title\": \"Feature Scaling\",\n",
            "                    \"text\": \"Standardization: X' = (X - $\\mu$) / $\\sigma$\"\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"chapter_number\": \"2\",\n",
            "        \"title\": \"Core Machine Learning Algorithms\",\n",
            "        \"sections\": [\n",
            "          {\n",
            "            \"section_number\": \"2.1\",\n",
            "            \"title\": \"Supervised Learning Algorithms\",\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"subsection_number\": \"2.1.1\",\n",
            "                \"title\": \"Linear Models\",\n",
            "                \"content_elements\": [\n",
            "                  {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.1\",\n",
            "                    \"source_heading_title\": \"Linear Regression\",\n",
            "                    \"text\": \"Linear regression is a fundamental supervised learning algorithm that\\nmodels the relationship between a dependent variable and independent\\nvariables by fitting a linear equation to the observed data. It assumes a linear\\nrelationship between features and the target variable.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.1\",\n",
            "                    \"source_heading_title\": \"Linear Regression\",\n",
            "                    \"text\": \"The algorithm finds the best-fitting line through the data points by minimizing\\nthe sum of squared residuals. The resulting model can be expressed as y = $\\beta$$_{0}$\\n+ $\\beta$$_{1}$x$_{1}$ + $\\beta$$_{2}$x$_{2}$ + ... + $\\beta$$_{n}$x$_{n}$, where $\\beta$ coefficients represent the weights of each\\nfeature.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"code\",\n",
            "                    \"source_heading_number\": \"2.1.1.1\",\n",
            "                    \"source_heading_title\": \"Linear Regression\",\n",
            "                    \"text\": \"# Linear regression implementation concept\\nimport numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n# Create and train model\\nmodel = LinearRegression().fit(X_train, y_train)\\npredictions = model.predict(X_test) # Make predictions\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.1\",\n",
            "                    \"source_heading_title\": \"Linear Regression\",\n",
            "                    \"text\": \"Linear regression is interpretable, computationally efficient, and serves as a\\nbaseline for many problems. However, it assumes linearity and can be\\nsensitive to outliers and multicollinearity.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.2\",\n",
            "                    \"source_heading_title\": \"Logistic Regression\",\n",
            "                    \"text\": \"Logistic regression extends linear regression to classification problems by\\nusing the logistic function to model the probability of class membership.\\nDespite its name, it's a classification algorithm that outputs probabilities\\nbetween 0 and 1.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.2\",\n",
            "                    \"source_heading_title\": \"Logistic Regression\",\n",
            "                    \"text\": \"The logistic function (sigmoid) transforms the linear combination of features\\ninto a probability: P(y=1|x) = 1 / (1 + e$^{-(}\\beta$$_{0}$ + $\\beta$$_{1}$x$_{1}$ + ... + $\\beta$$_{n}$x$_{n}^{)})$). This\\nensures outputs are always between 0 and 1.\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"formula\",\n",
            "                    \"source_heading_number\": \"2.1.1.2\",\n",
            "                    \"source_heading_title\": \"Logistic Regression\",\n",
            "                    \"text\": \"Logistic function: $\\sigma$(z) = 1 / (1 + e$^{-z}$)\"\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"formula\",\n",
            "                    \"source_heading_number\": \"2.1.1.2\",\n",
            "                    \"source_heading_title\": \"Logistic Regression\",\n",
            "                    \"text\": \"Log-odds: ln(p/(1-p)) = $\\beta$$_{0}$ + $\\beta$$_{1}$x$_{1}$ + ... + $\\beta$$_{n}$x$_{n}$\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.2\",\n",
            "                    \"source_heading_title\": \"Logistic Regression\",\n",
            "                    \"text\": \"Logistic regression is widely used for binary classification and can be\\nextended to multi-class problems. It provides probabilistic outputs and\\ncoefficient interpretability, making it valuable for many applications.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.3\",\n",
            "                    \"source_heading_title\": \"Regularization Techniques\",\n",
            "                    \"text\": \"Regularization techniques prevent overfitting by adding penalty terms to the\\nloss function. They constrain model complexity and improve generalization\\nto unseen data, especially important when dealing with high-dimensional\\ndatasets.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.3\",\n",
            "                    \"source_heading_title\": \"Regularization Techniques\",\n",
            "                    \"text\": \"L1 regularization (Lasso) adds the sum of absolute values of coefficients as a\\npenalty, promoting sparsity and automatic feature selection. L2\\nregularization (Ridge) adds the sum of squared coefficients, shrinking\\ncoefficients towards zero but keeping all features.\"\n",
            "                  },\n",
            "                   {\n",
            "                    \"type\": \"paragraph\",\n",
            "                    \"source_heading_number\": \"2.1.1.3\",\n",
            "                    \"source_heading_title\": \"Regularization Techniques\",\n",
            "                    \"text\": \"Elastic Net combines L1 and L2 regularization.\"\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(json_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoLIlwkD3E9V"
      },
      "source": [
        "------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "zdYl9BSg609e"
      },
      "outputs": [],
      "source": [
        "cleaned_json_content = re.sub(r'(?<!\\\\)\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', json_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Fwoc6MA-86ha"
      },
      "outputs": [],
      "source": [
        "# Remove triple backticks and optional \"json\"\n",
        "cleaned_json_content = cleaned_json_content.strip().removeprefix(\"```json\").removeprefix(\"```\").removesuffix(\"```\").strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "NhoGTVc08nMG",
        "outputId": "97c74a27-498b-4737-c6e5-61d175332c92"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\\n  \"document_metadata\": {\\n    \"title\": \"Machine Learning Fundamentals\",\\n    \"document_type\": \"technical\",\\n    \"total_pages\": \"N/A\",\\n    \"language\": \"en\",\\n    \"has_tables\": false,\\n    \"has_charts\": false,\\n    \"has_images\": false\\n  },\\n  \"document_structure\": {\\n    \"chapters\": [\\n      {\\n        \"chapter_number\": \"1\",\\n        \"title\": \"Foundations of Machine Learning\",\\n        \"sections\": [\\n          {\\n            \"section_number\": \"1.1\",\\n            \"title\": \"Introduction to Machine Learning\",\\n            \"subsections\": [\\n              {\\n                \"subsection_number\": \"1.1.1\",\\n                \"title\": \"What is Machine Learning?\",\\n                \"content_elements\": [\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.1.1\",\\n                    \"source_heading_title\": \"Definition and Scope\",\\n                    \"text\": \"Machine Learning (ML) is a subset of artificial intelligence that enables\\\\ncomputers to learn and improve from experience without being explicitly\\\\nprogrammed. It involves the development of algorithms and statistical\\\\nmodels that computer systems use to perform specific tasks effectively\\\\nwithout using explicit instructions.\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.1.1\",\\n                    \"source_heading_title\": \"Definition and Scope\",\\n                    \"text\": \"The core principle of machine learning is to build mathematical models\\\\nbased on training data to make predictions or decisions without being\\\\nexplicitly programmed for each scenario. This paradigm shift from traditional\\\\nprogramming approaches allows systems to adapt and improve their\\\\nperformance over time.\"\\n                  },\\n                  {\\n                    \"type\": \"note\",\\n                    \"source_heading_number\": \"1.1.1.1\",\\n                    \"source_heading_title\": \"Definition and Scope\",\\n                    \"text\": \"Machine learning is fundamentally different from traditional\\\\nprogramming where we write explicit rules. In ML, we provide data and\\\\nexpected outputs, and the algorithm learns the rules.\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.1.2\",\\n                    \"source_heading_title\": \"Historical Context\",\\n                    \"text\": \"The concept of machine learning has its roots in the 1940s and 1950s, with\\\\nearly pioneers like Alan Turing proposing the idea of machines that could\\\\nlearn. The field has evolved through several key periods:\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.1.2\",\\n                    \"source_heading_title\": \"Historical Context\",\\n                    \"text\": \"The symbolic AI period (1950s-1980s) focused on rule-based systems and\\\\nexpert systems. The connectionist revival (1980s-1990s) brought neural\\\\nnetworks back into prominence. The statistical learning era (1990s-2000s)\\\\nemphasized probabilistic models and support vector machines.\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.1.2\",\\n                    \"source_heading_title\": \"Historical Context\",\\n                    \"text\": \"The current deep learning revolution (2010s-present) has been enabled by\\\\nincreased computational power, large datasets, and improved algorithms,\\\\nleading to breakthroughs in computer vision, natural language processing,\\\\nand other domains.\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.1.3\",\\n                    \"source_heading_title\": \"Modern Applications\",\\n                    \"text\": \"Machine learning has found applications across numerous industries and\\\\ndomains. In healthcare, ML algorithms assist in medical diagnosis, drug\\\\ndiscovery, and personalized treatment plans. Financial services use ML for\\\\nfraud detection, algorithmic trading, and risk assessment.\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.1.3\",\\n                    \"source_heading_title\": \"Modern Applications\",\\n                    \"text\": \"Technology companies leverage ML for recommendation systems, search\\\\nengines, and autonomous vehicles. Other applications include natural\\\\nlanguage processing for chatbots and translation services, computer vision\\\\nfor image recognition and analysis, and predictive maintenance in\\\\nmanufacturing.\"\\n                  },\\n                  {\\n                    \"type\": \"callout\",\\n                    \"source_heading_number\": \"1.1.1.3\",\\n                    \"source_heading_title\": \"Modern Applications\",\\n                    \"text\": \"The success of ML applications depends heavily on data quality,\\\\nappropriate algorithm selection, and proper evaluation metrics.\"\\n                  }\\n                ]\\n              },\\n              {\\n                \"subsection_number\": \"1.1.2\",\\n                \"title\": \"Types of Machine Learning\",\\n                \"content_elements\": [\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.2.1\",\\n                    \"source_heading_title\": \"Supervised Learning\",\\n                    \"text\": \"Supervised learning involves training algorithms on labeled datasets where\\\\nboth input features and corresponding output labels are provided. The goal is\\\\nto learn a mapping function from inputs to outputs that can generalize to\\\\nnew, unseen data.\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.2.1\",\\n                    \"source_heading_title\": \"Supervised Learning\",\\n                    \"text\": \"Classification tasks involve predicting discrete categorical labels, such as\\\\nemail spam detection or image recognition. Regression tasks involve\\\\npredicting continuous numerical values, such as house prices or stock\\\\nmarket predictions.\"\\n                  },\\n                  {\\n                    \"type\": \"code\",\\n                    \"source_heading_number\": \"1.1.2.1\",\\n                    \"source_heading_title\": \"Supervised Learning\",\\n                    \"text\": \"# Example of supervised learning structure\\\\nX_train = [[feature1, feature2, ...],\\\\n...] # Training features\\\\ny_train = [label1, label2, ...] # Training labels\\\\nmodel.fit(X_train, y_train) # Learn from data\\\\npredictions = model.predict(X_test) # Make predictions\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.2.2\",\\n                    \"source_heading_title\": \"Unsupervised Learning\",\\n                    \"text\": \"Unsupervised learning works with unlabeled data to discover hidden\\\\npatterns, structures, or relationships within the dataset. Without explicit\\\\ntarget variables, these algorithms must identify meaningful patterns\\\\nindependently.\"\\n                  },\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.2.2\",\\n                    \"source_heading_title\": \"Unsupervised Learning\",\\n                    \"text\": \"Common unsupervised learning tasks include clustering (grouping similar\\\\ndata points), dimensionality reduction (reducing the number of features\\\\nwhile preserving important information), and anomaly detection (identifying\\\\noutliers or unusual patterns).\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.2.2\",\\n                    \"source_heading_title\": \"Unsupervised Learning\",\\n                    \"text\": \"Applications include customer segmentation, data compression, and\\\\nexploratory data analysis. These techniques are particularly valuable for\\\\nunderstanding complex datasets and preprocessing data for supervised\\\\nlearning tasks.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.2.3\",\\n                    \"source_heading_title\": \"Reinforcement Learning\",\\n                    \"text\": \"Reinforcement learning involves training agents to make sequential\\\\ndecisions in an environment to maximize cumulative rewards. The agent\\\\nlearns through interaction with the environment, receiving feedback in the\\\\nform of rewards or penalties.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.2.3\",\\n                    \"source_heading_title\": \"Reinforcement Learning\",\\n                    \"text\": \"Key components include the agent (decision maker), environment (world in\\\\nwhich the agent operates), actions (choices available to the agent), states\\\\n(current situation), and rewards (feedback signal). The agent learns an\\\\noptimal policy that maps states to actions.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.1.2.3\",\\n                    \"source_heading_title\": \"Reinforcement Learning\",\\n                    \"text\": \"Applications include game playing (chess, Go), autonomous navigation,\\\\nrobotics, and resource allocation. Notable successes include AlphaGo,\\\\nautonomous vehicles, and recommendation systems that adapt to user\\\\npreferences over time.\"\\n                  }\\n                ]\\n              }\\n            ]\\n          },\\n          {\\n            \"section_number\": \"1.2\",\\n            \"title\": \"Mathematical Foundations\",\\n            \"subsections\": [\\n              {\\n                \"subsection_number\": \"1.2.1\",\\n                \"title\": \"Linear Algebra\",\\n                \"content_elements\": [\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.1.1\",\\n                    \"source_heading_title\": \"Vectors and Matrices\",\\n                    \"text\": \"Vectors and matrices are fundamental mathematical structures in machine\\\\nlearning. A vector is an ordered collection of numbers, representing features\\\\nor data points in n-dimensional space. Matrices are rectangular arrays of\\\\nnumbers, often used to represent datasets or transformation operations.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.1.1\",\\n                    \"source_heading_title\": \"Vectors and Matrices\",\\n                    \"text\": \"Vector operations include addition, scalar multiplication, and dot products.\\\\nMatrix operations include multiplication, transposition, and inversion. These\\\\noperations form the basis for many machine learning algorithms.\"\\n                  },\\n                  {\\n                    \"type\": \"formula\",\\n                    \"source_heading_number\": \"1.2.1.1\",\\n                    \"source_heading_title\": \"Vectors and Matrices\",\\n                    \"text\": \"Vector dot product: a $\\\\\\\\cdot$ b = $\\\\\\\\Sigma$(a$_{i}$ $\\\\times$ b$_{i}$)\"\\n                  },\\n                  {\\n                    \"type\": \"formula\",\\n                     \"source_heading_number\": \"1.2.1.1\",\\n                    \"source_heading_title\": \"Vectors and Matrices\",\\n                    \"text\": \"Matrix multiplication: C = A $\\\\times$ B, where C$_{ij}$ = $\\\\\\\\Sigma$(A$_{ik}$ $\\\\times$ B$_{kj}$)\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.1.2\",\\n                    \"source_heading_title\": \"Eigenvalues and Eigenvectors\",\\n                    \"text\": \"Eigenvalues and eigenvectors are crucial concepts in linear algebra with\\\\nsignificant applications in machine learning. An eigenvector of a matrix A is a\\\\nnon-zero vector v such that Av = $\\\\\\\\lambda$v, where $\\\\\\\\lambda$ is the corresponding eigenvalue.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.1.2\",\\n                    \"source_heading_title\": \"Eigenvalues and Eigenvectors\",\\n                    \"text\": \"These concepts are fundamental to dimensionality reduction techniques like\\\\nPrincipal Component Analysis (PCA), where eigenvectors represent principal\\\\ncomponents and eigenvalues indicate their importance. They also play roles\\\\nin spectral clustering and graph-based algorithms.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.1.2\",\\n                    \"source_heading_title\": \"Eigenvalues and Eigenvectors\",\\n                    \"text\": \"Computing eigenvalues and eigenvectors helps understand the intrinsic\\\\nproperties of data transformations and can reveal important structural\\\\ninformation about datasets.\"\\n                  }\\n                ]\\n              },\\n              {\\n                \"subsection_number\": \"1.2.2\",\\n                \"title\": \"Statistics and Probability\",\\n                \"content_elements\": [\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.2.1\",\\n                    \"source_heading_title\": \"Probability Distributions\",\\n                    \"text\": \"Probability distributions describe how probabilities are distributed over possible\\\\noutcomes of a random variable. Understanding distributions is essential for\\\\nmodeling uncertainty in machine learning systems.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.2.1\",\\n                    \"source_heading_title\": \"Probability Distributions\",\\n                    \"text\": \"Common distributions include the normal (Gaussian) distribution for continuous\\\\nvariables, binomial distribution for binary outcomes, and Poisson distribution for\\\\ncount data. Each distribution has specific parameters that characterize its shape\\\\nand properties.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.2.1\",\\n                    \"source_heading_title\": \"Probability Distributions\",\\n                    \"text\": \"Machine learning algorithms often make assumptions about data distributions. For\\\\nexample, linear regression assumes normally distributed residuals, while Naive\\\\nBayes assumes conditional independence of features given the class.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.2.2\",\\n                    \"source_heading_title\": \"Bayes\\' Theorem\",\\n                    \"text\": \"Bayes\\' theorem is a fundamental principle in probability theory that describes how\\\\nto update beliefs based on new evidence. It forms the foundation for Bayesian\\\\nmachine learning approaches.\"\\n                  },\\n                  {\\n                    \"type\": \"formula\",\\n                    \"source_heading_number\": \"1.2.2.2\",\\n                    \"source_heading_title\": \"Bayes\\' Theorem\",\\n                    \"text\": \"P(A|B) = P(B|A) $\\\\times$ P(A) / P(B)\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.2.2.2\",\\n                    \"source_heading_title\": \"Bayes\\' Theorem\",\\n                    \"text\": \"In machine learning contexts, Bayes\\' theorem is used in classification algorithms\\\\nlike Naive Bayes, Bayesian networks, and Bayesian optimization. It provides a\\\\nprincipled way to incorporate prior knowledge and update predictions as new data\\\\nbecomes available.\"\\n                  }\\n                ]\\n              }\\n            ]\\n          },\\n          {\\n            \"section_number\": \"1.3\",\\n            \"title\": \"Data Processing and Feature Engineering\",\\n            \"subsections\": [\\n              {\\n                \"subsection_number\": \"1.3.1\",\\n                \"title\": \"Data Preprocessing\",\\n                \"content_elements\": [\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.1.1\",\\n                    \"source_heading_title\": \"Data Cleaning\",\\n                    \"text\": \"Data cleaning is the process of identifying and correcting errors,\\\\ninconsistencies, and inaccuracies in datasets. Real-world data often\\\\ncontains noise, outliers, duplicate records, and formatting issues that can\\\\nnegatively impact model performance.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.1.1\",\\n                    \"source_heading_title\": \"Data Cleaning\",\\n                    \"text\": \"Common data cleaning tasks include removing duplicates, handling\\\\ninconsistent formatting, correcting typos, and dealing with outliers.\\\\nAutomated tools and manual inspection are often combined to ensure data\\\\nquality.\"\\n                  },\\n                  {\\n                    \"type\": \"code\",\\n                    \"source_heading_number\": \"1.3.1.1\",\\n                    \"source_heading_title\": \"Data Cleaning\",\\n                    \"text\": \"# Example data cleaning operations\\\\ndf = df.drop_duplicates() # Remove duplicate rows\\\\ndf[\\'column\\'] = df[\\'column\\'].str.lower() # Standardize text case\\\\ndf = df[df[\\'value\\'] < threshold] # Remove outliers\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.1.2\",\\n                    \"source_heading_title\": \"Missing Data Handling\",\\n                    \"text\": \"Missing data is a common challenge in machine learning projects. Different\\\\nstrategies exist for handling missing values, including deletion (removing\\\\nrows or columns with missing values), imputation (filling in missing values),\\\\nand using algorithms that can handle missing data directly.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.1.2\",\\n                    \"source_heading_title\": \"Missing Data Handling\",\\n                    \"text\": \"Imputation methods include mean/median/mode imputation for simple\\\\ncases, regression imputation for more sophisticated approaches, and\\\\nmultiple imputation for maintaining uncertainty estimates. The choice\\\\ndepends on the nature of the missing data and the specific application.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.1.2\",\\n                    \"source_heading_title\": \"Missing Data Handling\",\\n                    \"text\": \"Understanding whether data is missing completely at random (MCAR),\\\\nmissing at random (MAR), or missing not at random (MNAR) is crucial for\\\\nchoosing appropriate handling strategies.\"\\n                  }\\n                ]\\n              },\\n              {\\n                \"subsection_number\": \"1.3.2\",\\n                \"title\": \"Feature Selection and Extraction\",\\n                \"content_elements\": [\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.2.1\",\\n                    \"source_heading_title\": \"Dimensionality Reduction\",\\n                    \"text\": \"Dimensionality reduction techniques aim to reduce the number of features while\\\\npreserving the most important information. This helps combat the curse of\\\\ndimensionality, improves computational efficiency, and can enhance model\\\\ninterpretability.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.2.1\",\\n                    \"source_heading_title\": \"Dimensionality Reduction\",\\n                    \"text\": \"Principal Component Analysis (PCA) is a popular linear dimensionality reduction\\\\ntechnique that finds orthogonal projections that maximize variance. Non-linear\\\\nmethods like t-SNE and UMAP are useful for visualization and capturing complex\\\\ndata structures.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.2.1\",\\n                    \"source_heading_title\": \"Dimensionality Reduction\",\\n                    \"text\": \"Feature selection methods include filter methods (based on statistical measures),\\\\nwrapper methods (using model performance), and embedded methods (integrated\\\\ninto the learning algorithm).\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.2.2\",\\n                    \"source_heading_title\": \"Feature Scaling\",\\n                    \"text\": \"Feature scaling ensures that all features contribute equally to the learning process,\\\\npreventing features with larger scales from dominating the algorithm. Different\\\\nscaling methods are appropriate for different situations.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"1.3.2.2\",\\n                    \"source_heading_title\": \"Feature Scaling\",\\n                    \"text\": \"Min-max scaling transforms features to a fixed range (typically 0-1), while\\\\nstandardization (z-score normalization) centers features around zero with unit\\\\nvariance. Robust scaling uses median and interquartile range to handle outliers\\\\nbetter.\"\\n                  },\\n                  {\\n                    \"type\": \"formula\",\\n                     \"source_heading_number\": \"1.3.2.2\",\\n                    \"source_heading_title\": \"Feature Scaling\",\\n                    \"text\": \"Min-max scaling: X\\' = (X - min(X)) / (max(X) - min(X))\"\\n                  },\\n                  {\\n                    \"type\": \"formula\",\\n                    \"source_heading_number\": \"1.3.2.2\",\\n                    \"source_heading_title\": \"Feature Scaling\",\\n                    \"text\": \"Standardization: X\\' = (X - $\\\\\\\\mu$) / $\\\\\\\\sigma$\"\\n                  }\\n                ]\\n              }\\n            ]\\n          }\\n        ]\\n      },\\n      {\\n        \"chapter_number\": \"2\",\\n        \"title\": \"Core Machine Learning Algorithms\",\\n        \"sections\": [\\n          {\\n            \"section_number\": \"2.1\",\\n            \"title\": \"Supervised Learning Algorithms\",\\n            \"subsections\": [\\n              {\\n                \"subsection_number\": \"2.1.1\",\\n                \"title\": \"Linear Models\",\\n                \"content_elements\": [\\n                  {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.1\",\\n                    \"source_heading_title\": \"Linear Regression\",\\n                    \"text\": \"Linear regression is a fundamental supervised learning algorithm that\\\\nmodels the relationship between a dependent variable and independent\\\\nvariables by fitting a linear equation to the observed data. It assumes a linear\\\\nrelationship between features and the target variable.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.1\",\\n                    \"source_heading_title\": \"Linear Regression\",\\n                    \"text\": \"The algorithm finds the best-fitting line through the data points by minimizing\\\\nthe sum of squared residuals. The resulting model can be expressed as y = $\\\\beta$$_{0}$\\\\n+ $\\\\beta$$_{1}$x$_{1}$ + $\\\\beta$$_{2}$x$_{2}$ + ... + $\\\\beta$$_{n}$x$_{n}$, where $\\\\beta$ coefficients represent the weights of each\\\\nfeature.\"\\n                  },\\n                   {\\n                    \"type\": \"code\",\\n                    \"source_heading_number\": \"2.1.1.1\",\\n                    \"source_heading_title\": \"Linear Regression\",\\n                    \"text\": \"# Linear regression implementation concept\\\\nimport numpy as np\\\\nfrom sklearn.linear_model import LinearRegression\\\\n# Create and train model\\\\nmodel = LinearRegression().fit(X_train, y_train)\\\\npredictions = model.predict(X_test) # Make predictions\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.1\",\\n                    \"source_heading_title\": \"Linear Regression\",\\n                    \"text\": \"Linear regression is interpretable, computationally efficient, and serves as a\\\\nbaseline for many problems. However, it assumes linearity and can be\\\\nsensitive to outliers and multicollinearity.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.2\",\\n                    \"source_heading_title\": \"Logistic Regression\",\\n                    \"text\": \"Logistic regression extends linear regression to classification problems by\\\\nusing the logistic function to model the probability of class membership.\\\\nDespite its name, it\\'s a classification algorithm that outputs probabilities\\\\nbetween 0 and 1.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.2\",\\n                    \"source_heading_title\": \"Logistic Regression\",\\n                    \"text\": \"The logistic function (sigmoid) transforms the linear combination of features\\\\ninto a probability: P(y=1|x) = 1 / (1 + e$^{-(}\\\\beta$$_{0}$ + $\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\beta$$_{n}$x$_{n}^{)})$). This\\\\nensures outputs are always between 0 and 1.\"\\n                  },\\n                  {\\n                    \"type\": \"formula\",\\n                    \"source_heading_number\": \"2.1.1.2\",\\n                    \"source_heading_title\": \"Logistic Regression\",\\n                    \"text\": \"Logistic function: $\\\\\\\\sigma$(z) = 1 / (1 + e$^{-z}$)\"\\n                  },\\n                  {\\n                    \"type\": \"formula\",\\n                    \"source_heading_number\": \"2.1.1.2\",\\n                    \"source_heading_title\": \"Logistic Regression\",\\n                    \"text\": \"Log-odds: ln(p/(1-p)) = $\\\\beta$$_{0}$ + $\\\\beta$$_{1}$x$_{1}$ + ... + $\\\\beta$$_{n}$x$_{n}$\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.2\",\\n                    \"source_heading_title\": \"Logistic Regression\",\\n                    \"text\": \"Logistic regression is widely used for binary classification and can be\\\\nextended to multi-class problems. It provides probabilistic outputs and\\\\ncoefficient interpretability, making it valuable for many applications.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.3\",\\n                    \"source_heading_title\": \"Regularization Techniques\",\\n                    \"text\": \"Regularization techniques prevent overfitting by adding penalty terms to the\\\\nloss function. They constrain model complexity and improve generalization\\\\nto unseen data, especially important when dealing with high-dimensional\\\\ndatasets.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.3\",\\n                    \"source_heading_title\": \"Regularization Techniques\",\\n                    \"text\": \"L1 regularization (Lasso) adds the sum of absolute values of coefficients as a\\\\npenalty, promoting sparsity and automatic feature selection. L2\\\\nregularization (Ridge) adds the sum of squared coefficients, shrinking\\\\ncoefficients towards zero but keeping all features.\"\\n                  },\\n                   {\\n                    \"type\": \"paragraph\",\\n                    \"source_heading_number\": \"2.1.1.3\",\\n                    \"source_heading_title\": \"Regularization Techniques\",\\n                    \"text\": \"Elastic Net combines L1 and L2 regularization.\"\\n                  }\\n                ]\\n              }\\n            ]\\n          }\\n        ]\\n      }\\n    ]\\n  }\\n}'"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_json_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "cKiwHln_8knn"
      },
      "outputs": [],
      "source": [
        "load_json = json.loads(cleaned_json_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s86R5q0R0Ham",
        "outputId": "1a9457ff-4608-4b05-aec4-4a27a4331ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Foundations of Machine Learning\n",
            "  1.1 Introduction to Machine Learning\n",
            "    1.1.1 What is Machine Learning?\n",
            "    1.1.2 Types of Machine Learning\n",
            "  1.2 Mathematical Foundations\n",
            "    1.2.1 Linear Algebra\n",
            "    1.2.2 Statistics and Probability\n",
            "  1.3 Data Processing and Feature Engineering\n",
            "    1.3.1 Data Preprocessing\n",
            "    1.3.2 Feature Selection and Extraction\n",
            "2 Core Machine Learning Algorithms\n",
            "  2.1 Supervised Learning Algorithms\n",
            "    2.1.1 Linear Models\n"
          ]
        }
      ],
      "source": [
        "data = load_json\n",
        "\n",
        "# Recursive function to print sections and subsections\n",
        "def print_sections(sections, indent=1):\n",
        "    for sec in sections:\n",
        "        number = sec.get(\"section_number\") or sec.get(\"subsection_number\")\n",
        "        title = sec.get(\"title\", \"\")\n",
        "        print(\"  \" * indent + f\"{number} {title}\")\n",
        "        if \"subsections\" in sec:\n",
        "            print_sections(sec[\"subsections\"], indent + 1)\n",
        "\n",
        "# Start from chapters\n",
        "for chapter in data[\"document_structure\"][\"chapters\"]:\n",
        "    print(f'{chapter[\"chapter_number\"]} {chapter[\"title\"]}')\n",
        "    print_sections(chapter[\"sections\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ2UP5hJ0HX8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czW5G5DQ0HVt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
