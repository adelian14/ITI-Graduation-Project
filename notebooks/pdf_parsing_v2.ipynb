{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AIWNL-6VE3uw"
      },
      "outputs": [],
      "source": [
        "# !pip install google-generativeai\n",
        "# !pip install pypdf\n",
        "# !pip install pdf2image\n",
        "# !pip install pillow\n",
        "# !pip install python-dotenv\n",
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IHfPGz4vmSQ9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOqwdovSE3oH"
      },
      "outputs": [],
      "source": [
        "# Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your-api-kay\"\n",
        "\n",
        "# Load Gemini model\n",
        "model = genai.GenerativeModel(\n",
        "    \"gemini-2.5-flash-preview-04-17\", #\"gemini-2.5-flash-preview-04-17\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ocq6nF5KE3lO"
      },
      "outputs": [],
      "source": [
        "def convert_pdf_to_images(pdf_path, output_folder, dpi=300):\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Convert PDF pages to images\n",
        "    images = convert_from_path(pdf_path, dpi=dpi)\n",
        "\n",
        "    # Save images to the output folder\n",
        "    image_paths = []\n",
        "    for i, image in enumerate(images):\n",
        "        image_path = os.path.join(output_folder, f'page_{i+1}.jpg')\n",
        "        image.save(image_path, 'JPEG')\n",
        "        image_paths.append(image_path)\n",
        "\n",
        "    return image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "-wLBkweGHLEx"
      },
      "outputs": [],
      "source": [
        "pdf_path = '/content/Machine Learning Fundamentals.pdf'\n",
        "output_folder = '/content/images'\n",
        "# image_paths = convert_pdf_to_images(pdf_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru_yVt1Iq02_",
        "outputId": "8e1e24a9-b5e5-4f1c-f440-5c7cc57a219b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/images': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "6hv8lZ09E3iH"
      },
      "outputs": [],
      "source": [
        "def batch_images(image_paths, batch_size=10):\n",
        "    \"\"\"Group images into batches for processing\"\"\"\n",
        "    for i in range(0, len(image_paths), batch_size):\n",
        "        yield image_paths[i:i + batch_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "75tmxDbTE3ff"
      },
      "outputs": [],
      "source": [
        "def ocr_with_gemini(image_paths, instruction):\n",
        "    images = [Image.open(path) for path in image_paths]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "        You are an expert document analysis AI with exceptional OCR capabilities.\n",
        "    Your task is to extract ALL textual content from the provided document images with perfect accuracy.\n",
        "\n",
        "    CRITICAL REQUIREMENTS:\n",
        "    1. ACCURACY: Every word, number, symbol, and punctuation mark must be captured exactly as shown\n",
        "    2. STRUCTURE: Maintain the original document structure, hierarchy, and formatting\n",
        "    3. COMPLETENESS: Do not skip any content, including headers, footers, page numbers, footnotes, watermarks, or marginalia\n",
        "    4. CONTEXT: Understand the document context to resolve ambiguous characters\n",
        "\n",
        "    {instruction}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    response = model.generate_content([prompt, *images])\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "9y0_Idq_E3ca"
      },
      "outputs": [],
      "source": [
        "def ocr_complex_document(image_paths):\n",
        "    instruction = \"\"\"\n",
        "    Extract ALL text content from these document pages.\n",
        "    For Tables:\n",
        "    - Use markdown table format with proper alignment\n",
        "    - Include all headers, subheaders, and merged cells\n",
        "    - Preserve numerical precision and units\n",
        "    - Note any table notes or footnotes\n",
        "\n",
        "    For Multi-column Text:\n",
        "    - Process columns in natural reading order (left to right, top to bottom)\n",
        "    - Clearly separate column content with appropriate breaks\n",
        "    - Maintain column-specific formatting\n",
        "\n",
        "    For Charts/Graphs:\n",
        "    - Describe chart type and purpose\n",
        "    - Extract all axis labels, legends, and data points\n",
        "    - Capture titles, captions, and source information\n",
        "    - Note any trends or key insights visible in the visual\n",
        "\n",
        "    For Special Elements:\n",
        "    - Preserve bullet points, numbered lists, and indentation\n",
        "    - Maintain emphasis (bold, italic, underline) using markdown\n",
        "    - Capture all hyperlinks and cross-references\n",
        "    - Include page numbers and section breaks\n",
        "\n",
        "    QUALITY ASSURANCE:\n",
        "    - Double-check all numerical data for accuracy\n",
        "    - Verify proper names, technical terms, and specialized vocabulary\n",
        "    - Ensure logical flow and coherence in extracted text\n",
        "    - Flag any unclear or potentially misread content with [UNCERTAIN: text]\n",
        "    Preserve all headers, footers, page numbers, and footnotes.\n",
        "    \"\"\"\n",
        "\n",
        "    return ocr_with_gemini(image_paths, instruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "bNYiZFaHE3W_"
      },
      "outputs": [],
      "source": [
        "def process_large_pdf(pdf_path, output_folder):\n",
        "    # Convert PDF to images\n",
        "    image_paths = convert_pdf_to_images(pdf_path, output_folder)\n",
        "\n",
        "    # Create batches of images (e.g., by chapter or section)\n",
        "    batches = batch_images(image_paths, 30)\n",
        "\n",
        "    full_text = \"\"\n",
        "    for i, batch in enumerate(batches):\n",
        "        print(f\"Processing batch {i+1}...\")\n",
        "        batch_text = ocr_with_gemini(batch, \"Extract all text, maintaining document structure\")\n",
        "        full_text += f\"\\n\\n--- BATCH {i+1} ---\\n\\n{batch_text}\"\n",
        "\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "bfVSXatimSRA",
        "outputId": "7e35ed1f-f26f-4174-d8f1-6ed11d239ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing batch 1...\n"
          ]
        }
      ],
      "source": [
        "text = process_large_pdf(pdf_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "1N2Amq2vLxxw"
      },
      "outputs": [],
      "source": [
        "def text_to_json_with_gemini(text_content):\n",
        "  \"\"\"Converts text content to JSON format using the Gemini model.\"\"\"\n",
        "\n",
        "  prompt = \"\"\"\n",
        "    You are a document structuring expert. Convert the provided text into a comprehensive, well-organized JSON structure.\n",
        "\n",
        "    REQUIREMENTS:\n",
        "    1. Create a logical, hierarchical JSON structure that reflects the document's organization\n",
        "    2. Preserve all content while organizing it meaningfully\n",
        "    3. Include metadata about the document structure and content\n",
        "\n",
        "  Convert the following text content into a structured JSON object.\n",
        "  Identify key sections, headings, paragraphs, tables, and other elements\n",
        "  and represent them appropriately in the JSON structure.\n",
        "\n",
        "\n",
        "  Text content to convert:\n",
        "\n",
        "  {}\n",
        "\n",
        "    JSON STRUCTURE TEMPLATE:\n",
        "    {{\n",
        "        \"document_metadata\": {{\n",
        "            \"title\": \"Document title if available\",\n",
        "            \"document_type\": \"academic|technical|report|manual|other\",\n",
        "            \"total_pages\": \"number of pages processed\",\n",
        "            \"language\": \"primary language detected\",\n",
        "            \"has_tables\": true/false,\n",
        "            \"has_charts\": true/false,\n",
        "            \"has_images\": true/false\n",
        "        }},\n",
        "        \"document_structure\": {{\n",
        "            \"sections\": [\n",
        "                {{\n",
        "                    \"section_number\": \"1\",\n",
        "                    \"title\": \"Section Title\",\n",
        "                    \"subsections\": [\n",
        "                        {{\n",
        "                            \"subsection_number\": \"1.1\",\n",
        "                            \"title\": \"Subsection Title\",\n",
        "                            \"content\": \"Full text content\",\n",
        "                            \"paragraphs\": [\"paragraph 1\", \"paragraph 2\"],\n",
        "                            \"lists\": [\"list items if any\"],\n",
        "                            \"tables\": [\n",
        "                                {{\n",
        "                                    \"table_number\": \"Table 1\",\n",
        "                                    \"caption\": \"Table caption\",\n",
        "                                    \"headers\": [\"Column 1\", \"Column 2\"],\n",
        "                                    \"rows\": [[\"Data 1\", \"Data 2\"]]\n",
        "                                }}\n",
        "                            ],\n",
        "                            \"figures\": [\n",
        "                                {{\n",
        "                                    \"figure_number\": \"Figure 1\",\n",
        "                                    \"caption\": \"Figure caption\",\n",
        "                                    \"description\": \"Description of visual content\"\n",
        "                                }}\n",
        "                            ]\n",
        "                        }}\n",
        "                    ]\n",
        "                }}\n",
        "            ]\n",
        "        }},\n",
        "    }}\n",
        "\n",
        "    ADAPTATION RULES:\n",
        "    - If document doesn't have clear sections, organize by pages or logical breaks\n",
        "    - For tables without clear structure, preserve as formatted text\n",
        "    - For charts/graphs, include detailed descriptions in figures array\n",
        "    - Adapt structure to match document type (academic papers, reports, manuals, etc.)\n",
        "    - Ensure all content is preserved even if structure is unclear\n",
        "\n",
        "    OUTPUT: Valid JSON only, no additional text or explanations.\n",
        "  \"\"\".format(text_content)\n",
        "\n",
        "  response = model.generate_content([prompt])\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yQJVL6X2L1Xb",
        "outputId": "024f7bdb-42b9-48a5-ee3a-b35d956ed0a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"document_metadata\": {\n",
            "    \"title\": \"Machine Learning Fundamentals\",\n",
            "    \"document_type\": \"technical\",\n",
            "    \"total_pages\": \"unknown\",\n",
            "    \"language\": \"English\",\n",
            "    \"has_tables\": false,\n",
            "    \"has_charts\": false,\n",
            "    \"has_images\": false\n",
            "  },\n",
            "  \"document_structure\": {\n",
            "    \"table_of_contents\": [\n",
            "      \"Chapter 1: Foundations of Machine Learning\",\n",
            "      \"1.1 Introduction to Machine Learning\",\n",
            "      \"1.1.1 What is Machine Learning?\",\n",
            "      \"1.1.1.1 Definition and Scope\",\n",
            "      \"1.1.1.2 Historical Context\",\n",
            "      \"1.1.1.3 Modern Applications\",\n",
            "      \"1.1.2 Types of Machine Learning\",\n",
            "      \"1.1.2.1 Supervised Learning\",\n",
            "      \"1.1.2.2 Unsupervised Learning\",\n",
            "      \"1.1.2.3 Reinforcement Learning\",\n",
            "      \"1.2 Mathematical Foundations\",\n",
            "      \"1.2.1 Linear Algebra\",\n",
            "      \"1.2.1.1 Vectors and Matrices\",\n",
            "      \"1.2.1.2 Eigenvalues and Eigenvectors\",\n",
            "      \"1.2.2 Statistics and Probability\",\n",
            "      \"1.2.2.1 Probability Distributions\",\n",
            "      \"1.2.2.2 Bayes' Theorem\",\n",
            "      \"1.3 Data Processing and Feature Engineering\",\n",
            "      \"1.3.1 Data Preprocessing\",\n",
            "      \"1.3.1.1 Data Cleaning\",\n",
            "      \"1.3.1.2 Missing Data Handling\",\n",
            "      \"1.3.2 Feature Selection and Extraction\",\n",
            "      \"1.3.2.1 Dimensionality Reduction\",\n",
            "      \"1.3.2.2 Feature Scaling\",\n",
            "      \"Chapter 2: Core Machine Learning Algorithms\",\n",
            "      \"2.1 Supervised Learning Algorithms\",\n",
            "      \"2.1.1 Linear Models\",\n",
            "      \"2.1.1.1 Linear Regression\",\n",
            "      \"2.1.1.2 Logistic Regression\",\n",
            "      \"2.1.1.3 Regularization Techniques\",\n",
            "      \"2.1.2 Tree-Based Methods\",\n",
            "      \"2.1.2.1 Decision Trees\",\n",
            "      \"2.1.2.2 Random Forests\",\n",
            "      \"2.1.2.3 Gradient Boosting\",\n",
            "      \"2.2 Unsupervised Learning Algorithms\",\n",
            "      \"2.2.1 Clustering Algorithms\",\n",
            "      \"2.2.1.1 K-Means Clustering\",\n",
            "      \"2.2.1.2 Hierarchical Clustering\",\n",
            "      \"2.2.1.3 DBSCAN\",\n",
            "      \"2.2.2 Dimensionality Reduction\",\n",
            "      \"2.2.2.1 Principal Component Analysis\",\n",
            "      \"2.2.2.2 t-SNE\",\n",
            "      \"2.3 Model Evaluation and Selection\",\n",
            "      \"2.3.1 Performance Metrics\",\n",
            "      \"2.3.1.1 Classification Metrics\",\n",
            "      \"2.3.1.2 Regression Metrics\",\n",
            "      \"2.3.2 Cross-Validation\",\n",
            "      \"2.3.2.1 K-Fold Cross-Validation\",\n",
            "      \"2.3.2.2 Stratified Sampling\"\n",
            "    ],\n",
            "    \"sections\": [\n",
            "      {\n",
            "        \"type\": \"chapter\",\n",
            "        \"number\": \"1\",\n",
            "        \"title\": \"Foundations of Machine Learning\",\n",
            "        \"content\": null,\n",
            "        \"subsections\": [\n",
            "          {\n",
            "            \"type\": \"section\",\n",
            "            \"number\": \"1.1\",\n",
            "            \"title\": \"Introduction to Machine Learning\",\n",
            "            \"content\": null,\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"1.1.1\",\n",
            "                \"title\": \"What is Machine Learning?\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.1.1.1\",\n",
            "                    \"title\": \"Definition and Scope\",\n",
            "                    \"content\": \"Machine Learning (ML) is a subset of artificial intelligence that enables\\ncomputers to learn and improve from experience without being explicitly\\nprogrammed. It involves the development of algorithms and statistical\\nmodels that computer systems use to perform specific tasks effectively\\nwithout using explicit instructions.\\n\\nThe core principle of machine learning is to build mathematical models\\nbased on training data to make predictions or decisions without being\\nexplicitly programmed for each scenario. This paradigm shift from traditional\\nprogramming approaches allows systems to adapt and improve their\\nperformance over time.\\n\\nNote: Machine learning is fundamentally different from traditional\\nprogramming where we write explicit rules. In ML, we provide data and\\nexpected outputs, and the algorithm learns the rules.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Machine Learning (ML) is a subset of artificial intelligence that enables\\ncomputers to learn and improve from experience without being explicitly\\nprogrammed. It involves the development of algorithms and statistical\\nmodels that computer systems use to perform specific tasks effectively\\nwithout using explicit instructions.\",\n",
            "                      \"The core principle of machine learning is to build mathematical models\\nbased on training data to make predictions or decisions without being\\nexplicitly programmed for each scenario. This paradigm shift from traditional\\nprogramming approaches allows systems to adapt and improve their\\nperformance over time.\"\n",
            "                    ],\n",
            "                    \"notes\": [\n",
            "                      \"Machine learning is fundamentally different from traditional\\nprogramming where we write explicit rules. In ML, we provide data and\\nexpected outputs, and the algorithm learns the rules.\"\n",
            "                    ],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.1.1.2\",\n",
            "                    \"title\": \"Historical Context\",\n",
            "                    \"content\": \"The concept of machine learning has its roots in the 1940s and 1950s, with\\nearly pioneers like Alan Turing proposing the idea of machines that could\\nlearn. The field has evolved through several key periods:\\n\\nThe symbolic Al period (1950s-1980s) focused on rule-based systems and\\nexpert systems. The connectionist revival (1980s-1990s) brought neural\\nnetworks back into prominence. The statistical learning era (1990s-2000s)\\nemphasized probabilistic models and support vector machines.\\n\\nThe current deep learning revolution (2010s-present) has been enabled by\\nincreased computational power, large datasets, and improved algorithms,\\nleading to breakthroughs in computer vision, natural language processing,\\nand other domains.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"The concept of machine learning has its roots in the 1940s and 1950s, with\\nearly pioneers like Alan Turing proposing the idea of machines that could\\nlearn. The field has evolved through several key periods:\",\n",
            "                      \"The symbolic Al period (1950s-1980s) focused on rule-based systems and\\nexpert systems. The connectionist revival (1980s-1990s) brought neural\\nnetworks back into prominence. The statistical learning era (1990s-2000s)\\nemphasized probabilistic models and support vector machines.\",\n",
            "                      \"The current deep learning revolution (2010s-present) has been enabled by\\nincreased computational power, large datasets, and improved algorithms,\\nleading to breakthroughs in computer vision, natural language processing,\\nand other domains.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.1.1.3\",\n",
            "                    \"title\": \"Modern Applications\",\n",
            "                    \"content\": \"Machine learning has found applications across numerous industries and\\ndomains. In healthcare, ML algorithms assist in medical diagnosis, drug\\ndiscovery, and personalized treatment plans. Financial services use ML for\\nfraud detection, algorithmic trading, and risk assessment.\\n\\nTechnology companies leverage ML for recommendation systems, search\\nengines, and autonomous vehicles. Other applications include natural\\nlanguage processing for chatbots and translation services, computer vision\\nfor image recognition and analysis, and predictive maintenance in\\nmanufacturing.\\n\\nImportant: The success of ML applications depends heavily on data quality,\\nappropriate algorithm selection, and proper evaluation metrics.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Machine learning has found applications across numerous industries and\\ndomains. In healthcare, ML algorithms assist in medical diagnosis, drug\\ndiscovery, and personalized treatment plans. Financial services use ML for\\nfraud detection, algorithmic trading, and risk assessment.\",\n",
            "                      \"Technology companies leverage ML for recommendation systems, search\\nengines, and autonomous vehicles. Other applications include natural\\nlanguage processing for chatbots and translation services, computer vision\\nfor image recognition and analysis, and predictive maintenance in\\nmanufacturing.\"\n",
            "                    ],\n",
            "                    \"notes\": [\n",
            "                      \"The success of ML applications depends heavily on data quality,\\nappropriate algorithm selection, and proper evaluation metrics.\"\n",
            "                    ],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"1.1.2\",\n",
            "                \"title\": \"Types of Machine Learning\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.1.2.1\",\n",
            "                    \"title\": \"Supervised Learning\",\n",
            "                    \"content\": \"Supervised learning involves training algorithms on labeled datasets where\\nboth input features and corresponding output labels are provided. The goal is\\nto learn a mapping function from inputs to outputs that can generalize to\\nnew, unseen data.\\n\\nClassification tasks involve predicting discrete categorical labels, such as\\nemail spam detection or image recognition. Regression tasks involve\\npredicting continuous numerical values, such as house prices or stock\\nmarket predictions.\\n\\n# Example of supervised learning structure X_train = [[feature1, feature2, ...],\\n...] # Training features y_train = [label1, label2, ...] # Training labels\\nmodel.fit(X_train, y_train) # Learn from data predictions =\\nmodel.predict(X_test) # Make predictions\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Supervised learning involves training algorithms on labeled datasets where\\nboth input features and corresponding output labels are provided. The goal is\\nto learn a mapping function from inputs to outputs that can generalize to\\nnew, unseen data.\",\n",
            "                      \"Classification tasks involve predicting discrete categorical labels, such as\\nemail spam detection or image recognition. Regression tasks involve\\npredicting continuous numerical values, such as house prices or stock\\nmarket predictions.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [\n",
            "                      \"# Example of supervised learning structure X_train = [[feature1, feature2, ...],\\n...]\",\n",
            "                      \"# Training features y_train = [label1, label2, ...]\",\n",
            "                      \"# Training labels\\nmodel.fit(X_train, y_train)\",\n",
            "                      \"# Learn from data predictions =\\nmodel.predict(X_test)\",\n",
            "                      \"# Make predictions\"\n",
            "                    ],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.1.2.2\",\n",
            "                    \"title\": \"Unsupervised Learning\",\n",
            "                    \"content\": \"Unsupervised learning works with unlabeled data to discover hidden\\npatterns, structures, or relationships within the dataset. Without explicit\\ntarget variables, these algorithms must identify meaningful patterns\\nindependently.\\n\\nCommon unsupervised learning tasks include clustering (grouping similar\\ndata points), dimensionality reduction (reducing the number of features\\nwhile preserving important information), and anomaly detection (identifying\\noutliers or unusual patterns).\\n\\nApplications include customer segmentation, data compression, and\\nexploratory data analysis. These techniques are particularly valuable for\\nunderstanding complex datasets and preprocessing data for supervised\\nlearning tasks.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Unsupervised learning works with unlabeled data to discover hidden\\npatterns, structures, or relationships within the dataset. Without explicit\\ntarget variables, these algorithms must identify meaningful patterns\\nindependently.\",\n",
            "                      \"Common unsupervised learning tasks include clustering (grouping similar\\ndata points), dimensionality reduction (reducing the number of features\\nwhile preserving important information), and anomaly detection (identifying\\noutliers or unusual patterns).\",\n",
            "                      \"Applications include customer segmentation, data compression, and\\nexploratory data analysis. These techniques are particularly valuable for\\nunderstanding complex datasets and preprocessing data for supervised\\nlearning tasks.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.1.2.3\",\n",
            "                    \"title\": \"Reinforcement Learning\",\n",
            "                    \"content\": \"Reinforcement learning involves training agents to make sequential\\ndecisions in an environment to maximize cumulative rewards. The agent\\nlearns through interaction with the environment, receiving feedback in the\\nform of rewards or penalties.\\n\\nKey components include the agent (decision maker), environment (world in\\nwhich the agent operates), actions (choices available to the agent), states\\n(current situation), and rewards (feedback signal). The agent learns an\\noptimal policy that maps states to actions.\\n\\nApplications include game playing (chess, Go), autonomous navigation,\\nrobotics, and resource allocation. Notable successes include AlphaGo,\\nautonomous vehicles, and recommendation systems that adapt to user\\npreferences over time.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Reinforcement learning involves training agents to make sequential\\ndecisions in an environment to maximize cumulative rewards. The agent\\nlearns through interaction with the environment, receiving feedback in the\\nform of rewards or penalties.\",\n",
            "                      \"Key components include the agent (decision maker), environment (world in\\nwhich the agent operates), actions (choices available to the agent), states\\n(current situation), and rewards (feedback signal). The agent learns an\\noptimal policy that maps states to actions.\",\n",
            "                      \"Applications include game playing (chess, Go), autonomous navigation,\\nrobotics, and resource allocation. Notable successes include AlphaGo,\\nautonomous vehicles, and recommendation systems that adapt to user\\npreferences over time.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"type\": \"section\",\n",
            "            \"number\": \"1.2\",\n",
            "            \"title\": \"Mathematical Foundations\",\n",
            "            \"content\": null,\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"1.2.1\",\n",
            "                \"title\": \"Linear Algebra\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.2.1.1\",\n",
            "                    \"title\": \"Vectors and Matrices\",\n",
            "                    \"content\": \"Vectors and matrices are fundamental mathematical structures in machine\\nlearning. A vector is an ordered collection of numbers, representing features\\nor data points in n-dimensional space. Matrices are rectangular arrays of\\nnumbers, often used to represent datasets or transformation operations.\\n\\nVector operations include addition, scalar multiplication, and dot products.\\nMatrix operations include multiplication, transposition, and inversion. These\\noperations form the basis for many machine learning algorithms.\\n\\nVector dot product: a · b = Σ(a; × b;)\\nMatrix multiplication: C = A x B, where C¡¡ = Σ(Aik × Bkj)\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Vectors and matrices are fundamental mathematical structures in machine\\nlearning. A vector is an ordered collection of numbers, representing features\\nor data points in n-dimensional space. Matrices are rectangular arrays of\\nnumbers, often used to represent datasets or transformation operations.\",\n",
            "                      \"Vector operations include addition, scalar multiplication, and dot products.\\nMatrix operations include multiplication, transposition, and inversion. These\\noperations form the basis for many machine learning algorithms.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [\n",
            "                      \"Vector dot product: a · b = Σ(a; × b;)\",\n",
            "                      \"Matrix multiplication: C = A x B, where C¡¡ = Σ(Aik × Bkj)\"\n",
            "                    ],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.2.1.2\",\n",
            "                    \"title\": \"Eigenvalues and Eigenvectors\",\n",
            "                    \"content\": \"Eigenvalues and eigenvectors are crucial concepts in linear algebra with\\nsignificant applications in machine learning. An eigenvector of a matrix A is a\\nnon-zero vector v such that Av = Av, where X is the corresponding eigenvalue.\\n\\nThese concepts are fundamental to dimensionality reduction techniques like\\nPrincipal Component Analysis (PCA), where eigenvectors represent principal\\ncomponents and eigenvalues indicate their importance. They also play roles\\nin spectral clustering and graph-based algorithms.\\n\\nComputing eigenvalues and eigenvectors helps understand the intrinsic\\nproperties of data transformations and can reveal important structural\\ninformation about datasets.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Eigenvalues and eigenvectors are crucial concepts in linear algebra with\\nsignificant applications in machine learning. An eigenvector of a matrix A is a\\nnon-zero vector v such that Av = Av, where X is the corresponding eigenvalue.\",\n",
            "                      \"These concepts are fundamental to dimensionality reduction techniques like\\nPrincipal Component Analysis (PCA), where eigenvectors represent principal\\ncomponents and eigenvalues indicate their importance. They also play roles\\nin spectral clustering and graph-based algorithms.\",\n",
            "                      \"Computing eigenvalues and eigenvectors helps understand the intrinsic\\nproperties of data transformations and can reveal important structural\\ninformation about datasets.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"1.2.2\",\n",
            "                \"title\": \"Statistics and Probability\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.2.2.1\",\n",
            "                    \"title\": \"Probability Distributions\",\n",
            "                    \"content\": \"Probability distributions describe how probabilities are distributed over possible\\noutcomes of a random variable. Understanding distributions is essential for\\nmodeling uncertainty in machine learning systems.\\n\\nCommon distributions include the normal (Gaussian) distribution for continuous\\nvariables, binomial distribution for binary outcomes, and Poisson distribution for\\ncount data. Each distribution has specific parameters that characterize its shape\\nand properties.\\n\\nMachine learning algorithms often make assumptions about data distributions. For\\nexample, linear regression assumes normally distributed residuals, while Naive\\nBayes assumes conditional independence of features given the class.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Probability distributions describe how probabilities are distributed over possible\\noutcomes of a random variable. Understanding distributions is essential for\\nmodeling uncertainty in machine learning systems.\",\n",
            "                      \"Common distributions include the normal (Gaussian) distribution for continuous\\nvariables, binomial distribution for binary outcomes, and Poisson distribution for\\ncount data. Each distribution has specific parameters that characterize its shape\\nand properties.\",\n",
            "                      \"Machine learning algorithms often make assumptions about data distributions. For\\nexample, linear regression assumes normally distributed residuals, while Naive\\nBayes assumes conditional independence of features given the class.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.2.2.2\",\n",
            "                    \"title\": \"Bayes' Theorem\",\n",
            "                    \"content\": \"Bayes' theorem is a fundamental principle in probability theory that describes how\\nto update beliefs based on new evidence. It forms the foundation for Bayesian\\nmachine learning approaches.\\n\\nP(A|B) = P(B|A) × P(A) / P(B)\\n\\nIn machine learning contexts, Bayes' theorem is used in classification algorithms\\nlike Naive Bayes, Bayesian networks, and Bayesian optimization. It provides a\\nprincipled way to incorporate prior knowledge and update predictions as new data\\nbecomes available.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Bayes' theorem is a fundamental principle in probability theory that describes how\\nto update beliefs based on new evidence. It forms the foundation for Bayesian\\nmachine learning approaches.\",\n",
            "                      \"In machine learning contexts, Bayes' theorem is used in classification algorithms\\nlike Naive Bayes, Bayesian networks, and Bayesian optimization. It provides a\\nprincipled way to incorporate prior knowledge and update predictions as new data\\nbecomes available.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [\n",
            "                      \"P(A|B) = P(B|A) × P(A) / P(B)\"\n",
            "                    ],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"type\": \"section\",\n",
            "            \"number\": \"1.3\",\n",
            "            \"title\": \"Data Processing and Feature Engineering\",\n",
            "            \"content\": null,\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"1.3.1\",\n",
            "                \"title\": \"Data Preprocessing\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.3.1.1\",\n",
            "                    \"title\": \"Data Cleaning\",\n",
            "                    \"content\": \"Data cleaning is the process of identifying and correcting errors,\\ninconsistencies, and inaccuracies in datasets. Real-world data often\\ncontains noise, outliers, duplicate records, and formatting issues that can\\nnegatively impact model performance.\\n\\nCommon data cleaning tasks include removing duplicates, handling\\ninconsistent formatting, correcting typos, and dealing with outliers.\\nAutomated tools and manual inspection are often combined to ensure data\\nquality.\\n\\n# Example data cleaning operations df = df.drop_duplicates() # Remove\\nduplicate rows df['column'] = df['column'].str.lower() # Standardize text case\\ndf = df[df['value'] < threshold] # Remove outliers\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Data cleaning is the process of identifying and correcting errors,\\ninconsistencies, and inaccuracies in datasets. Real-world data often\\ncontains noise, outliers, duplicate records, and formatting issues that can\\nnegatively impact model performance.\",\n",
            "                      \"Common data cleaning tasks include removing duplicates, handling\\ninconsistent formatting, correcting typos, and dealing with outliers.\\nAutomated tools and manual inspection are often combined to ensure data\\nquality.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [\n",
            "                      \"# Example data cleaning operations df = df.drop_duplicates()\",\n",
            "                      \"# Remove\\nduplicate rows df['column'] = df['column'].str.lower()\",\n",
            "                      \"# Standardize text case\\ndf = df[df['value'] < threshold]\",\n",
            "                      \"# Remove outliers\"\n",
            "                    ],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.3.1.2\",\n",
            "                    \"title\": \"Missing Data Handling\",\n",
            "                    \"content\": \"Missing data is a common challenge in machine learning projects. Different\\nstrategies exist for handling missing values, including deletion (removing\\nrows or columns with missing values), imputation (filling in missing values),\\nand using algorithms that can handle missing data directly.\\n\\nImputation methods include mean/median/mode imputation for simple\\ncases, regression imputation for more sophisticated approaches, and\\nmultiple imputation for maintaining uncertainty estimates. The choice\\ndepends on the nature of the missing data and the specific application.\\n\\nUnderstanding whether data is missing completely at random (MCAR),\\nmissing at random (MAR), or missing not at random (MNAR) is crucial for\\nchoosing appropriate handling strategies.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Missing data is a common challenge in machine learning projects. Different\\nstrategies exist for handling missing values, including deletion (removing\\nrows or columns with missing values), imputation (filling in missing values),\\nand using algorithms that can handle missing data directly.\",\n",
            "                      \"Imputation methods include mean/median/mode imputation for simple\\ncases, regression imputation for more sophisticated approaches, and\\nmultiple imputation for maintaining uncertainty estimates. The choice\\ndepends on the nature of the missing data and the specific application.\",\n",
            "                      \"Understanding whether data is missing completely at random (MCAR),\\nmissing at random (MAR), or missing not at random (MNAR) is crucial for\\nchoosing appropriate handling strategies.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"1.3.2\",\n",
            "                \"title\": \"Feature Selection and Extraction\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.3.2.1\",\n",
            "                    \"title\": \"Dimensionality Reduction\",\n",
            "                    \"content\": \"Dimensionality reduction techniques aim to reduce the number of features while\\npreserving the most important information. This helps combat the curse of\\ndimensionality, improves computational efficiency, and can enhance model\\ninterpretability.\\n\\nPrincipal Component Analysis (PCA) is a popular linear dimensionality reduction\\ntechnique that finds orthogonal projections that maximize variance. Non-linear\\nmethods like t-SNE and UMAP are useful for visualization and capturing complex\\ndata structures.\\n\\nFeature selection methods include filter methods (based on statistical measures),\\nwrapper methods (using model performance), and embedded methods (integrated\\ninto the learning algorithm).\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Dimensionality reduction techniques aim to reduce the number of features while\\npreserving the most important information. This helps combat the curse of\\ndimensionality, improves computational efficiency, and can enhance model\\ninterpretability.\",\n",
            "                      \"Principal Component Analysis (PCA) is a popular linear dimensionality reduction\\ntechnique that finds orthogonal projections that maximize variance. Non-linear\\nmethods like t-SNE and UMAP are useful for visualization and capturing complex\\ndata structures.\",\n",
            "                      \"Feature selection methods include filter methods (based on statistical measures),\\nwrapper methods (using model performance), and embedded methods (integrated\\ninto the learning algorithm).\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"1.3.2.2\",\n",
            "                    \"title\": \"Feature Scaling\",\n",
            "                    \"content\": \"Feature scaling ensures that all features contribute equally to the learning process,\\npreventing features with larger scales from dominating the algorithm. Different\\nscaling methods are appropriate for different situations.\\n\\nMin-max scaling transforms features to a fixed range (typically 0-1), while\\nstandardization (z-score normalization) centers features around zero with unit\\nvariance. Robust scaling uses median and interquartile range to handle outliers\\nbetter.\\n\\nMin-max scaling: X' = (X - min(X)) / (max(X) - min(X))\\nStandardization: X' = (X - μ) / σ\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Feature scaling ensures that all features contribute equally to the learning process,\\npreventing features with larger scales from dominating the algorithm. Different\\nscaling methods are appropriate for different situations.\",\n",
            "                      \"Min-max scaling transforms features to a fixed range (typically 0-1), while\\nstandardization (z-score normalization) centers features around zero with unit\\nvariance. Robust scaling uses median and interquartile range to handle outliers\\nbetter.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [\n",
            "                      \"Min-max scaling: X' = (X - min(X)) / (max(X) - min(X))\",\n",
            "                      \"Standardization: X' = (X - μ) / σ\"\n",
            "                    ],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"type\": \"chapter\",\n",
            "        \"number\": \"2\",\n",
            "        \"title\": \"Core Machine Learning Algorithms\",\n",
            "        \"content\": null,\n",
            "        \"subsections\": [\n",
            "          {\n",
            "            \"type\": \"section\",\n",
            "            \"number\": \"2.1\",\n",
            "            \"title\": \"Supervised Learning Algorithms\",\n",
            "            \"content\": null,\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"2.1.1\",\n",
            "                \"title\": \"Linear Models\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.1.1.1\",\n",
            "                    \"title\": \"Linear Regression\",\n",
            "                    \"content\": \"Linear regression is a fundamental supervised learning algorithm that\\nmodels the relationship between a dependent variable and independent\\nvariables by fitting a linear equation to the observed data. It assumes a linear\\nrelationship between features and the target variable.\\n\\nThe algorithm finds the best-fitting line through the data points by minimizing\\nthe sum of squared residuals. The resulting model can be expressed as y = ßo\\n+ ß₁x₁ + ß₂x₂ + ... + ẞnxn, where ẞ coefficients represent the weights of each\\nfeature.\\n\\n# Linear regression implementation concept import numpy as np from\\nsklearn.linear_model import LinearRegression # Create and train model\\nmodel = LinearRegression() model.fit(X_train, y_train) # Make predictions\\npredictions = model.predict(X_test) Linear regression is interpretable,\\ncomputationally efficient, and serves as a baseline for many problems.\\nHowever, it assumes linearity and can be sensitive to outliers and multicollinearity.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Linear regression is a fundamental supervised learning algorithm that\\nmodels the relationship between a dependent variable and independent\\nvariables by fitting a linear equation to the observed data. It assumes a linear\\nrelationship between features and the target variable.\",\n",
            "                      \"The algorithm finds the best-fitting line through the data points by minimizing\\nthe sum of squared residuals. The resulting model can be expressed as y = ßo\\n+ ß₁x₁ + ß₂x₂ + ... + ẞnxn, where ẞ coefficients represent the weights of each\\nfeature.\",\n",
            "                      \"Linear regression is interpretable,\\ncomputationally efficient, and serves as a baseline for many problems.\\nHowever, it assumes linearity and can be sensitive to outliers and multicollinearity.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [\n",
            "                      \"# Linear regression implementation concept import numpy as np from\\nsklearn.linear_model import LinearRegression\",\n",
            "                      \"# Create and train model\\nmodel = LinearRegression() model.fit(X_train, y_train)\",\n",
            "                      \"# Make predictions\\npredictions = model.predict(X_test)\"\n",
            "                    ],\n",
            "                    \"formulas\": [\n",
            "                      \"y = ßo\\n+ ß₁x₁ + ß₂x₂ + ... + ẞnxn\"\n",
            "                    ],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.1.1.2\",\n",
            "                    \"title\": \"Logistic Regression\",\n",
            "                    \"content\": \"Logistic regression extends linear regression to classification problems by\\nusing the logistic function to model the probability of class membership.\\nDespite its name, it's a classification algorithm that outputs probabilities\\nbetween 0 and 1.\\n\\nThe logistic function (sigmoid) transforms the linear combination of features\\ninto a probability: P(y=1|x) = 1 / (1 + e^(-z)), where z = Bo + B₁x₁ + ... + Bnxn. This\\nensures outputs are always between 0 and 1.\\n\\nLogistic function: σ(z) = 1 / (1 + e^(-z))\\nLog-odds: In(p/(1-p)) = ßo + ß₁x₁ + ... + ßnxn\\n\\nLogistic regression is widely used for binary classification and can be\\nextended to multi-class problems. It provides probabilistic outputs and\\ncoefficient interpretability, making it valuable for many applications.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Logistic regression extends linear regression to classification problems by\\nusing the logistic function to model the probability of class membership.\\nDespite its name, it's a classification algorithm that outputs probabilities\\nbetween 0 and 1.\",\n",
            "                      \"The logistic function (sigmoid) transforms the linear combination of features\\ninto a probability: P(y=1|x) = 1 / (1 + e^(-z)), where z = Bo + B₁x₁ + ... + Bnxn. This\\nensures outputs are always between 0 and 1.\",\n",
            "                      \"Logistic regression is widely used for binary classification and can be\\nextended to multi-class problems. It provides probabilistic outputs and\\ncoefficient interpretability, making it valuable for many applications.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [\n",
            "                      \"P(y=1|x) = 1 / (1 + e^(-z))\",\n",
            "                      \"z = Bo + B₁x₁ + ... + Bnxn\",\n",
            "                      \"Logistic function: σ(z) = 1 / (1 + e^(-z))\",\n",
            "                      \"Log-odds: In(p/(1-p)) = ßo + ß₁x₁ + ... + ßnxn\"\n",
            "                    ],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.1.1.3\",\n",
            "                    \"title\": \"Regularization Techniques\",\n",
            "                    \"content\": \"Regularization techniques prevent overfitting by adding penalty terms to the\\nloss function. They constrain model complexity and improve generalization\\nto unseen data, especially important when dealing with high-dimensional\\ndatasets.\\n\\nL1 regularization (Lasso) adds the sum of absolute values of coefficients as a\\npenalty, promoting sparsity and automatic feature selection. L2\\nregularization (Ridge) adds the sum of squared coefficients, shrinking\\ncoefficients towards zero but keeping all features.\\n\\nElastic Net combines L1 and L2 regularization.\",\n",
            "                    \"paragraphs\": [\n",
            "                      \"Regularization techniques prevent overfitting by adding penalty terms to the\\nloss function. They constrain model complexity and improve generalization\\nto unseen data, especially important when dealing with high-dimensional\\ndatasets.\",\n",
            "                      \"L1 regularization (Lasso) adds the sum of absolute values of coefficients as a\\npenalty, promoting sparsity and automatic feature selection. L2\\nregularization (Ridge) adds the sum of squared coefficients, shrinking\\ncoefficients towards zero but keeping all features.\",\n",
            "                      \"Elastic Net combines L1 and L2 regularization.\"\n",
            "                    ],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"2.1.2\",\n",
            "                \"title\": \"Tree-Based Methods\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.1.2.1\",\n",
            "                    \"title\": \"Decision Trees\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.1.2.2\",\n",
            "                    \"title\": \"Random Forests\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.1.2.3\",\n",
            "                    \"title\": \"Gradient Boosting\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"type\": \"section\",\n",
            "            \"number\": \"2.2\",\n",
            "            \"title\": \"Unsupervised Learning Algorithms\",\n",
            "            \"content\": null,\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"2.2.1\",\n",
            "                \"title\": \"Clustering Algorithms\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.2.1.1\",\n",
            "                    \"title\": \"K-Means Clustering\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.2.1.2\",\n",
            "                    \"title\": \"Hierarchical Clustering\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.2.1.3\",\n",
            "                    \"title\": \"DBSCAN\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"2.2.2\",\n",
            "                \"title\": \"Dimensionality Reduction\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.2.2.1\",\n",
            "                    \"title\": \"Principal Component Analysis\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.2.2.2\",\n",
            "                    \"title\": \"t-SNE\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          },\n",
            "          {\n",
            "            \"type\": \"section\",\n",
            "            \"number\": \"2.3\",\n",
            "            \"title\": \"Model Evaluation and Selection\",\n",
            "            \"content\": null,\n",
            "            \"subsections\": [\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"2.3.1\",\n",
            "                \"title\": \"Performance Metrics\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.3.1.1\",\n",
            "                    \"title\": \"Classification Metrics\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.3.1.2\",\n",
            "                    \"title\": \"Regression Metrics\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              },\n",
            "              {\n",
            "                \"type\": \"section\",\n",
            "                \"number\": \"2.3.2\",\n",
            "                \"title\": \"Cross-Validation\",\n",
            "                \"content\": null,\n",
            "                \"subsections\": [\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.3.2.1\",\n",
            "                    \"title\": \"K-Fold Cross-Validation\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  },\n",
            "                  {\n",
            "                    \"type\": \"section\",\n",
            "                    \"number\": \"2.3.2.2\",\n",
            "                    \"title\": \"Stratified Sampling\",\n",
            "                    \"content\": null,\n",
            "                    \"paragraphs\": [],\n",
            "                    \"notes\": [],\n",
            "                    \"code_examples\": [],\n",
            "                    \"formulas\": [],\n",
            "                    \"subsections\": []\n",
            "                  }\n",
            "                ]\n",
            "              }\n",
            "            ]\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Convert the extracted text to JSON\n",
        "json_output = text_to_json_with_gemini(text)\n",
        "print(json_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
