{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTERR2zBG_yN",
        "outputId": "85bc587f-9def-44e9-d4da-d277a2cb53f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.67)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain  langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf pymupdf langchain_google_genai unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r87_mBaMYq4",
        "outputId": "021063fc-8055-490e-d10d-47b7087eeb5d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.7.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.1.6)\n",
            "Collecting unstructured\n",
            "  Downloading unstructured-0.18.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.6.18)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.67)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.7)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.0.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.14.0)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.38.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (0.4.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2025.6.15)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (3.10.18)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain_google_genai) (0.23.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Downloading unstructured-0.18.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.38.1-py3-none-any.whl (212 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.6/212.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=8355c193f6a6ae3d628d5f9b25036261c8251a95411980127e0a85d8722ecf5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: rapidfuzz, python-magic, python-iso639, olefile, langdetect, emoji, backoff, python-oxmsg, unstructured-client, unstructured\n",
            "Successfully installed backoff-2.2.1 emoji-2.14.1 langdetect-1.0.9 olefile-0.47 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 unstructured-0.18.3 unstructured-client-0.38.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-pptx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lkQLf_OrIot",
        "outputId": "ec35255b-be17-4635-c797-8b338e5bad8a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.14.0)\n",
            "Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.5-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.2.5 python-pptx-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVQo6UXfeSyI",
        "outputId": "a2158745-0054-4e72-b92b-715ecef3143b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: connect with drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6kqO06cjUdC",
        "outputId": "db422688-38ac-498d-a41b-791999b543fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_path='/content/drive/MyDrive/matrials'"
      ],
      "metadata": {
        "id": "5i0-CYZ6Krxu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Step 1: Load the PDF book\n",
        "pdf_path = \"/content/Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf\"\n",
        "loader = PyMuPDFLoader(pdf_path)\n",
        "pages = loader.load()\n",
        "\n",
        "# Optional: Attach metadata to each page\n",
        "for i, page in enumerate(pages):\n",
        "    page.metadata[\"page_number\"] = i + 1  # Add page number\n",
        "    page.metadata[\"source\"] = pdf_path\n",
        "    page.metadata[\"title\"] = \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"\n",
        "\n",
        "# Step 2: Split each page while preserving metadata\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks = []\n",
        "for page in pages:\n",
        "    split_page_chunks = text_splitter.split_documents([page])\n",
        "    for chunk in split_page_chunks:\n",
        "        chunk.metadata.update(page.metadata)  # Propagate metadata to chunk\n",
        "        chunks.append(chunk)\n",
        "\n",
        "# Step 3: View some chunks with metadata\n",
        "for i, chunk in enumerate(chunks[:5]):\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print(chunk.page_content[:300], \"\\n\")\n"
      ],
      "metadata": {
        "id": "oq8ib8V_HGHL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.document_loaders import PDFPlumberLoader\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCMXDcWH1AGT-doRIL5lV6r6bk9BcDWSPE\"\n",
        "from langchain.document_loaders import PyMuPDFLoader, TextLoader, UnstructuredWordDocumentLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "epHxHUi4PXSG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "ejRtJ7JQPXcK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.document_loaders import (\n",
        "    PyMuPDFLoader,\n",
        "    TextLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        "    UnstructuredPowerPointLoader,\n",
        ")\n",
        "\n",
        "def load_documents(directory_path):\n",
        "    \"\"\"Load all PDF, TXT, DOCX, and PPTX documents from a directory\"\"\"\n",
        "    documents = []\n",
        "    titles = []\n",
        "\n",
        "    for filename in os.listdir(directory_path):\n",
        "        path = os.path.join(directory_path, filename)\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "        # Select appropriate loader\n",
        "        if ext == \".pdf\":\n",
        "            loader = PyMuPDFLoader(path)\n",
        "        elif ext == \".txt\":\n",
        "            loader = TextLoader(path, encoding=\"utf-8\")\n",
        "        elif ext == \".docx\":\n",
        "            loader = UnstructuredWordDocumentLoader(path)\n",
        "        elif ext == \".pptx\":\n",
        "            loader = UnstructuredPowerPointLoader(path)\n",
        "        else:\n",
        "            print(f\"Skipping unsupported file: {filename}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Load and enrich documents\n",
        "            docs = loader.load()\n",
        "            title = os.path.splitext(filename)[0]\n",
        "            titles.append(title)\n",
        "\n",
        "            for doc in docs:\n",
        "                doc.metadata[\"title\"] = title\n",
        "                doc.metadata[\"source_file\"] = filename\n",
        "\n",
        "            documents.extend(docs)\n",
        "            print(f\"Loaded document: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ Total documents loaded: {len(documents)}\")\n",
        "    return documents, titles\n"
      ],
      "metadata": {
        "id": "N8Yh6cLgPXjV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_chunk_size(documents, chunk_size=1000):\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    doc_groups = {}\n",
        "    for doc in documents:\n",
        "        doc_id = doc.metadata.get(\"title\") or doc.metadata.get(\"source_file\") or \"Unknown_Document\"\n",
        "        doc_groups.setdefault(doc_id, []).append(doc)\n",
        "\n",
        "    # Process each grouped document\n",
        "    for doc_id, docs in doc_groups.items():\n",
        "        full_text = \" \".join([doc.page_content for doc in docs])\n",
        "        source_file = docs[0].metadata.get(\"source_file\", doc_id)\n",
        "\n",
        "        num_chunks = (len(full_text) + chunk_size - 1) // chunk_size\n",
        "\n",
        "        for i in range(0, len(full_text), chunk_size):\n",
        "            chunk_text = full_text[i:i + chunk_size]\n",
        "            chunk_index = i // chunk_size\n",
        "\n",
        "            chunk = Document(\n",
        "                page_content=chunk_text,\n",
        "                metadata={\n",
        "                    \"title\": doc_id,\n",
        "                    \"source_file\": source_file,\n",
        "                    \"chunk_index\": chunk_index,\n",
        "                    \"total_chunks\": num_chunks\n",
        "                }\n",
        "            )\n",
        "            all_chunks.append(chunk)\n",
        "\n",
        "        print(f\"✅ Split document '{doc_id}' into {num_chunks} chunks\")\n",
        "\n",
        "    print(f\"\\n📄 Total chunks created: {len(all_chunks)}\")\n",
        "    return all_chunks\n"
      ],
      "metadata": {
        "id": "JxNAtnowPrYs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(chunks):\n",
        "    \"\"\"\n",
        "    Create a vector store from document chunks\n",
        "    \"\"\"\n",
        "    print(\"Creating vector store...\")\n",
        "    # Using HuggingFace embeddings (open-source alternative)\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    # Create the vector store\n",
        "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "    print(\"Vector store created successfully\")\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "_Az2o-AgUuT1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = \"/content/drive/MyDrive/matrials\"\n",
        "documents, candidate_names = load_documents(folder_path)\n",
        "chunks=split_by_chunk_size(documents)\n",
        "vector_store = create_vector_store(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJC0FTWwU23R",
        "outputId": "676d15aa-409d-4584-d228-d7da689fd717"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded document: Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow.pdf\n",
            "Loaded document: Machine Learning.pptx\n",
            "\n",
            "✅ Total documents loaded: 865\n",
            "✅ Split document 'Hands_On_Machine_Learning_with_Scikit_Learn,_Keras,_and_TensorFlow' into 1778 chunks\n",
            "✅ Split document 'Machine Learning' into 51 chunks\n",
            "\n",
            "📄 Total chunks created: 1829\n",
            "Creating vector store...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-3357096028.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt):\n",
        "    \"\"\"Generate response using Google Generative AI (Gemini) directly\"\"\"\n",
        "\n",
        "    # ✅ Configure API key securely\n",
        "    genai.configure(api_key=\"AIzaSyDHk_I3kK8zCwOX2tKkm4xlCYXkEcyNfn0\")\n",
        "\n",
        "    # ✅ Load the Gemini model\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")  # or \"gemini-1.5-pro\"\n",
        "\n",
        "    # ✅ Generate response\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "NvwU-sJtkXS2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_main_topics(topic: str) -> list:\n",
        "    prompt = f\"\"\"\n",
        "You are an expert curriculum designer.\n",
        "\n",
        "🎯 Task:\n",
        "List the **essential subtopics** someone should learn to master **{topic}**.\n",
        "\n",
        "🔒 Rules:\n",
        "- Output only a valid **JSON array** of strings.\n",
        "- Do NOT include explanations, bullet points, or markdown formatting.\n",
        "- Return ONLY the list. No extra text.\n",
        "\n",
        "✅ Correct Format Example:\n",
        "[\"Topic A\", \"Topic B\", \"Topic C\"]\n",
        "\"\"\"\n",
        "\n",
        "    response = generate_response(prompt)  # Your LLM call\n",
        "\n",
        "    # Remove Markdown code block (e.g., ```json ... ```)\n",
        "    cleaned = re.sub(r\"```(?:json)?\", \"\", response.strip(), flags=re.IGNORECASE).strip(\"`\").strip()\n",
        "\n",
        "    try:\n",
        "        topic_list = json.loads(cleaned)\n",
        "        return topic_list\n",
        "    except Exception as e:\n",
        "        print(\"❌ Failed to parse list from response:\", e)\n",
        "        print(\"⚠️ Cleaned response:\", cleaned)\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "JgvQ_HDIlQjJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=extract_main_topics(\"classical machine learning\")"
      ],
      "metadata": {
        "id": "_Pjnvu0elU3y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G3eD0Dbmcf-",
        "outputId": "10d7cf95-975c-47a1-f26e-e14d6819f906"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Linear Algebra', 'Probability and Statistics', 'Calculus', 'Data Preprocessing', 'Supervised Learning', 'Unsupervised Learning', 'Model Evaluation and Selection', 'Bias-Variance Tradeoff', 'Regularization Techniques', 'Hyperparameter Tuning', 'Cross-Validation', 'Feature Engineering', 'Dimensionality Reduction', 'Ensemble Methods', 'Decision Trees', 'Support Vector Machines', 'Naive Bayes', 'Logistic Regression', 'Linear Regression', 'K-Nearest Neighbors', 'K-Means Clustering', 'Hierarchical Clustering', 'Principal Component Analysis', 'Clustering Evaluation Metrics', 'Model Deployment', 'Handling Imbalanced Datasets', 'A/B Testing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#topic = \"ensemble learning\"\n",
        "#relevant_docs = vector_store.similarity_search(topic, k=30)\n",
        "\n",
        "#relevant_text = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "#slides_output = generate_topic_slides_from_doc(doc_id, relevant_text, topic)\n",
        "#print(slides_output)\n"
      ],
      "metadata": {
        "id": "4f2WaDN4VWis"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_topic_slides_from_doc(doc_id: str, full_context: str, topic: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "You are a highly skilled AI assistant tasked with creating a professional, educational slide deck based on a document.\n",
        "\n",
        "📄 Document Title: {doc_id}\n",
        "\n",
        "🎯 Topic of Focus: \"{topic}\"\n",
        "\n",
        "📚 Document Content:\n",
        "\\\"\\\"\\\"\n",
        "{full_context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "---\n",
        "\n",
        "📝 Your Task:\n",
        "Create exactly **10 detailed slides** that thoroughly explain the topic **\"{topic}\"**, using only the information from the document above.\n",
        "\n",
        "Each slide must include:\n",
        "- A clear and **descriptive title**\n",
        "- 3–5 bullet points that explain key concepts, steps, facts, or insights\n",
        "- Use formal and educational tone\n",
        "- Avoid generalizations — focus only on what's found in the document\n",
        "- Do NOT make up any facts that aren't present in the content\n",
        "- Avoid repetition — cover **different aspects** of the topic in each slide\n",
        "\n",
        "If the topic is broad, break it down into subtopics or steps across the slides to give full understanding.\n",
        "\n",
        "---\n",
        "\n",
        "🎓 Output Format (Markdown Style):\n",
        "\n",
        "Slide 1️⃣\n",
        "**Title:** [Slide Title]\n",
        "- Bullet point 1\n",
        "- Bullet point 2\n",
        "...\n",
        "\n",
        "Slide 2️⃣\n",
        "**Title:** [Slide Title]\n",
        "- Bullet point 1\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "Slide 🔟\n",
        "**Title:** [Slide Title]\n",
        "- Bullet point 1\n",
        "...\n",
        "\n",
        "Ensure formatting is clean and easy to read. Make sure the entire topic is covered in a structured and progressive way across the 10 slides.\n",
        "\"\"\"\n",
        "\n",
        "    response = generate_response(prompt)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "f61-fpr1RtqU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_id=\"classical machine learning\""
      ],
      "metadata": {
        "id": "TX2zPCZ1qIKj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx import Presentation\n",
        "from pptx.util import Inches, Pt\n",
        "from pptx.dml.color import RGBColor\n",
        "from pptx.enum.text import PP_ALIGN\n",
        "from math import ceil\n",
        "\n",
        "# ==================== 🎨 STYLES ====================\n",
        "TITLE_FONT_SIZE = Pt(36)\n",
        "BODY_FONT_SIZE = Pt(20)\n",
        "TITLE_COLOR = RGBColor(0, 51, 102)        # Navy Blue\n",
        "BODY_COLOR = RGBColor(30, 30, 30)         # Dark gray\n",
        "BACKGROUND_COLOR = RGBColor(240, 248, 255)  # Light Blue-Gray\n",
        "\n",
        "\n",
        "# ==================== 🎨 HELPERS ====================\n",
        "def set_background(slide, color: RGBColor):\n",
        "    \"\"\"Set background color of a slide.\"\"\"\n",
        "    background = slide.background\n",
        "    fill = background.fill\n",
        "    fill.solid()\n",
        "    fill.fore_color.rgb = color\n",
        "\n",
        "\n",
        "def format_textbox(text_frame, font_size=BODY_FONT_SIZE, font_color=BODY_COLOR):\n",
        "    \"\"\"Apply font styling to a text frame.\"\"\"\n",
        "    for p in text_frame.paragraphs:\n",
        "        for run in p.runs:\n",
        "            run.font.size = font_size\n",
        "            run.font.color.rgb = font_color\n",
        "        p.alignment = PP_ALIGN.LEFT\n",
        "\n",
        "\n",
        "# ==================== 📋 AGENDA ====================\n",
        "def add_agenda_slides(prs, topics, max_per_slide=12):\n",
        "    \"\"\"Add agenda slides with two-column layout, split across multiple slides if needed.\"\"\"\n",
        "    num_slides = ceil(len(topics) / max_per_slide)\n",
        "\n",
        "    for i in range(num_slides):\n",
        "        agenda_slide = prs.slides.add_slide(prs.slide_layouts[5])  # Title Only layout\n",
        "        set_background(agenda_slide, BACKGROUND_COLOR)\n",
        "        agenda_slide.shapes.title.text = f\"📚 Agenda (Part {i+1})\" if num_slides > 1 else \"📚 Agenda\"\n",
        "\n",
        "        # Text box area\n",
        "        left = Inches(0.7)\n",
        "        top = Inches(1.8)\n",
        "        width = Inches(8.0)\n",
        "        height = Inches(5.0)\n",
        "\n",
        "        textbox = agenda_slide.shapes.add_textbox(left, top, width, height)\n",
        "        frame = textbox.text_frame\n",
        "        frame.word_wrap = True\n",
        "        frame.clear()\n",
        "\n",
        "        # Current chunk of topics\n",
        "        current_chunk = topics[i * max_per_slide : (i + 1) * max_per_slide]\n",
        "        midpoint = ceil(len(current_chunk) / 2)\n",
        "        col1 = current_chunk[:midpoint]\n",
        "        col2 = current_chunk[midpoint:]\n",
        "\n",
        "        # Column 1\n",
        "        for idx, topic in enumerate(col1):\n",
        "            p = frame.add_paragraph()\n",
        "            p.text = f\"{i * max_per_slide + idx + 1}. {topic}\"\n",
        "            p.level = 0\n",
        "\n",
        "        # Column 2 (right margin)\n",
        "        for idx, topic in enumerate(col2):\n",
        "            p = frame.add_paragraph()\n",
        "            p.text = f\"{i * max_per_slide + midpoint + idx + 1}. {topic}\"\n",
        "            p.level = 0\n",
        "            p.margin_left = Inches(4)\n",
        "\n",
        "        format_textbox(frame)\n",
        "\n",
        "\n",
        "# ==================== 🏗️ MAIN FUNCTION ====================\n",
        "def markdown_to_pptx(slides_dict: dict, output_file: str):\n",
        "    \"\"\"Generate a beautifully formatted PowerPoint presentation from markdown slide dict.\"\"\"\n",
        "    prs = Presentation()\n",
        "\n",
        "    # Agenda (multi-slide, multi-column)\n",
        "    topics = list(slides_dict.keys())\n",
        "    add_agenda_slides(prs, topics, max_per_slide=12)\n",
        "\n",
        "    for topic, slides in slides_dict.items():\n",
        "        # Divider slide\n",
        "        topic_slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
        "        set_background(topic_slide, BACKGROUND_COLOR)\n",
        "        topic_slide.shapes.title.text = topic\n",
        "        try:\n",
        "            topic_slide.placeholders[1].text = \"Section Begins\"\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Parse individual slides\n",
        "        for slide_text in slides.split(\"Slide \")[1:]:\n",
        "            slide_lines = slide_text.strip().splitlines()\n",
        "            slide_title = \"\"\n",
        "            bullet_points = []\n",
        "\n",
        "            for line in slide_lines:\n",
        "                if line.startswith(\"**Title:**\"):\n",
        "                    slide_title = line.replace(\"**Title:**\", \"\").strip()\n",
        "                elif line.startswith(\"- \"):\n",
        "                    bullet_points.append(line[2:].strip())\n",
        "\n",
        "            # Content slide\n",
        "            slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
        "            set_background(slide, RGBColor(255, 255, 255))\n",
        "            slide.shapes.title.text = slide_title if slide_title else topic\n",
        "\n",
        "            # Style title\n",
        "            title_shape = slide.shapes.title\n",
        "            p = title_shape.text_frame.paragraphs[0]\n",
        "            p.runs[0].font.size = TITLE_FONT_SIZE\n",
        "            p.runs[0].font.color.rgb = TITLE_COLOR\n",
        "\n",
        "            # Add bullet content\n",
        "            content_shape = slide.placeholders[1]\n",
        "            text_frame = content_shape.text_frame\n",
        "            text_frame.clear()\n",
        "\n",
        "            for bullet in bullet_points:\n",
        "                p = text_frame.add_paragraph()\n",
        "                p.text = bullet\n",
        "                p.level = 0\n",
        "                p.space_after = Pt(4)\n",
        "\n",
        "            format_textbox(text_frame)\n",
        "\n",
        "    prs.save(output_file)\n",
        "    print(f\"✅ Presentation saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "NWGXPoIArT9i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = extract_main_topics(doc_id)"
      ],
      "metadata": {
        "id": "B9jzoTctt8L0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5WzjOtqx4bo",
        "outputId": "544cadf8-7222-4e4d-9a10-bd72af4ed188"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Linear Algebra',\n",
              " 'Calculus',\n",
              " 'Probability and Statistics',\n",
              " 'Data Preprocessing',\n",
              " 'Supervised Learning',\n",
              " 'Unsupervised Learning',\n",
              " 'Model Evaluation Metrics',\n",
              " 'Regularization Techniques',\n",
              " 'Bias-Variance Tradeoff',\n",
              " 'Feature Engineering',\n",
              " 'Cross-Validation',\n",
              " 'Hyperparameter Tuning',\n",
              " 'Model Selection',\n",
              " 'Dimensionality Reduction',\n",
              " 'Clustering Algorithms',\n",
              " 'Classification Algorithms',\n",
              " 'Regression Algorithms',\n",
              " 'Ensemble Methods',\n",
              " 'Overfitting and Underfitting',\n",
              " 'Reinforcement Learning Basics (optional)']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # progress bar\n",
        "\n",
        "def generate_all_content(doc_id: str, vector_store, output_file=\"Course_Slides.pptx\"):\n",
        "    # 1. Extract course topics\n",
        "    all_slides = {}\n",
        "\n",
        "    # 2. Loop over each topic with a progress bar\n",
        "    for topic in tqdm(topics, desc=\"📚 Generating Slides\", unit=\"topic\"):\n",
        "        try:\n",
        "            # Search for relevant document chunks\n",
        "            relevant_docs = vector_store.similarity_search(topic, k=30)\n",
        "            combined_context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "            # Generate slides\n",
        "            slides_output = generate_topic_slides_from_doc(doc_id, combined_context, topic)\n",
        "\n",
        "            # Store in dictionary\n",
        "            all_slides[topic] = slides_output\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Skipped topic '{topic}' due to error: {e}\")\n",
        "\n",
        "    # 3. Generate PowerPoint from all topics\n",
        "    markdown_to_pptx(all_slides, output_file)\n",
        "\n",
        "    print(f\"✅ Slides generated and saved to: {output_file}\")\n",
        "    return output_file\n"
      ],
      "metadata": {
        "id": "PPajsmN1p1K_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_pptx = generate_all_content(\n",
        "    doc_id=doc_id,\n",
        "    vector_store=vector_store,\n",
        "    output_file=\"Course_Slides.pptx\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "ZJlFGnairGRK",
        "outputId": "15207032-976e-4447-c28e-e4d074ba7eac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "📚 Generating Slides: 100%|██████████| 20/20 [02:51<00:00,  8.56s/topic]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Presentation saved to Course_Slides.pptx\n",
            "✅ Slides generated and saved to: Course_Slides.pptx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx import Presentation\n",
        "\n",
        "def extract_slide_texts(pptx_path):\n",
        "    prs = Presentation(pptx_path)\n",
        "    slides = []\n",
        "\n",
        "    for i, slide in enumerate(prs.slides):\n",
        "        content = {\n",
        "            \"index\": i,\n",
        "            \"title\": \"\",\n",
        "            \"text\": [],\n",
        "        }\n",
        "\n",
        "        for shape in slide.shapes:\n",
        "            if shape.has_text_frame:\n",
        "                if not content[\"title\"]:\n",
        "                    content[\"title\"] = shape.text.strip()\n",
        "                else:\n",
        "                    content[\"text\"].append(shape.text.strip())\n",
        "        slides.append(content)\n",
        "    return slides\n"
      ],
      "metadata": {
        "id": "jJhYn3lWE6kj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def organize_slides_with_llm(slides):\n",
        "    slide_text = \"\\n\\n\".join([f\"Slide {i+1}\\nTitle: {s['title']}\\nText: {' '.join(s['text'])}\" for i, s in enumerate(slides)])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an AI presentation assistant.\n",
        "\n",
        "Here is a set of disorganized slides from a presentation:\n",
        "\n",
        "{slide_text}\n",
        "\n",
        "Your task is to:\n",
        "- Identify and merge duplicate topics (e.g., two slides about Linear Regression → merge into one section).\n",
        "- Group slides into structured sections: Introduction, Topics (with headers), Conclusion.\n",
        "- Place code blocks after explanations.\n",
        "- Reformat math equations (e.g., y = mx + b → nicely typeset).\n",
        "- Add a clean introduction and conclusion.\n",
        "- Suggest slide titles and reordering.\n",
        "- Avoid repeated content.\n",
        "- Output structured slides as a JSON array like below:\n",
        "\n",
        "[\n",
        "  {{\n",
        "    \"title\": \"Section Header: Linear Regression\",\n",
        "    \"type\": \"header\"\n",
        "  }},\n",
        "  {{\n",
        "    \"title\": \"Understanding Linear Regression\",\n",
        "    \"bullets\": [\"Linear regression models relationship between X and Y\", \"Uses least squares method\"],\n",
        "    \"code\": \"from sklearn.linear_model import LinearRegression\\\\nmodel = LinearRegression().fit(X, y)\",\n",
        "    \"equations\": [\"y = mx + b\"]\n",
        "  }}\n",
        "]\n",
        "\n",
        "Only return valid JSON. No commentary, no markdown, no explanation.\n",
        "\"\"\"\n",
        "\n",
        "    response = generate_response(prompt).strip()\n",
        "\n",
        "    # Remove markdown ```json wrappers if they exist\n",
        "    if response.startswith(\"```json\") or response.startswith(\"```\"):\n",
        "        response = re.sub(r\"```(?:json)?\", \"\", response, flags=re.IGNORECASE).strip(\"`\").strip()\n",
        "\n",
        "    try:\n",
        "        return json.loads(response)\n",
        "    except Exception as e:\n",
        "        print(\"❌ LLM failed to return valid JSON:\", e)\n",
        "        print(\"⚠️ Raw LLM response:\")\n",
        "        print(response)\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "xthEnt65e60u"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pptx import Presentation\n",
        "from pptx.util import Pt\n",
        "\n",
        "def build_final_presentation(organized_slides, output_path):\n",
        "    prs = Presentation()\n",
        "\n",
        "    for slide in organized_slides:\n",
        "        # Choose layout\n",
        "        layout = prs.slide_layouts[0] if slide.get(\"type\") == \"header\" else prs.slide_layouts[1]\n",
        "        ppt_slide = prs.slides.add_slide(layout)\n",
        "\n",
        "        # Add title\n",
        "        ppt_slide.shapes.title.text = slide.get(\"title\", \"Untitled Slide\")\n",
        "\n",
        "        # Skip content for header slides\n",
        "        if slide.get(\"type\") == \"header\":\n",
        "            continue\n",
        "\n",
        "        # Build the content text\n",
        "        content_text = []\n",
        "\n",
        "        if slide.get(\"bullets\"):\n",
        "            content_text.append(\"• \" + \"\\n• \".join(slide[\"bullets\"]))\n",
        "        if slide.get(\"code\"):\n",
        "            content_text.append(\"\\nCode:\\n\" + slide[\"code\"])\n",
        "        if slide.get(\"equations\"):\n",
        "            content_text.append(\"\\nEquations:\\n\" + \", \".join(slide[\"equations\"]))\n",
        "\n",
        "        # Write to placeholder[1] if exists\n",
        "        try:\n",
        "            content = ppt_slide.placeholders[1]\n",
        "            content.text = \"\\n\\n\".join(content_text)\n",
        "        except IndexError:\n",
        "            print(f\"⚠️ No content placeholder found on slide: {slide.get('title')}\")\n",
        "\n",
        "    prs.save(output_path)\n",
        "    print(f\"✅ Cleaned presentation saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "TPXcBPemFFUU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_organize_presentation(input_pptx: str, output_pptx: str):\n",
        "    raw_slides = extract_slide_texts(input_pptx)\n",
        "    organized = organize_slides_with_llm(raw_slides)\n",
        "    build_final_presentation(organized, output_pptx)\n"
      ],
      "metadata": {
        "id": "k-OZJWf2FFcw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_and_organize_presentation(\"/content/Course_Slides.pptx\", \"final_Course_Slides.pptx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "5FTeyQ7KFNUd",
        "outputId": "74688d63-f9e0-4250-bc8c-6fc2beffca4e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned presentation saved to: final_Course_Slides.pptx\n"
          ]
        }
      ]
    }
  ]
}